This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-06-07T02:51:10.351Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
CHANGELOG.md
docs/user_manual.md
Example_usage.py
MANIFEST.in
parallel_example.py
PARALLEL_TESTING_GUIDE.md
pyproject.toml
README_cn.md
README.md
requirements.txt
setup.py
SPACE_PATH_FIX.md
src/__init__.py
src/cli_test_framework.egg-info/dependency_links.txt
src/cli_test_framework.egg-info/entry_points.txt
src/cli_test_framework.egg-info/PKG-INFO
src/cli_test_framework.egg-info/requires.txt
src/cli_test_framework.egg-info/SOURCES.txt
src/cli_test_framework.egg-info/top_level.txt
src/cli_test_framework/__init__.py
src/cli_test_framework/cli.py
src/cli_test_framework/commands/__init__.py
src/cli_test_framework/commands/compare.py
src/cli_test_framework/core/__init__.py
src/cli_test_framework/core/assertions.py
src/cli_test_framework/core/base_runner.py
src/cli_test_framework/core/parallel_runner.py
src/cli_test_framework/core/process_worker.py
src/cli_test_framework/core/test_case.py
src/cli_test_framework/file_comparator/__init__.py
src/cli_test_framework/file_comparator/base_comparator.py
src/cli_test_framework/file_comparator/binary_comparator.py
src/cli_test_framework/file_comparator/csv_comparator.py
src/cli_test_framework/file_comparator/factory.py
src/cli_test_framework/file_comparator/h5_comparator.py
src/cli_test_framework/file_comparator/json_comparator.py
src/cli_test_framework/file_comparator/result.py
src/cli_test_framework/file_comparator/text_comparator.py
src/cli_test_framework/file_comparator/xml_comparator.py
src/cli_test_framework/runners/__init__.py
src/cli_test_framework/runners/json_runner.py
src/cli_test_framework/runners/parallel_json_runner.py
src/cli_test_framework/runners/yaml_runner.py
src/cli_test_framework/utils/__init__.py
src/cli_test_framework/utils/path_resolver.py
src/cli_test_framework/utils/report_generator.py
tests/__init__.py
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json
tests/performance_test.py
tests/test_comprehensive_space.py
tests/test_parallel_runner.py
tests/test_parallel_space.py
tests/test_report.txt
tests/test_runners.py
tests/test1.py
User_Manual.md

================================================================
Files
================================================================

================
File: CHANGELOG.md
================
# å˜æ›´æ—¥å¿—

## [v2.0.0] - 2025-05-27

### ðŸš€ é‡å¤§æ–°åŠŸèƒ½

#### å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ
- **å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šæ”¯æŒ I/O å¯†é›†åž‹æµ‹è¯•çš„é«˜æ•ˆå¹¶è¡Œå¤„ç†
- **å¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šæ”¯æŒ CPU å¯†é›†åž‹æµ‹è¯•çš„å®Œå…¨éš”ç¦»æ‰§è¡Œ
- **å¯é…ç½®å¹¶å‘æ•°**ï¼šçµæ´»è®¾ç½®æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°
- **æ€§èƒ½æå‡**ï¼šå…¸åž‹åœºæ™¯ä¸‹å¯èŽ·å¾— 2-4 å€åŠ é€Ÿæ¯”

#### æ™ºèƒ½å‘½ä»¤è§£æž
- **å¤æ‚å‘½ä»¤æ”¯æŒ**ï¼šæ™ºèƒ½å¤„ç† `"python ./script.py"` ç­‰å¤æ‚å‘½ä»¤æ ¼å¼
- **ç³»ç»Ÿå‘½ä»¤è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿå‘½ä»¤å’Œç›¸å¯¹è·¯å¾„å¯æ‰§è¡Œæ–‡ä»¶
- **è·¯å¾„è§£æžå¢žå¼º**ï¼šæ›´å‡†ç¡®çš„è·¯å¾„è½¬æ¢å’Œå‘½ä»¤æž„å»º

### âœ¨ åŠŸèƒ½å¢žå¼º

#### çº¿ç¨‹å®‰å…¨è®¾è®¡
- **ç»“æžœæ”¶é›†é”**ï¼šç¡®ä¿å¹¶å‘çŽ¯å¢ƒä¸‹æµ‹è¯•ç»“æžœçš„æ­£ç¡®æ€§
- **è¾“å‡ºæŽ§åˆ¶é”**ï¼šé˜²æ­¢å¹¶å‘è¾“å‡ºæ··ä¹±
- **å¼‚å¸¸éš”ç¦»**ï¼šå•ä¸ªæµ‹è¯•å¤±è´¥ä¸å½±å“å…¶ä»–æµ‹è¯•

#### æ–°å¢žè¿è¡Œå™¨
- **ParallelJSONRunner**ï¼šæ”¯æŒå¹¶è¡Œæ‰§è¡Œçš„ JSON æµ‹è¯•è¿è¡Œå™¨
- **è¿›ç¨‹å·¥ä½œå™¨**ï¼šç‹¬ç«‹çš„è¿›ç¨‹æ‰§è¡Œæ¨¡å—ï¼Œè§£å†³åºåˆ—åŒ–é—®é¢˜

### ðŸ”§ é—®é¢˜ä¿®å¤

#### å‘½ä»¤è§£æžä¿®å¤
- **ä¿®å¤é—®é¢˜**ï¼š`'D:\Document\xcode\Compare-File-Tool\python' ä¸æ˜¯å†…éƒ¨æˆ–å¤–éƒ¨å‘½ä»¤`
- **æ ¹æœ¬åŽŸå› **ï¼šå¤æ‚å‘½ä»¤å­—ç¬¦ä¸²è¢«æ•´ä½“å½“ä½œå‘½ä»¤åå¤„ç†
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ™ºèƒ½åˆ†å‰²å‘½ä»¤å­—ç¬¦ä¸²ï¼Œåˆ†åˆ«å¤„ç†å‘½ä»¤å’Œå‚æ•°éƒ¨åˆ†

#### è·¯å¾„è§£æžä¼˜åŒ–
- **ç³»ç»Ÿå‘½ä»¤ç™½åå•**ï¼šæ‰©å±•ç³»ç»Ÿå‘½ä»¤è¯†åˆ«åˆ—è¡¨
- **ç›¸å¯¹è·¯å¾„å¤„ç†**ï¼šæ”¹è¿›ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„çš„åˆ¤æ–­é€»è¾‘

### ðŸ“š æ–‡æ¡£æ›´æ–°

#### README å…¨é¢é‡å†™
- **è‹±æ–‡ç‰ˆ README.md**ï¼šå…¨æ–°çš„ç»“æž„å’Œå†…å®¹ç»„ç»‡
- **ä¸­æ–‡ç‰ˆ README_cn.md**ï¼šåŒæ­¥æ›´æ–°ï¼Œä¿æŒä¸€è‡´æ€§
- **å¹¶è¡Œæµ‹è¯•æŒ‡å—**ï¼šè¯¦ç»†çš„ PARALLEL_TESTING_GUIDE.md

#### æ–°å¢žç¤ºä¾‹
- **parallel_example.py**ï¼šæ€§èƒ½æ¯”è¾ƒæ¼”ç¤º
- **performance_test.py**ï¼šè‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•
- **test_parallel_runner.py**ï¼šå¹¶è¡ŒåŠŸèƒ½å•å…ƒæµ‹è¯•

### ðŸŽ¯ æ€§èƒ½åŸºå‡†

| æµ‹è¯•åœºæ™¯ | é¡ºåºæ‰§è¡Œ | å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹) | å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹) | åŠ é€Ÿæ¯” |
|----------|----------|----------------|----------------|--------|
| 6ä¸ªç®€å•æµ‹è¯• | 0.12ç§’ | 0.03ç§’ | 0.16ç§’ | 3.84x |
| 10ä¸ªI/Oæµ‹è¯• | 5.2ç§’ | 1.4ç§’ | 2.1ç§’ | 3.7x |
| 20ä¸ªCPUæµ‹è¯• | 12.8ç§’ | 8.9ç§’ | 6.2ç§’ | 2.1x |

### ðŸ”„ å‘åŽå…¼å®¹æ€§

- **å®Œå…¨å…¼å®¹**ï¼šçŽ°æœ‰çš„ JSONRunner ä»£ç æ— éœ€ä¿®æ”¹
- **æ¸è¿›å¼å‡çº§**ï¼šå¯ä»¥é€æ­¥è¿ç§»åˆ°å¹¶è¡Œæ‰§è¡Œ
- **é…ç½®å…¼å®¹**ï¼šçŽ°æœ‰æµ‹è¯•ç”¨ä¾‹é…ç½®æ–‡ä»¶æ— éœ€æ›´æ”¹

### ðŸ“¦ ä¾èµ–æ›´æ–°

```txt
pytest
PyYAML  # ä¿®æ­£äº†ä¹‹å‰çš„ pyyaml æ‹¼å†™é”™è¯¯
concurrent.futures  # æ ‡å‡†åº“ï¼Œæ— éœ€é¢å¤–å®‰è£…
```

### ðŸš€ ä½¿ç”¨ç¤ºä¾‹

#### åŸºæœ¬å¹¶è¡Œæ‰§è¡Œ
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
success = runner.run_tests()
```

#### æ€§èƒ½æ¯”è¾ƒ
```bash
# è¿è¡Œæ€§èƒ½æ¯”è¾ƒç¤ºä¾‹
python parallel_example.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# é¡ºåºæ‰§è¡Œæ—¶é—´:     0.12 ç§’
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): 0.03 ç§’ (åŠ é€Ÿæ¯”: 3.84x)
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): 0.16 ç§’ (åŠ é€Ÿæ¯”: 0.73x)
```

### ðŸ”® æœªæ¥è®¡åˆ’

- **åˆ†å¸ƒå¼æ‰§è¡Œ**ï¼šæ”¯æŒè·¨æœºå™¨çš„æµ‹è¯•æ‰§è¡Œ
- **Web UI**ï¼šæä¾›å›¾å½¢åŒ–çš„æµ‹è¯•ç®¡ç†ç•Œé¢
- **æ›´å¤šæ ¼å¼æ”¯æŒ**ï¼šXMLã€TOML ç­‰é…ç½®æ ¼å¼
- **æ€§èƒ½ç›‘æŽ§**ï¼šè¯¦ç»†çš„æ‰§è¡Œæ—¶é—´å’Œèµ„æºä½¿ç”¨ç»Ÿè®¡

---

## [v1.0.0] - 2025-03-05

### åˆå§‹ç‰ˆæœ¬
- åŸºç¡€çš„ JSON/YAML æµ‹è¯•è¿è¡Œå™¨
- è·¯å¾„è§£æžåŠŸèƒ½
- æ–­è¨€æœºåˆ¶
- æŠ¥å‘Šç”Ÿæˆ

================
File: docs/user_manual.md
================
# CLI Testing Framework User Manual

## Table of Contents
1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Basic Usage](#basic-usage)
4. [Test Case Definition](#test-case-definition)
5. [Parallel Testing](#parallel-testing)
6. [File Comparison](#file-comparison)
7. [Advanced Features](#advanced-features)
8. [Troubleshooting](#troubleshooting)
9. [API Reference](#api-reference)
10. [Examples](#examples)

## Introduction

The CLI Testing Framework is a powerful tool designed for testing command-line applications and scripts. It provides a structured way to define, execute, and verify test cases, with support for parallel execution and advanced file comparison capabilities.

### Key Features
- Parallel test execution with thread and process support
- JSON/YAML test case definition
- Advanced file comparison capabilities
- Comprehensive reporting
- Extensible architecture

## Installation

### Prerequisites
- Python 3.6 or higher
- pip package manager

### Basic Installation
```bash
pip install cli-test-framework
```

### Development Installation
```bash
git clone https://github.com/yourusername/cli-test-framework.git
cd cli-test-framework
pip install -e .
```

## Basic Usage

### Creating a Test Case

1. Create a JSON test case file (e.g., `test_cases.json`):
```json
{
    "test_cases": [
        {
            "name": "Basic Command Test",
            "command": "echo",
            "args": ["Hello, World!"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Hello, World!"]
            }
        }
    ]
}
```

2. Run the test:
```python
from cli_test_framework.runners import JSONRunner

runner = JSONRunner(
    config_file="test_cases.json",
    workspace="/path/to/workspace"
)
success = runner.run_tests()
```

### Using the Command Line

```bash
# Run tests from a JSON file
cli-test run test_cases.json

# Run tests in parallel
cli-test run test_cases.json --parallel --workers 4
```

## Test Case Definition

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "Test Case Name",
            "command": "command_to_execute",
            "args": ["arg1", "arg2"],
            "expected": {
                "return_code": 0,
                "output_contains": ["expected text"],
                "output_matches": [".*regex pattern.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Test Case Name
    command: command_to_execute
    args:
      - arg1
      - arg2
    expected:
      return_code: 0
      output_contains:
        - expected text
      output_matches:
        - ".*regex pattern.*"
```

## Parallel Testing

### Thread Mode
```python
from cli_test_framework.runners import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
success = runner.run_tests()
```

### Process Mode
```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=2,
    execution_mode="process"
)
success = runner.run_tests()
```

## File Comparison

### Basic File Comparison
```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare with specific options
compare-files file1.txt file2.txt --start-line 10 --end-line 20
```

### JSON File Comparison
```bash
# Exact comparison
compare-files data1.json data2.json

# Key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id
```

### HDF5 File Comparison
```bash
# Compare specific tables
compare-files data1.h5 data2.h5 --h5-table table1,table2

# Compare with numerical tolerance
compare-files data1.h5 data2.h5 --h5-rtol 1e-5 --h5-atol 1e-8
```

### Binary File Comparison
```bash
# Compare with similarity check
compare-files binary1.bin binary2.bin --similarity

# Compare with custom chunk size
compare-files binary1.bin binary2.bin --chunk-size 16384
```

## Advanced Features

### Custom Assertions
```python
from cli_test_framework.assertions import BaseAssertion

class CustomAssertion(BaseAssertion):
    def assert_custom_condition(self, actual, expected):
        if not self._check_custom_condition(actual, expected):
            raise AssertionError("Custom condition not met")
```

### Custom Runners
```python
from cli_test_framework.runners import BaseRunner

class CustomRunner(BaseRunner):
    def load_test_cases(self):
        # Custom test case loading logic
        pass

    def run_test(self, test_case):
        # Custom test execution logic
        pass
```

### Output Formats
```python
# JSON output
runner = JSONRunner(config_file="test_cases.json", output_format="json")

# HTML output
runner = JSONRunner(config_file="test_cases.json", output_format="html")
```

## Troubleshooting

### Common Issues

1. **Command Not Found**
   - Ensure the command is in the system PATH
   - Use absolute paths for scripts
   - Check command permissions

2. **Parallel Execution Issues**
   - Reduce number of workers
   - Check for resource conflicts
   - Use process mode for CPU-intensive tests

3. **File Comparison Issues**
   - Verify file permissions
   - Check file encoding
   - Ensure sufficient memory for large files

### Debug Mode
```python
runner = JSONRunner(
    config_file="test_cases.json",
    debug=True
)
```

## API Reference

### Core Classes

#### JSONRunner
```python
class JSONRunner:
    def __init__(self, config_file, workspace=None, debug=False):
        """
        Initialize JSONRunner
        :param config_file: Path to JSON test case file
        :param workspace: Working directory for test execution
        :param debug: Enable debug mode
        """
```

#### ParallelJSONRunner
```python
class ParallelJSONRunner:
    def __init__(self, config_file, max_workers=None, execution_mode="thread"):
        """
        Initialize ParallelJSONRunner
        :param config_file: Path to JSON test case file
        :param max_workers: Maximum number of parallel workers
        :param execution_mode: "thread" or "process"
        """
```

### File Comparison

#### ComparatorFactory
```python
class ComparatorFactory:
    @staticmethod
    def create_comparator(file_type, **kwargs):
        """
        Create a comparator instance
        :param file_type: Type of file to compare
        :param kwargs: Additional comparator options
        :return: Comparator instance
        """
```

## Examples

### Complete Test Suite
```python
from cli_test_framework.runners import JSONRunner
from cli_test_framework.assertions import Assertions

# Create test runner
runner = JSONRunner(
    config_file="test_suite.json",
    workspace="/project/root",
    debug=True
)

# Run tests
success = runner.run_tests()

# Process results
if success:
    print("All tests passed!")
else:
    print("Some tests failed:")
    for result in runner.results["details"]:
        if result["status"] == "failed":
            print(f"- {result['name']}: {result['message']}")
```

### Parallel Test Suite
```python
from cli_test_framework.runners import ParallelJSONRunner
import os

# Create parallel runner
runner = ParallelJSONRunner(
    config_file="test_suite.json",
    max_workers=os.cpu_count() * 2,
    execution_mode="thread"
)

# Run tests in parallel
success = runner.run_tests()

# Generate report
runner.generate_report("test_report.html")
```

### File Comparison Suite
```python
from cli_test_framework.file_comparator import ComparatorFactory

# Compare text files
text_comparator = ComparatorFactory.create_comparator(
    "text",
    encoding="utf-8",
    verbose=True
)
text_result = text_comparator.compare_files("file1.txt", "file2.txt")

# Compare JSON files
json_comparator = ComparatorFactory.create_comparator(
    "json",
    compare_mode="key-based",
    key_field="id"
)
json_result = json_comparator.compare_files("data1.json", "data2.json")
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

================
File: Example_usage.py
================
# Example usage
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator
import sys

def main():
    runner = JSONRunner(config_file="D:/Document/xcode/Compare-File-Tool/test_script/test_cases.json", workspace="D:/Document/xcode/Compare-File-Tool")
    success = runner.run_tests()
    
    # Generate and save the report
    report_generator = ReportGenerator(runner.results, "D:/Document/xcode/Compare-File-Tool/test_script/test_report.txt")
    report_generator.print_report()
    report_generator.save_report()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: MANIFEST.in
================
include README.md
include LICENSE
include CHANGELOG.md
include CONTRIBUTING.md
recursive-include docs *
recursive-include tests *

================
File: parallel_example.py
================
# å¹¶è¡Œæµ‹è¯•ç¤ºä¾‹ç”¨æ³•
from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner
from src.utils.report_generator import ReportGenerator
import sys
import time

def run_sequential_test():
    """è¿è¡Œé¡ºåºæµ‹è¯•"""
    print("=" * 60)
    print("è¿è¡Œé¡ºåºæµ‹è¯•...")
    print("=" * 60)
    
    start_time = time.time()
    runner = JSONRunner(config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", workspace=".")
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\né¡ºåºæµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
    return success, runner.results, end_time - start_time

def run_parallel_test(max_workers=None, execution_mode="thread"):
    """è¿è¡Œå¹¶è¡Œæµ‹è¯•"""
    print("=" * 60)
    print(f"è¿è¡Œå¹¶è¡Œæµ‹è¯• (æ¨¡å¼: {execution_mode}, å·¥ä½œçº¿ç¨‹: {max_workers or 'auto'})...")
    print("=" * 60)
    
    start_time = time.time()
    runner = ParallelJSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", 
        workspace=".",
        max_workers=max_workers,
        execution_mode=execution_mode
    )
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\nå¹¶è¡Œæµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
    return success, runner.results, end_time - start_time

def main():
    """ä¸»å‡½æ•°ï¼šæ¯”è¾ƒé¡ºåºå’Œå¹¶è¡Œæµ‹è¯•çš„æ€§èƒ½"""
    
    # è¿è¡Œé¡ºåºæµ‹è¯•
    seq_success, seq_results, seq_time = run_sequential_test()
    
    # è¿è¡Œå¹¶è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼‰
    par_success, par_results, par_time = run_parallel_test(max_workers=4, execution_mode="thread")
    
    # è¿è¡Œå¹¶è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼‰
    proc_success, proc_results, proc_time = run_parallel_test(max_workers=2, execution_mode="process")
    
    # æ€§èƒ½æ¯”è¾ƒ
    print("\n" + "=" * 60)
    print("æ€§èƒ½æ¯”è¾ƒç»“æžœ:")
    print("=" * 60)
    print(f"é¡ºåºæ‰§è¡Œæ—¶é—´:     {seq_time:.2f} ç§’")
    print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): {par_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x)")
    print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): {proc_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/proc_time:.2f}x)")
    
    # ç”ŸæˆæŠ¥å‘Š
    if par_success:
        report_generator = ReportGenerator(par_results, "parallel_test_report.txt")
        report_generator.print_report()
        report_generator.save_report()
        print(f"\nå¹¶è¡Œæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: parallel_test_report.txt")
    
    # è¿”å›žæœ€ç»ˆç»“æžœ
    return seq_success and par_success and proc_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

================
File: PARALLEL_TESTING_GUIDE.md
================
# å¹¶è¡Œæµ‹è¯•åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

ä½ çš„æµ‹è¯•æ¡†æž¶çŽ°åœ¨æ”¯æŒå¹¶è¡Œæµ‹è¯•æ‰§è¡Œï¼Œå¯ä»¥æ˜¾è‘—æå‡æµ‹è¯•æ‰§è¡Œæ•ˆçŽ‡ã€‚æ¡†æž¶æä¾›äº†ä¸¤ç§å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼š**çº¿ç¨‹æ¨¡å¼**å’Œ**è¿›ç¨‹æ¨¡å¼**ã€‚

## åŠŸèƒ½ç‰¹æ€§

### âœ… å·²å®žçŽ°çš„åŠŸèƒ½

- **å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šé€‚ç”¨äºŽI/Oå¯†é›†åž‹æµ‹è¯•
- **å¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šé€‚ç”¨äºŽCPUå¯†é›†åž‹æµ‹è¯•ï¼Œæä¾›å®Œå…¨éš”ç¦»çš„æ‰§è¡ŒçŽ¯å¢ƒ
- **å¯é…ç½®å¹¶å‘æ•°**ï¼šæ”¯æŒè‡ªå®šä¹‰æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°
- **çº¿ç¨‹å®‰å…¨è®¾è®¡**ï¼šç¡®ä¿æµ‹è¯•ç»“æžœçš„æ­£ç¡®æ€§å’Œè¾“å‡ºçš„æ¸…æ™°æ€§
- **æ€§èƒ½ç›‘æŽ§**ï¼šæä¾›æ‰§è¡Œæ—¶é—´ç»Ÿè®¡å’ŒåŠ é€Ÿæ¯”åˆ†æž
- **å‘åŽå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹çŽ°æœ‰çš„é¡ºåºæ‰§è¡Œä»£ç 

### ðŸ“Š æ€§èƒ½æå‡

æ ¹æ®æµ‹è¯•ç»“æžœï¼Œå¹¶è¡Œæ‰§è¡Œå¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

- **çº¿ç¨‹æ¨¡å¼**ï¼šé€šå¸¸å¯èŽ·å¾— **2-4å€** çš„åŠ é€Ÿæ¯”
- **è¿›ç¨‹æ¨¡å¼**ï¼šé€‚åˆéœ€è¦å®Œå…¨éš”ç¦»çš„åœºæ™¯ï¼Œä½†å¯åŠ¨å¼€é”€è¾ƒå¤§

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# åˆ›å»ºå¹¶è¡Œè¿è¡Œå™¨
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # æœ€å¤§å¹¶å‘æ•°
    execution_mode="thread"  # æ‰§è¡Œæ¨¡å¼ï¼šthread æˆ– process
)

# è¿è¡Œæµ‹è¯•
success = runner.run_tests()
```

### é…ç½®é€‰é¡¹

| å‚æ•° | ç±»åž‹ | é»˜è®¤å€¼ | è¯´æ˜Ž |
|------|------|--------|------|
| `config_file` | str | "test_cases.json" | æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„ |
| `workspace` | str | None | å·¥ä½œç›®å½• |
| `max_workers` | int | None (è‡ªåŠ¨) | æœ€å¤§å¹¶å‘æ•° |
| `execution_mode` | str | "thread" | æ‰§è¡Œæ¨¡å¼ï¼šthread/process |

### æ‰§è¡Œæ¨¡å¼é€‰æ‹©

#### çº¿ç¨‹æ¨¡å¼ (thread)
- **é€‚ç”¨åœºæ™¯**ï¼šI/Oå¯†é›†åž‹æµ‹è¯•ï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶æ“ä½œï¼‰
- **ä¼˜åŠ¿**ï¼šå¯åŠ¨å¿«ï¼Œå†…å­˜å…±äº«ï¼Œé€‚åˆå¤§å¤šæ•°æµ‹è¯•åœºæ™¯
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•° Ã— 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### è¿›ç¨‹æ¨¡å¼ (process)
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å®Œå…¨éš”ç¦»çš„æµ‹è¯•ï¼ŒCPUå¯†é›†åž‹ä»»åŠ¡
- **ä¼˜åŠ¿**ï¼šå®Œå…¨éš”ç¦»ï¼Œé¿å…GILé™åˆ¶
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•°

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json", 
    max_workers=2,
    execution_mode="process"
)
```

## ç¤ºä¾‹ä»£ç 

### æ€§èƒ½æ¯”è¾ƒç¤ºä¾‹

```python
# è¿è¡Œæ€§èƒ½æ¯”è¾ƒ
python parallel_example.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# é¡ºåºæ‰§è¡Œæ—¶é—´:     0.12 ç§’
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): 0.03 ç§’ (åŠ é€Ÿæ¯”: 3.58x)
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): 0.10 ç§’ (åŠ é€Ÿæ¯”: 1.15x)
```

### å¿«é€ŸéªŒè¯

```python
# å¿«é€ŸéªŒè¯å¹¶è¡ŒåŠŸèƒ½
python test_parallel_simple.py

# å¿«é€Ÿæ€§èƒ½æµ‹è¯•
python performance_test.py
```

## æœ€ä½³å®žè·µ

### 1. é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°

```python
import os

# CPUå¯†é›†åž‹ä»»åŠ¡
max_workers = os.cpu_count()

# I/Oå¯†é›†åž‹ä»»åŠ¡  
max_workers = os.cpu_count() * 2
```

### 2. æµ‹è¯•ç”¨ä¾‹è®¾è®¡åŽŸåˆ™

- âœ… **ç¡®ä¿æµ‹è¯•ç‹¬ç«‹æ€§**ï¼šæµ‹è¯•ç”¨ä¾‹ä¹‹é—´ä¸åº”æœ‰ä¾èµ–å…³ç³»
- âœ… **é¿å…å…±äº«èµ„æºå†²çª**ï¼šä¸åŒæµ‹è¯•ä¸åº”æ“ä½œç›¸åŒçš„æ–‡ä»¶æˆ–ç«¯å£
- âœ… **ä½¿ç”¨ç›¸å¯¹è·¯å¾„**ï¼šæ¡†æž¶ä¼šè‡ªåŠ¨å¤„ç†è·¯å¾„è§£æž

### 3. é”™è¯¯å¤„ç†

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"å¤±è´¥çš„æµ‹è¯•: {detail['name']}")
                print(f"é”™è¯¯ä¿¡æ¯: {detail['message']}")
                
except Exception as e:
    print(f"æ‰§è¡Œå‡ºé”™: {e}")
    # å›žé€€åˆ°é¡ºåºæ‰§è¡Œ
    runner.run_tests_sequential()
```

## æŠ€æœ¯å®žçŽ°

### æž¶æž„è®¾è®¡

```
ParallelRunner (åŸºç±»)
â”œâ”€â”€ çº¿ç¨‹å®‰å…¨çš„ç»“æžœæ”¶é›†
â”œâ”€â”€ å¯é…ç½®çš„æ‰§è¡Œæ¨¡å¼
â””â”€â”€ å¼‚å¸¸å¤„ç†æœºåˆ¶

ParallelJSONRunner (å®žçŽ°ç±»)
â”œâ”€â”€ ç»§æ‰¿ ParallelRunner
â”œâ”€â”€ JSONé…ç½®è§£æž
â””â”€â”€ è·¯å¾„è§£æžåŠŸèƒ½

è¿›ç¨‹å·¥ä½œå™¨ (process_worker.py)
â”œâ”€â”€ ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œ
â”œâ”€â”€ é¿å…åºåˆ—åŒ–é—®é¢˜
â””â”€â”€ å®Œå…¨éš”ç¦»çŽ¯å¢ƒ
```

### çº¿ç¨‹å®‰å…¨æœºåˆ¶

- **ç»“æžœæ”¶é›†é”**ï¼š`threading.Lock()` ä¿æŠ¤å…±äº«ç»“æžœæ•°æ®
- **è¾“å‡ºæŽ§åˆ¶é”**ï¼šé¿å…å¹¶å‘è¾“å‡ºæ··ä¹±
- **å¼‚å¸¸éš”ç¦»**ï¼šå•ä¸ªæµ‹è¯•å¤±è´¥ä¸å½±å“å…¶ä»–æµ‹è¯•

## æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿›ç¨‹æ¨¡å¼åºåˆ—åŒ–é”™è¯¯**
   - åŽŸå› ï¼šå¯¹è±¡åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å±žæ€§ï¼ˆå¦‚é”ï¼‰
   - è§£å†³ï¼šä½¿ç”¨ç‹¬ç«‹çš„è¿›ç¨‹å·¥ä½œå™¨å‡½æ•°

2. **è·¯å¾„è§£æžé”™è¯¯**
   - åŽŸå› ï¼šç³»ç»Ÿå‘½ä»¤è¢«å½“ä½œç›¸å¯¹è·¯å¾„å¤„ç†
   - è§£å†³ï¼šæ›´æ–° `PathResolver` çš„ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨

3. **æ€§èƒ½æå‡ä¸æ˜Žæ˜¾**
   - åŽŸå› ï¼šæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ—¶é—´å¤ªçŸ­ï¼Œå¹¶è¡Œå¼€é”€å¤§äºŽæ”¶ç›Š
   - è§£å†³ï¼šå¢žåŠ æµ‹è¯•ç”¨ä¾‹æ•°é‡æˆ–ä½¿ç”¨æ›´å¤æ‚çš„æµ‹è¯•

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†è¾“å‡º
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=1,  # è®¾ä¸º1ä¾¿äºŽè°ƒè¯•
    execution_mode="thread"
)

# æŸ¥çœ‹è¯¦ç»†ç»“æžœ
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## ç‰ˆæœ¬å…¼å®¹æ€§

- **Pythonç‰ˆæœ¬**ï¼š3.6+
- **ä¾èµ–é¡¹**ï¼šæ— é¢å¤–ä¾èµ–ï¼Œä½¿ç”¨æ ‡å‡†åº“
- **å‘åŽå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹çŽ°æœ‰çš„ `JSONRunner` ä»£ç 

## æ€»ç»“

å¹¶è¡Œæµ‹è¯•åŠŸèƒ½ä¸ºä½ çš„æµ‹è¯•æ¡†æž¶å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«é€‚åˆï¼š

- ðŸš€ **å¤§è§„æ¨¡æµ‹è¯•å¥—ä»¶**ï¼šæ•°åä¸ªæˆ–æ•°ç™¾ä¸ªæµ‹è¯•ç”¨ä¾‹
- ðŸŒ **I/Oå¯†é›†åž‹æµ‹è¯•**ï¼šç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶æ“ä½œç­‰
- âš¡ **CI/CDæµæ°´çº¿**ï¼šç¼©çŸ­æž„å»ºæ—¶é—´
- ðŸ”„ **å›žå½’æµ‹è¯•**ï¼šå¿«é€ŸéªŒè¯ä»£ç å˜æ›´

é€šè¿‡åˆç†é…ç½®å¹¶å‘å‚æ•°å’Œé€‰æ‹©é€‚å½“çš„æ‰§è¡Œæ¨¡å¼ï¼Œä½ å¯ä»¥åœ¨ä¿è¯æµ‹è¯•å¯é æ€§çš„åŒæ—¶ï¼Œå¤§å¹…æå‡æµ‹è¯•æ‰§è¡Œæ•ˆçŽ‡ï¼

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

================
File: README_cn.md
================
# CLI æµ‹è¯•æ¡†æž¶

## 1. æ¦‚è¿°
è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æž¶ï¼Œæ”¯æŒé€šè¿‡JSON/YAMLæ ¼å¼å®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼Œæä¾›å®Œæ•´çš„æµ‹è¯•æ‰§è¡Œã€ç»“æžœéªŒè¯å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚æ¡†æž¶ä¸“ä¸ºå‘½ä»¤è¡Œå·¥å…·å’Œè„šæœ¬æä¾›æ ‡å‡†åŒ–æµ‹è¯•ç®¡ç†ï¼Œå…·å¤‡ä¼ä¸šçº§å¹¶è¡Œæ‰§è¡Œæ”¯æŒã€‚

## 2. åŠŸèƒ½ç‰¹ç‚¹
- **ðŸš€ å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ**ï¼šæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹å¹¶è¡Œæµ‹è¯•ï¼Œæ˜¾è‘—æå‡æ‰§è¡Œæ•ˆçŽ‡
- **ðŸ—ï¸ æ¨¡å—åŒ–æž¶æž„**ï¼šæ ¸å¿ƒç»„ä»¶è§£è€¦è®¾è®¡ï¼ˆè¿è¡Œå™¨/æ–­è¨€/æŠ¥å‘Šï¼‰
- **ðŸ“„ å¤šæ ¼å¼æ”¯æŒ**ï¼šåŽŸç”Ÿæ”¯æŒJSON/YAMLæµ‹è¯•ç”¨ä¾‹æ ¼å¼
- **ðŸ§  æ™ºèƒ½å‘½ä»¤è§£æž**ï¼šæ™ºèƒ½å¤„ç†å¤æ‚å‘½ä»¤å¦‚ `"python ./script.py"`
- **ðŸ“ æ™ºèƒ½è·¯å¾„è§£æž**ï¼šè‡ªåŠ¨å¤„ç†ç›¸å¯¹è·¯å¾„ä¸Žç»å¯¹è·¯å¾„è½¬æ¢
- **âœ… ä¸°å¯Œæ–­è¨€æœºåˆ¶**ï¼šè¿”å›žå€¼æ ¡éªŒã€è¾“å‡ºå†…å®¹åŒ¹é…ã€æ­£åˆ™è¡¨è¾¾å¼éªŒè¯
- **ðŸ”Œ å¯æ‰©å±•æŽ¥å£**ï¼šé€šè¿‡ç»§æ‰¿BaseRunnerå¯å¿«é€Ÿå®žçŽ°æ–°æµ‹è¯•æ ¼å¼æ”¯æŒ
- **ðŸ”’ æ‰§è¡ŒçŽ¯å¢ƒéš”ç¦»**ï¼šç‹¬ç«‹å­è¿›ç¨‹è¿è¡Œä¿è¯æµ‹è¯•éš”ç¦»æ€§
- **ðŸ“Š å…¨é¢æŠ¥å‘Š**ï¼šè¯¦ç»†çš„é€šè¿‡çŽ‡ç»Ÿè®¡å’Œå¤±è´¥è¯Šæ–­
- **ðŸ”§ çº¿ç¨‹å®‰å…¨è®¾è®¡**ï¼šç¨³å¥çš„å¹¶å‘æ‰§è¡Œå’Œé€‚å½“çš„åŒæ­¥æœºåˆ¶

## 3. ä½¿ç”¨è¯´æ˜Ž

### çŽ¯å¢ƒè¦æ±‚
```bash
pip install -r requirements.txt
Python >= 3.6
```

### å¿«é€Ÿå¼€å§‹

1. åˆ›å»ºæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ï¼ˆç¤ºä¾‹è§`tests/fixtures/`ï¼‰
2. ç¼–å†™æ‰§è¡Œè„šæœ¬ï¼š

**é¡ºåºæ‰§è¡Œ**ï¼š
```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

**å¹¶è¡Œæ‰§è¡Œï¼ˆæ–°åŠŸèƒ½ï¼ï¼‰**ï¼š
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# å¤šçº¿ç¨‹æ‰§è¡Œï¼ˆæŽ¨èç”¨äºŽI/Oå¯†é›†åž‹æµ‹è¯•ï¼‰
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # æœ€å¤§å¹¶å‘æ•°
    execution_mode="thread"  # æ‰§è¡Œæ¨¡å¼ï¼šthread æˆ– process
)
success = runner.run_tests()

# æ€§èƒ½æ¯”è¾ƒç¤ºä¾‹
python parallel_example.py
```

**YAMLæ”¯æŒ**ï¼š
```python
from src.runners.yaml_runner import YAMLRunner

runner = YAMLRunner(
    config_file="path/to/test_cases.yaml",
    workspace="/project/root"
)
success = runner.run_tests()
```

## 4. æµ‹è¯•ç”¨ä¾‹æ ¼å¼

### JSONæ ¼å¼

```json
{
  "test_cases": [
    {
      "name": "æ–‡ä»¶å¤„ç†æµ‹è¯•",
      "command": "python ./process_files.py",
      "args": ["file1.txt", "file2.txt", "--verbose"],
      "expected": {
        "return_code": 0,
        "output_contains": ["æ–‡ä»¶å¤„ç†å®Œæˆ"],
        "output_matches": [".*æ¯”è¾ƒå®Œæˆ.*"]
      }
    }
  ]
}
```

### YAMLæ ¼å¼

```yaml
test_cases:
  - name: ç›®å½•æ‰«ææµ‹è¯•
    command: ls
    args: 
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

### æ”¯æŒçš„å‘½ä»¤æ ¼å¼

æ¡†æž¶æ™ºèƒ½å¤„ç†å„ç§å‘½ä»¤æ ¼å¼ï¼š

```json
{
    "command": "echo",                    // ç®€å•å‘½ä»¤
    "command": "python script.py",       // å‘½ä»¤+è„šæœ¬
    "command": "node ./app.js --port",   // å¤æ‚å‘½ä»¤+å‚æ•°
}
```

## 5. å¹¶è¡Œæµ‹è¯•åŠŸèƒ½

### æ€§èƒ½ä¼˜åŠ¿

å¹¶è¡Œæ‰§è¡Œå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

```bash
# è¿è¡Œæ€§èƒ½æ¯”è¾ƒ
python parallel_example.py

# å…¸åž‹è¾“å‡ºï¼š
# é¡ºåºæ‰§è¡Œæ—¶é—´:     12.45 ç§’
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): 3.21 ç§’ (3.88x åŠ é€Ÿ)
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): 4.12 ç§’ (3.02x åŠ é€Ÿ)
```

### æ‰§è¡Œæ¨¡å¼

#### çº¿ç¨‹æ¨¡å¼ï¼ˆæŽ¨èï¼‰
- **é€‚ç”¨åœºæ™¯**ï¼šI/Oå¯†é›†åž‹æµ‹è¯•ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶æ“ä½œï¼‰
- **ä¼˜åŠ¿**ï¼šå¯åŠ¨å¿«ï¼Œå†…å­˜å…±äº«ï¼Œé€‚åˆå¤§å¤šæ•°æµ‹è¯•åœºæ™¯
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•° Ã— 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### è¿›ç¨‹æ¨¡å¼
- **é€‚ç”¨åœºæ™¯**ï¼šCPUå¯†é›†åž‹æµ‹è¯•ï¼Œéœ€è¦å®Œå…¨éš”ç¦»çš„åœºæ™¯
- **ä¼˜åŠ¿**ï¼šå®Œå…¨éš”ç¦»ï¼Œç»•è¿‡GILé™åˆ¶
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•°

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=2,
    execution_mode="process"
)
```

### çº¿ç¨‹å®‰å…¨ç‰¹æ€§

- **ç»“æžœæ”¶é›†é”**ï¼š`threading.Lock()` ä¿æŠ¤å…±äº«ç»“æžœæ•°æ®
- **è¾“å‡ºæŽ§åˆ¶é”**ï¼šé˜²æ­¢å¹¶å‘è¾“å‡ºæ··ä¹±
- **å¼‚å¸¸éš”ç¦»**ï¼šå•ä¸ªæµ‹è¯•å¤±è´¥ä¸å½±å“å…¶ä»–æµ‹è¯•

## 6. ç³»ç»Ÿæž¶æž„

### å¢žå¼ºçš„æž¶æž„æµç¨‹

```mermaid
graph TD
    A[æµ‹è¯•ç”¨ä¾‹] --> B{æ‰§è¡Œæ¨¡å¼}
    B -->|é¡ºåº| C[JSONRunner/YAMLRunner]
    B -->|å¹¶è¡Œ| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[å‘½ä»¤è§£æžå™¨]
    E --> F
    F --> G[è·¯å¾„è§£æžå™¨]
    G --> H[å­è¿›ç¨‹æ‰§è¡Œ]
    H --> I[æ–­è¨€å¼•æ“Ž]
    I --> J[çº¿ç¨‹å®‰å…¨ç»“æžœæ”¶é›†]
    J --> K[æŠ¥å‘Šç”Ÿæˆå™¨]
```

### æ ¸å¿ƒç»„ä»¶

#### 1. æ™ºèƒ½å‘½ä»¤è§£æžå™¨
```python
# å¤„ç†å¤æ‚å‘½ä»¤å¦‚ "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> å®Œæ•´è·¯å¾„
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. å¢žå¼ºçš„è·¯å¾„è§£æžå™¨
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. å¹¶è¡Œè¿è¡Œå™¨åŸºç±»
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. é«˜çº§ç”¨æ³•

### æ€§èƒ½æµ‹è¯•

```python
# å¿«é€Ÿæ€§èƒ½æµ‹è¯•
python performance_test.py

# å¹¶è¡ŒåŠŸèƒ½å•å…ƒæµ‹è¯•
python -m pytest tests/test_parallel_runner.py -v
```

### é”™è¯¯å¤„ç†å’Œå›žé€€

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"å¤±è´¥çš„æµ‹è¯•: {detail['name']}")
                print(f"é”™è¯¯ä¿¡æ¯: {detail['message']}")
                
except Exception as e:
    print(f"æ‰§è¡Œå‡ºé”™: {e}")
    # å›žé€€åˆ°é¡ºåºæ‰§è¡Œ
    runner.run_tests_sequential()
```

### æœ€ä½³å®žè·µ

1. **é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°**ï¼š
   ```python
   import os
   
   # CPUå¯†é›†åž‹ä»»åŠ¡
   max_workers = os.cpu_count()
   
   # I/Oå¯†é›†åž‹ä»»åŠ¡
   max_workers = os.cpu_count() * 2
   ```

2. **æµ‹è¯•ç”¨ä¾‹è®¾è®¡**ï¼š
   - âœ… ç¡®ä¿æµ‹è¯•ç‹¬ç«‹æ€§ï¼ˆæµ‹è¯•é—´æ— ä¾èµ–å…³ç³»ï¼‰
   - âœ… é¿å…å…±äº«èµ„æºå†²çªï¼ˆä¸åŒæ–‡ä»¶/ç«¯å£ï¼‰
   - âœ… ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆæ¡†æž¶è‡ªåŠ¨å¤„ç†è§£æžï¼‰

3. **è°ƒè¯•æŠ€å·§**ï¼š
   ```python
   # å¯ç”¨è¯¦ç»†è¾“å‡ºä¾¿äºŽè°ƒè¯•
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # è®¾ä¸º1ä¾¿äºŽè°ƒè¯•
       execution_mode="thread"
   )
   ```

## 8. ç³»ç»Ÿæµç¨‹

### æž¶æž„æ¨¡å—

```mermaid
graph TD
    A[æµ‹è¯•ç”¨ä¾‹] --> B[Runner]
    B --> C[è·¯å¾„è§£æž]
    B --> D[å­è¿›ç¨‹æ‰§è¡Œ]
    D --> E[æ–­è¨€éªŒè¯]
    E --> F[ç»“æžœæ”¶é›†]
    F --> G[æŠ¥å‘Šç”Ÿæˆ]
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜Ž

1. **Test Runner**

   - åŠ è½½æµ‹è¯•é…ç½®
   - ç®¡ç†æµ‹è¯•ç”Ÿå‘½å‘¨æœŸ
   - åè°ƒå„ç»„ä»¶åä½œ

2. **PathResolver**

   ```python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   æ™ºèƒ½å¤„ç†è·¯å¾„å‚æ•°ï¼Œè‡ªåŠ¨å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºåŸºäºŽworkspaceçš„ç»å¯¹è·¯å¾„

3. **Assertion Engine**

   - è¿”å›žå€¼æ ¡éªŒï¼ˆreturn_codeï¼‰
   - è¾“å‡ºå†…å®¹åŒ¹é…ï¼ˆcontains/matchesï¼‰
   - å¼‚å¸¸æ•èŽ·æœºåˆ¶

4. **Report Generator**

   - å®žæ—¶ç»Ÿè®¡æµ‹è¯•è¿›åº¦
   - ç”Ÿæˆå¸¦é”™è¯¯å®šä½çš„è¯¦ç»†æŠ¥å‘Š
   - æ”¯æŒæŽ§åˆ¶å°è¾“å‡ºå’Œæ–‡ä»¶ä¿å­˜



## 9. ç¤ºä¾‹æ¼”ç¤º

### è¾“å…¥æ ·ä¾‹

```json
{
    "test_cases": [
        {
            "name": "Pythonç‰ˆæœ¬æ£€æŸ¥",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "æ–‡ä»¶å¤„ç†æµ‹è¯•",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["å¤„ç†å®Œæˆ"]
            }
        }
    ]
}
```

### è¾“å‡ºæŠ¥å‘Š

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
âœ“ Pythonç‰ˆæœ¬æ£€æŸ¥
âœ“ æ–‡ä»¶å¤„ç†æµ‹è¯•
âœ“ JSONæ¯”è¾ƒæµ‹è¯•
...
```

## 10. æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿›ç¨‹æ¨¡å¼åºåˆ—åŒ–é”™è¯¯**
   - **åŽŸå› **ï¼šå¯¹è±¡åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å±žæ€§ï¼ˆå¦‚é”ï¼‰
   - **è§£å†³**ï¼šä½¿ç”¨ç‹¬ç«‹çš„è¿›ç¨‹å·¥ä½œå™¨å‡½æ•°

2. **è·¯å¾„è§£æžé”™è¯¯**
   - **åŽŸå› **ï¼šç³»ç»Ÿå‘½ä»¤è¢«å½“ä½œç›¸å¯¹è·¯å¾„å¤„ç†
   - **è§£å†³**ï¼šæ›´æ–° `PathResolver` çš„ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨

3. **æ€§èƒ½æå‡ä¸æ˜Žæ˜¾**
   - **åŽŸå› **ï¼šæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ—¶é—´å¤ªçŸ­ï¼Œå¹¶è¡Œå¼€é”€å¤§äºŽæ”¶ç›Š
   - **è§£å†³**ï¼šå¢žåŠ æµ‹è¯•ç”¨ä¾‹æ•°é‡æˆ–ä½¿ç”¨æ›´å¤æ‚çš„æµ‹è¯•

4. **å‘½ä»¤æœªæ‰¾åˆ°é”™è¯¯**
   - **åŽŸå› **ï¼šå¤æ‚å‘½ä»¤å¦‚ `"python ./script.py"` è§£æžä¸æ­£ç¡®
   - **è§£å†³**ï¼šæ¡†æž¶çŽ°å·²è‡ªåŠ¨å¤„ç†æ­¤é—®é¢˜ï¼ˆæœ€æ–°ç‰ˆæœ¬å·²ä¿®å¤ï¼‰

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æŸ¥çœ‹è¯¦ç»†ç»“æžœ
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 11. æ‰©å±•å’Œè‡ªå®šä¹‰

### æ·»åŠ æ–°çš„è¿è¡Œå™¨

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # è§£æžXMLç»“æž„å¹¶è½¬æ¢ä¸ºTestCaseå¯¹è±¡
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # åœ¨æµ‹è¯•æ‰§è¡Œå‰æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        pass
```

### è‡ªå®šä¹‰æ–­è¨€

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"æ‰§è¡Œå¤ªæ…¢: {execution_time}s > {max_time}s")
```

## 12. ç‰ˆæœ¬å…¼å®¹æ€§

- **Pythonç‰ˆæœ¬**ï¼š3.6+
- **ä¾èµ–é¡¹**ï¼šä»…ä½¿ç”¨æ ‡å‡†åº“ï¼ˆæ ¸å¿ƒåŠŸèƒ½æ— å¤–éƒ¨ä¾èµ–ï¼‰
- **å‘åŽå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹çŽ°æœ‰çš„ `JSONRunner` ä»£ç 
- **å¹³å°æ”¯æŒ**ï¼šWindowsã€macOSã€Linux

## 13. æ€§èƒ½åŸºå‡†æµ‹è¯•

| æµ‹è¯•åœºæ™¯ | é¡ºåºæ‰§è¡Œ | å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹) | å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹) | åŠ é€Ÿæ¯” |
|----------|----------|----------------|----------------|--------|
| 10ä¸ªI/Oæµ‹è¯• | 5.2ç§’ | 1.4ç§’ | 2.1ç§’ | 3.7x |
| 20ä¸ªCPUæµ‹è¯• | 12.8ç§’ | 8.9ç§’ | 6.2ç§’ | 2.1x |
| æ··åˆæµ‹è¯• | 8.5ç§’ | 2.3ç§’ | 3.1ç§’ | 3.7x |

## 14. è´¡çŒ®æŒ‡å—

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•
4. ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼š`python -m pytest tests/ -v`
5. æäº¤ Pull Request

## 15. è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ LICENSE æ–‡ä»¶ã€‚

---

**ðŸš€ å‡†å¤‡å¥½ç”¨å¹¶è¡Œæ‰§è¡Œæ¥åŠ é€Ÿä½ çš„æµ‹è¯•å·¥ä½œæµç¨‹å§ï¼**

è¯¦ç»†çš„å¹¶è¡Œæµ‹è¯•æŒ‡å—è¯·å‚è§ï¼š[PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: README.md
================
# CLI Testing Framework

## 1. Overview

This is a lightweight and extensible automated testing framework that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The framework is designed to provide standardized test management for command-line tools and scripts, with enterprise-grade parallel execution support and advanced file comparison features.

## 2. Features

- **ðŸš€ Parallel Test Execution**: Support for multi-threading and multi-processing parallel testing with significant performance improvements
- **ðŸ—ï¸ Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **ðŸ“„ Multi-Format Support**: Native support for JSON/YAML test case formats
- **ðŸ§  Intelligent Command Parsing**: Smart handling of complex commands like `"python ./script.py"`
- **ðŸ“ Smart Path Resolution**: Automatic handling of relative and absolute path conversions
- **âœ… Rich Assertion Mechanism**: Return code validation, output content matching, regex verification
- **ðŸ”Œ Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **ðŸ”’ Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **ðŸ“Š Comprehensive Reports**: Detailed pass rate statistics and failure diagnostics
- **ðŸ”§ Thread-Safe Design**: Robust concurrent execution with proper synchronization
- **ðŸ“ Advanced File Comparison**: Support for comparing various file types (text, binary, JSON, HDF5) with detailed diff output

## 3. Quick Start

### Environment Requirements

```bash
pip install cli-test-framework
Python >= 3.6
```

### Sequential Execution

```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Parallel Execution

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# Multi-threaded execution (recommended for I/O-intensive tests)
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # Maximum concurrent workers
    execution_mode="thread"  # "thread" or "process"
)
success = runner.run_tests()
```

### File Comparison

```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare JSON files with key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id

# Compare HDF5 files with specific options
compare-files data1.h5 data2.h5 --h5-table table1,table2 --h5-rtol 1e-6

# Compare binary files with similarity check
compare-files binary1.bin binary2.bin --similarity
```

## 4. Test Case Format

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "compare-files",
            "args": ["file1.txt", "file2.txt", "--verbose"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Files are identical"],
                "output_matches": [".*comparison completed.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Directory Scan Test
    command: ls
    args:
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

## 5. File Comparison Features

### Supported File Types

- **Text Files**: Plain text, source code, markdown, etc.
- **JSON Files**: With exact or key-based comparison
- **HDF5 Files**: Structure and content comparison with numerical tolerance
- **Binary Files**: With optional similarity index calculation

### Comparison Options

#### Text Comparison
```bash
compare-files file1.txt file2.txt \
    --start-line 10 \
    --end-line 20 \
    --encoding utf-8
```

#### JSON Comparison
```bash
compare-files data1.json data2.json \
    --json-compare-mode key-based \
    --json-key-field id,name
```

#### HDF5 Comparison
```bash
compare-files data1.h5 data2.h5 \
    --h5-table table1,table2 \
    --h5-structure-only \
    --h5-rtol 1e-5 \
    --h5-atol 1e-8
```

#### Binary Comparison
```bash
compare-files binary1.bin binary2.bin \
    --similarity \
    --chunk-size 16384
```

### Output Formats

- **Text**: Human-readable diff output
- **JSON**: Structured comparison results
- **HTML**: Visual diff with syntax highlighting

## 6. System Architecture

### Enhanced Architecture Flow

```mermaid
graph TD
    A[Test Cases] --> B{Execution Mode}
    B -->|Sequential| C[JSONRunner/YAMLRunner]
    B -->|Parallel| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[Command Parser]
    E --> F
    F --> G[Path Resolver]
    G --> H[Sub-process Execution]
    H --> I[Assertion Engine]
    I --> J[Thread-Safe Result Collection]
    J --> K[Report Generator]
    L[File Comparator] --> M[Text Comparator]
    L --> N[JSON Comparator]
    L --> O[HDF5 Comparator]
    L --> P[Binary Comparator]
```

### Core Components

#### 1. Intelligent Command Parser
```python
# Handles complex commands like "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> full path
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. Enhanced Path Resolver
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. Parallel Runner Base Class
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. Advanced Usage

### Performance Testing

```python
# Quick performance test
python performance_test.py

# Unit tests for parallel functionality
python -m pytest tests/test_parallel_runner.py -v
```

### Error Handling and Fallback

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # Check failed tests
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"Failed test: {detail['name']}")
                print(f"Error: {detail['message']}")
                
except Exception as e:
    print(f"Execution error: {e}")
    # Fallback to sequential execution
    runner.run_tests_sequential()
```

### Best Practices

1. **Choose Appropriate Concurrency**:
   ```python
   import os
   
   # For CPU-intensive tasks
   max_workers = os.cpu_count()
   
   # For I/O-intensive tasks
   max_workers = os.cpu_count() * 2
   ```

2. **Test Case Design**:
   - âœ… Ensure test independence (no dependencies between tests)
   - âœ… Avoid shared resource conflicts (different files/ports)
   - âœ… Use relative paths (framework handles resolution automatically)

3. **Debugging**:
   ```python
   # Enable verbose output for debugging
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # Set to 1 for easier debugging
       execution_mode="thread"
   )
   ```

## 8. Example Demonstrations

### Input Example

```json
{
    "test_cases": [
        {
            "name": "Python Version Check",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "File Processing Test",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Processing completed"]
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
âœ“ Python Version Check
âœ“ File Processing Test
âœ“ JSON Comparison Test
...
```

## 9. Troubleshooting

### Common Issues

1. **Process Mode Serialization Error**
   - **Cause**: Objects contain non-serializable attributes (like locks)
   - **Solution**: Use independent process worker functions

2. **Path Resolution Error**
   - **Cause**: System commands treated as relative paths
   - **Solution**: Update `PathResolver` system command list

3. **Performance Not Improved**
   - **Cause**: Test cases too short, parallel overhead exceeds benefits
   - **Solution**: Increase test case count or use more complex tests

4. **Command Not Found Error**
   - **Cause**: Complex commands like `"python ./script.py"` not parsed correctly
   - **Solution**: Framework now automatically handles this (fixed in latest version)

### Debug Tips

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check detailed results
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 10. Extension and Customization

### Adding New Runners

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # Add custom logic before test execution
        pass
```

### Custom Assertions

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"Execution too slow: {execution_time}s > {max_time}s")
```

## 11. Version Compatibility

- **Python Version**: 3.6+
- **Dependencies**: Standard library only (no external dependencies for core functionality)
- **Backward Compatibility**: Fully compatible with existing `JSONRunner` code
- **Platform Support**: Windows, macOS, Linux

## 12. Performance Benchmarks

| Test Scenario | Sequential | Parallel (Thread) | Parallel (Process) | Speedup |
|---------------|------------|-------------------|-------------------|---------|
| 10 I/O tests  | 5.2s       | 1.4s              | 2.1s              | 3.7x    |
| 20 CPU tests  | 12.8s      | 8.9s              | 6.2s              | 2.1x    |
| Mixed tests   | 8.5s       | 2.3s              | 3.1s              | 3.7x    |

## 13. Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass: `python -m pytest tests/ -v`
5. Submit a pull request

## 14. License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**ðŸš€ Ready to supercharge your testing workflow with parallel execution and advanced file comparison!**

For detailed parallel testing guide, see: [PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: requirements.txt
================
pytest
PyYAML
concurrent.futures

================
File: setup.py
================
from setuptools import setup, find_packages
import os

# Read the contents of the README file
this_directory = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

setup(
    name="cli-test-framework",
    version="0.2.4",
    author="Xiaotong Wang",
    author_email="xiaotongwang98@gmail.com",
    description="A small command line testing framework in Python with file comparison capabilities.",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/cli-test-framework",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "dukpy==0.5.0",
        "h5py>=3.8.0",
        "numpy>=2.0.1",
        "setuptools>=75.8.0",
        "wheel>=0.45.1"
    ],
    entry_points={
        'console_scripts': [
            'cli-test=cli_test_framework.cli:main',
            'compare-files=cli_test_framework.commands.compare:main',
        ],
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Topic :: Software Development :: Testing",
        "Topic :: Software Development :: Libraries :: Python Modules",
    ],
    python_requires='>=3.6',
    project_urls={
        'Documentation': 'https://github.com/yourusername/cli-test-framework/docs/user_manual.md',
        'Source': 'https://github.com/yourusername/cli-test-framework',
        'Tracker': 'https://github.com/yourusername/cli-test-framework/issues',
    },
)

================
File: SPACE_PATH_FIX.md
================
# åŒ…å«ç©ºæ ¼çš„è·¯å¾„è§£æžä¿®å¤

## é—®é¢˜æè¿°

ä¹‹å‰çš„å‘½ä»¤è§£æžå™¨åœ¨å¤„ç†åŒ…å«ç©ºæ ¼çš„è·¯å¾„æ—¶å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚ï¼š
- `"C:\Program Files (x86)\python.exe script.py"` ä¼šè¢«é”™è¯¯è§£æž
- `C:\Program Files (x86)\python.exe script.py` ä¼šè¢«åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†

## ä¿®å¤æ–¹æ¡ˆ

### 1. æ™ºèƒ½å‘½ä»¤è§£æžå‡½æ•°

åœ¨ `PathResolver` ç±»ä¸­æ·»åŠ äº† `parse_command_string()` æ–¹æ³•ï¼Œèƒ½å¤Ÿï¼š

- **å¤„ç†å¸¦å¼•å·çš„è·¯å¾„**ï¼šä½¿ç”¨ `shlex.split(posix=True)` æ­£ç¡®è§£æžå¼•å·
- **å¤„ç†ä¸å¸¦å¼•å·çš„ç»å¯¹è·¯å¾„**ï¼šè¯†åˆ«Windowsç»å¯¹è·¯å¾„æ¨¡å¼ï¼ˆå¦‚ `C:\...`ï¼‰
- **æ™ºèƒ½å‚æ•°åˆ†ç±»**ï¼šåŒºåˆ†å‘½ä»¤ã€æ–‡ä»¶è·¯å¾„å’Œé€‰é¡¹å‚æ•°

### 2. æ ¸å¿ƒæ”¹è¿›

#### å¼•å·å¤„ç†
```python
# ä¿®å¤å‰ï¼šå¼•å·è¢«ä¿ç•™
'"C:\Program Files (x86)\python.exe"' â†’ '"C:\Program Files (x86)\python.exe"'

# ä¿®å¤åŽï¼šå¼•å·è¢«æ­£ç¡®åŽ»é™¤
'"C:\Program Files (x86)\python.exe"' â†’ 'C:\Program Files (x86)\python.exe'
```

#### ç»å¯¹è·¯å¾„è¯†åˆ«
```python
# ä¿®å¤å‰ï¼šç©ºæ ¼å¯¼è‡´è·¯å¾„è¢«é”™è¯¯åˆ†å‰²
'C:\Program Files (x86)\python.exe script.py' â†’ ['C:\Program', 'Files', '(x86)\python.exe', 'script.py']

# ä¿®å¤åŽï¼šæ­£ç¡®è¯†åˆ«å®Œæ•´è·¯å¾„
'C:\Program Files (x86)\python.exe script.py' â†’ 'C:\Program Files (x86)\python.exe script.py'
```

### 3. æ›´æ–°çš„è¿è¡Œå™¨

æ‰€æœ‰è¿è¡Œå™¨éƒ½å·²æ›´æ–°ä½¿ç”¨æ–°çš„è§£æžæ–¹æ³•ï¼š
- `JSONRunner`
- `ParallelJSONRunner` 
- `YAMLRunner`

## æµ‹è¯•éªŒè¯

### æµ‹è¯•ç”¨ä¾‹

1. **ç®€å•å‘½ä»¤**ï¼š`echo hello` âœ“
2. **ç›¸å¯¹è·¯å¾„è„šæœ¬**ï¼š`python script.py` âœ“
3. **å¸¦å¼•å·çš„Windowsè·¯å¾„**ï¼š`"C:\Program Files (x86)\Python\python.exe" script.py` âœ“
4. **ä¸å¸¦å¼•å·çš„Windowsè·¯å¾„**ï¼š`C:\Program Files (x86)\Python\python.exe script.py` âœ“
5. **å¤æ‚å‘½ä»¤**ï¼š`node app.js --port 3000` âœ“
6. **Unixé£Žæ ¼è·¯å¾„**ï¼š`"/usr/local/bin/my app" script.py` âœ“

### è¿è¡Œæµ‹è¯•

```bash
# åŸºæœ¬æµ‹è¯•
python test_simple_space.py

# å…¨é¢æµ‹è¯•
python test_comprehensive_space.py

# å¹¶è¡Œè¿è¡Œå™¨æµ‹è¯•
python test_parallel_space.py
```

## æŠ€æœ¯ç»†èŠ‚

### å…³é”®å‡½æ•°

```python
def parse_command_string(self, command_string: str) -> str:
    """æ™ºèƒ½è§£æžå‘½ä»¤å­—ç¬¦ä¸²ï¼Œæ­£ç¡®å¤„ç†åŒ…å«ç©ºæ ¼çš„è·¯å¾„"""
    
    # 1. å¤„ç†å¸¦å¼•å·çš„å‘½ä»¤
    if '"' in command_string or "'" in command_string:
        parts = shlex.split(command_string, posix=True)
        # è§£æžå‘½ä»¤å’Œå‚æ•°...
    
    # 2. å¤„ç†ç»å¯¹è·¯å¾„å¼€å¤´çš„å‘½ä»¤
    elif self._starts_with_absolute_path(command_string):
        return self._parse_absolute_path_command(command_string)
    
    # 3. æ™®é€šå‘½ä»¤å¤„ç†
    else:
        parts = command_string.split()
        # æ ‡å‡†è§£æž...
```

### è·¯å¾„è¯†åˆ«é€»è¾‘

```python
def _starts_with_absolute_path(self, command_string: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä»¥ç»å¯¹è·¯å¾„å¼€å¤´"""
    if os.name == 'nt':  # Windows
        return (len(command_string) >= 3 and 
                command_string[1:3] == ':\\') or command_string.startswith('\\\\')
    else:  # Unix/Linux
        return command_string.startswith('/')
```

## å…¼å®¹æ€§

- âœ… Windows è·¯å¾„ï¼ˆ`C:\Program Files\...`ï¼‰
- âœ… Unix/Linux è·¯å¾„ï¼ˆ`/usr/local/bin/...`ï¼‰
- âœ… ç›¸å¯¹è·¯å¾„ï¼ˆ`./script.py`ï¼‰
- âœ… ç³»ç»Ÿå‘½ä»¤ï¼ˆ`echo`, `python`, `node`ç­‰ï¼‰
- âœ… å¤æ‚å‚æ•°ï¼ˆ`--port 3000`, `--env development`ï¼‰

## æ€§èƒ½å½±å“

- è§£æžæ€§èƒ½æå‡ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†å‰²é€»è¾‘
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼šé¿å…ä¸å¿…è¦çš„è·¯å¾„è½¬æ¢
- å¹¶è¡Œæ‰§è¡Œå…¼å®¹ï¼šæ‰€æœ‰å¹¶è¡ŒåŠŸèƒ½æ­£å¸¸å·¥ä½œ

## æ€»ç»“

è¿™æ¬¡ä¿®å¤å½»åº•è§£å†³äº†åŒ…å«ç©ºæ ¼çš„è·¯å¾„è§£æžé—®é¢˜ï¼Œä½¿æµ‹è¯•æ¡†æž¶èƒ½å¤Ÿæ­£ç¡®å¤„ç†å„ç§å¤æ‚çš„å‘½ä»¤æ ¼å¼ï¼Œç‰¹åˆ«æ˜¯WindowsçŽ¯å¢ƒä¸‹çš„è·¯å¾„ã€‚æ‰€æœ‰çŽ°æœ‰åŠŸèƒ½ä¿æŒå…¼å®¹ï¼Œå¹¶è¡Œæµ‹è¯•åŠŸèƒ½ä¹Ÿå®Œå…¨æ­£å¸¸ã€‚

================
File: src/__init__.py
================
# File: /python-test-framework/python-test-framework/src/__init__.py

# This file is intentionally left blank.

================
File: src/cli_test_framework.egg-info/dependency_links.txt
================


================
File: src/cli_test_framework.egg-info/entry_points.txt
================
[console_scripts]
cli-test = cli_test_framework.cli:main
compare-files = cli_test_framework.commands.compare:main

================
File: src/cli_test_framework.egg-info/PKG-INFO
================
Metadata-Version: 2.4
Name: cli-test-framework
Version: 0.2.4
Summary: A small command line testing framework in Python with file comparison capabilities.
Home-page: https://github.com/yourusername/cli-test-framework
Author: Xiaotong Wang
Author-email: xiaotongwang98@gmail.com
Project-URL: Documentation, https://github.com/yourusername/cli-test-framework/docs/user_manual.md
Project-URL: Source, https://github.com/yourusername/cli-test-framework
Project-URL: Tracker, https://github.com/yourusername/cli-test-framework/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Testing
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: dukpy==0.5.0
Requires-Dist: h5py>=3.8.0
Requires-Dist: numpy>=2.0.1
Requires-Dist: setuptools>=75.8.0
Requires-Dist: wheel>=0.45.1
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# CLI Testing Framework

## 1. Overview

This is a lightweight and extensible automated testing framework that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The framework is designed to provide standardized test management for command-line tools and scripts, with enterprise-grade parallel execution support and advanced file comparison features.

## 2. Features

- **ðŸš€ Parallel Test Execution**: Support for multi-threading and multi-processing parallel testing with significant performance improvements
- **ðŸ—ï¸ Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **ðŸ“„ Multi-Format Support**: Native support for JSON/YAML test case formats
- **ðŸ§  Intelligent Command Parsing**: Smart handling of complex commands like `"python ./script.py"`
- **ðŸ“ Smart Path Resolution**: Automatic handling of relative and absolute path conversions
- **âœ… Rich Assertion Mechanism**: Return code validation, output content matching, regex verification
- **ðŸ”Œ Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **ðŸ”’ Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **ðŸ“Š Comprehensive Reports**: Detailed pass rate statistics and failure diagnostics
- **ðŸ”§ Thread-Safe Design**: Robust concurrent execution with proper synchronization
- **ðŸ“ Advanced File Comparison**: Support for comparing various file types (text, binary, JSON, HDF5) with detailed diff output

## 3. Quick Start

### Environment Requirements

```bash
pip install cli-test-framework
Python >= 3.6
```

### Sequential Execution

```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Parallel Execution

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# Multi-threaded execution (recommended for I/O-intensive tests)
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # Maximum concurrent workers
    execution_mode="thread"  # "thread" or "process"
)
success = runner.run_tests()
```

### File Comparison

```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare JSON files with key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id

# Compare HDF5 files with specific options
compare-files data1.h5 data2.h5 --h5-table table1,table2 --h5-rtol 1e-6

# Compare binary files with similarity check
compare-files binary1.bin binary2.bin --similarity
```

## 4. Test Case Format

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "compare-files",
            "args": ["file1.txt", "file2.txt", "--verbose"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Files are identical"],
                "output_matches": [".*comparison completed.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Directory Scan Test
    command: ls
    args:
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

## 5. File Comparison Features

### Supported File Types

- **Text Files**: Plain text, source code, markdown, etc.
- **JSON Files**: With exact or key-based comparison
- **HDF5 Files**: Structure and content comparison with numerical tolerance
- **Binary Files**: With optional similarity index calculation

### Comparison Options

#### Text Comparison
```bash
compare-files file1.txt file2.txt \
    --start-line 10 \
    --end-line 20 \
    --encoding utf-8
```

#### JSON Comparison
```bash
compare-files data1.json data2.json \
    --json-compare-mode key-based \
    --json-key-field id,name
```

#### HDF5 Comparison
```bash
compare-files data1.h5 data2.h5 \
    --h5-table table1,table2 \
    --h5-structure-only \
    --h5-rtol 1e-5 \
    --h5-atol 1e-8
```

#### Binary Comparison
```bash
compare-files binary1.bin binary2.bin \
    --similarity \
    --chunk-size 16384
```

### Output Formats

- **Text**: Human-readable diff output
- **JSON**: Structured comparison results
- **HTML**: Visual diff with syntax highlighting

## 6. System Architecture

### Enhanced Architecture Flow

```mermaid
graph TD
    A[Test Cases] --> B{Execution Mode}
    B -->|Sequential| C[JSONRunner/YAMLRunner]
    B -->|Parallel| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[Command Parser]
    E --> F
    F --> G[Path Resolver]
    G --> H[Sub-process Execution]
    H --> I[Assertion Engine]
    I --> J[Thread-Safe Result Collection]
    J --> K[Report Generator]
    L[File Comparator] --> M[Text Comparator]
    L --> N[JSON Comparator]
    L --> O[HDF5 Comparator]
    L --> P[Binary Comparator]
```

### Core Components

#### 1. Intelligent Command Parser
```python
# Handles complex commands like "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> full path
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. Enhanced Path Resolver
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. Parallel Runner Base Class
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. Advanced Usage

### Performance Testing

```python
# Quick performance test
python performance_test.py

# Unit tests for parallel functionality
python -m pytest tests/test_parallel_runner.py -v
```

### Error Handling and Fallback

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # Check failed tests
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"Failed test: {detail['name']}")
                print(f"Error: {detail['message']}")
                
except Exception as e:
    print(f"Execution error: {e}")
    # Fallback to sequential execution
    runner.run_tests_sequential()
```

### Best Practices

1. **Choose Appropriate Concurrency**:
   ```python
   import os
   
   # For CPU-intensive tasks
   max_workers = os.cpu_count()
   
   # For I/O-intensive tasks
   max_workers = os.cpu_count() * 2
   ```

2. **Test Case Design**:
   - âœ… Ensure test independence (no dependencies between tests)
   - âœ… Avoid shared resource conflicts (different files/ports)
   - âœ… Use relative paths (framework handles resolution automatically)

3. **Debugging**:
   ```python
   # Enable verbose output for debugging
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # Set to 1 for easier debugging
       execution_mode="thread"
   )
   ```

## 8. Example Demonstrations

### Input Example

```json
{
    "test_cases": [
        {
            "name": "Python Version Check",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "File Processing Test",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Processing completed"]
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
âœ“ Python Version Check
âœ“ File Processing Test
âœ“ JSON Comparison Test
...
```

## 9. Troubleshooting

### Common Issues

1. **Process Mode Serialization Error**
   - **Cause**: Objects contain non-serializable attributes (like locks)
   - **Solution**: Use independent process worker functions

2. **Path Resolution Error**
   - **Cause**: System commands treated as relative paths
   - **Solution**: Update `PathResolver` system command list

3. **Performance Not Improved**
   - **Cause**: Test cases too short, parallel overhead exceeds benefits
   - **Solution**: Increase test case count or use more complex tests

4. **Command Not Found Error**
   - **Cause**: Complex commands like `"python ./script.py"` not parsed correctly
   - **Solution**: Framework now automatically handles this (fixed in latest version)

### Debug Tips

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check detailed results
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 10. Extension and Customization

### Adding New Runners

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # Add custom logic before test execution
        pass
```

### Custom Assertions

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"Execution too slow: {execution_time}s > {max_time}s")
```

## 11. Version Compatibility

- **Python Version**: 3.6+
- **Dependencies**: Standard library only (no external dependencies for core functionality)
- **Backward Compatibility**: Fully compatible with existing `JSONRunner` code
- **Platform Support**: Windows, macOS, Linux

## 12. Performance Benchmarks

| Test Scenario | Sequential | Parallel (Thread) | Parallel (Process) | Speedup |
|---------------|------------|-------------------|-------------------|---------|
| 10 I/O tests  | 5.2s       | 1.4s              | 2.1s              | 3.7x    |
| 20 CPU tests  | 12.8s      | 8.9s              | 6.2s              | 2.1x    |
| Mixed tests   | 8.5s       | 2.3s              | 3.1s              | 3.7x    |

## 13. Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass: `python -m pytest tests/ -v`
5. Submit a pull request

## 14. License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**ðŸš€ Ready to supercharge your testing workflow with parallel execution and advanced file comparison!**

For detailed parallel testing guide, see: [PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: src/cli_test_framework.egg-info/requires.txt
================
dukpy==0.5.0
h5py>=3.8.0
numpy>=2.0.1
setuptools>=75.8.0
wheel>=0.45.1

================
File: src/cli_test_framework.egg-info/SOURCES.txt
================
CHANGELOG.md
MANIFEST.in
README.md
pyproject.toml
setup.py
docs/user_manual.md
src/cli_test_framework/__init__.py
src/cli_test_framework/cli.py
src/cli_test_framework.egg-info/PKG-INFO
src/cli_test_framework.egg-info/SOURCES.txt
src/cli_test_framework.egg-info/dependency_links.txt
src/cli_test_framework.egg-info/entry_points.txt
src/cli_test_framework.egg-info/requires.txt
src/cli_test_framework.egg-info/top_level.txt
src/cli_test_framework/commands/__init__.py
src/cli_test_framework/commands/compare.py
src/cli_test_framework/core/__init__.py
src/cli_test_framework/core/assertions.py
src/cli_test_framework/core/base_runner.py
src/cli_test_framework/core/parallel_runner.py
src/cli_test_framework/core/process_worker.py
src/cli_test_framework/core/test_case.py
src/cli_test_framework/file_comparator/__init__.py
src/cli_test_framework/file_comparator/base_comparator.py
src/cli_test_framework/file_comparator/binary_comparator.py
src/cli_test_framework/file_comparator/csv_comparator.py
src/cli_test_framework/file_comparator/factory.py
src/cli_test_framework/file_comparator/h5_comparator.py
src/cli_test_framework/file_comparator/json_comparator.py
src/cli_test_framework/file_comparator/result.py
src/cli_test_framework/file_comparator/text_comparator.py
src/cli_test_framework/file_comparator/xml_comparator.py
src/cli_test_framework/runners/__init__.py
src/cli_test_framework/runners/json_runner.py
src/cli_test_framework/runners/parallel_json_runner.py
src/cli_test_framework/runners/yaml_runner.py
src/cli_test_framework/utils/__init__.py
src/cli_test_framework/utils/path_resolver.py
src/cli_test_framework/utils/report_generator.py
tests/__init__.py
tests/performance_test.py
tests/test1.py
tests/test_comprehensive_space.py
tests/test_parallel_runner.py
tests/test_parallel_space.py
tests/test_report.txt
tests/test_runners.py
tests/__pycache__/__init__.cpython-312.pyc
tests/__pycache__/test_parallel_runner.cpython-312-pytest-7.4.4.pyc
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json

================
File: src/cli_test_framework.egg-info/top_level.txt
================
cli_test_framework

================
File: src/cli_test_framework/__init__.py
================
"""
CLI Test Framework - A powerful command-line testing framework

This package provides tools for testing command-line applications and scripts
with support for parallel execution and advanced file comparison capabilities.
"""

__version__ = "0.2.4"
__author__ = "Xiaotong Wang"
__email__ = "xiaotongwang98@gmail.com"

# Import main classes for convenient access
from .runners.json_runner import JSONRunner
from .runners.parallel_json_runner import ParallelJSONRunner
from .runners.yaml_runner import YAMLRunner

__all__ = [
    'JSONRunner',
    'ParallelJSONRunner', 
    'YAMLRunner',
]

================
File: src/cli_test_framework/cli.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CLI Test Framework - Command Line Interface

This module provides the main command-line interface for the CLI Testing Framework.
"""

import argparse
import sys
import os
from pathlib import Path

from .runners import JSONRunner, ParallelJSONRunner, YAMLRunner


def create_parser():
    """Create and configure the argument parser"""
    parser = argparse.ArgumentParser(
        description="CLI Testing Framework - A powerful tool for testing command-line applications",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  cli-test run test_cases.json
  cli-test run test_cases.json --parallel --workers 4
  cli-test run test_cases.yaml --workspace /path/to/project
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Run command
    run_parser = subparsers.add_parser('run', help='Run test cases from a configuration file')
    run_parser.add_argument('config_file', help='Path to the test configuration file (JSON or YAML)')
    run_parser.add_argument('--workspace', '-w', help='Working directory for test execution')
    run_parser.add_argument('--parallel', '-p', action='store_true', help='Run tests in parallel')
    run_parser.add_argument('--workers', type=int, help='Number of parallel workers (default: CPU count)')
    run_parser.add_argument('--execution-mode', choices=['thread', 'process'], default='thread',
                           help='Parallel execution mode (default: thread)')
    run_parser.add_argument('--output-format', choices=['text', 'json', 'html'], default='text',
                           help='Output format for test results')
    run_parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')
    run_parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    
    return parser


def run_tests(args):
    """Run tests based on command line arguments"""
    config_file = Path(args.config_file)
    
    if not config_file.exists():
        print(f"Error: Configuration file not found: {config_file}")
        return False
    
    # Determine file type
    file_ext = config_file.suffix.lower()
    
    try:
        if args.parallel:
            # Use parallel runner
            runner = ParallelJSONRunner(
                config_file=str(config_file),
                workspace=args.workspace,
                max_workers=args.workers,
                execution_mode=args.execution_mode
            )
        else:
            # Use appropriate single-threaded runner
            if file_ext in ['.json']:
                runner = JSONRunner(
                    config_file=str(config_file),
                    workspace=args.workspace
                )
            elif file_ext in ['.yaml', '.yml']:
                runner = YAMLRunner(
                    config_file=str(config_file),
                    workspace=args.workspace
                )
            else:
                print(f"Error: Unsupported configuration file format: {file_ext}")
                return False
        
        # Run tests
        print(f"Running tests from: {config_file}")
        if args.parallel:
            print(f"Parallel mode: {args.execution_mode}, workers: {args.workers or 'auto'}")
        
        success = runner.run_tests()
        
        # Output results
        if hasattr(runner, 'results'):
            results = runner.results
            print(f"\nTest Results:")
            print(f"Total tests: {results.get('total_tests', 0)}")
            print(f"Passed: {results.get('passed', 0)}")
            print(f"Failed: {results.get('failed', 0)}")
            
            if args.verbose and 'details' in results:
                print("\nDetailed Results:")
                for result in results['details']:
                    status_symbol = "âœ“" if result['status'] == 'passed' else "âœ—"
                    print(f"  {status_symbol} {result['name']}: {result['status']}")
                    if result['status'] == 'failed' and result.get('message'):
                        print(f"    Error: {result['message']}")
        
        return success
        
    except Exception as e:
        print(f"Error running tests: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return False


def main():
    """Main entry point for the CLI"""
    parser = create_parser()
    args = parser.parse_args()
    
    if args.command == 'run':
        success = run_tests(args)
        sys.exit(0 if success else 1)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()

================
File: src/cli_test_framework/commands/__init__.py
================
"""
Command-line commands for the CLI Testing Framework
"""

from . import compare

__all__ = [
    'compare'
]

================
File: src/cli_test_framework/commands/compare.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file compare.py
@brief Command for comparing files in cli-test-framework
@author Xiaotong Wang
@date 2024
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from ..file_comparator.factory import ComparatorFactory
from ..file_comparator.result import ComparisonResult

def configure_logging():
    """Configure logging settings for the application"""
    logger = logging.getLogger("cli_test_framework.file_comparator")
    logger.setLevel(logging.INFO)
    
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    
    return logger

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Compare two files.")
    parser.add_argument("file1", help="Path to the first file")
    parser.add_argument("file2", help="Path to the second file")
    parser.add_argument("--start-line", type=int, default=1, help="Starting line number (1-based)")
    parser.add_argument("--end-line", type=int, help="Ending line number (1-based)")
    parser.add_argument("--start-column", type=int, default=1, help="Starting column number (1-based)")
    parser.add_argument("--end-column", type=int, help="Ending column number (1-based)")
    parser.add_argument("--file-type", help="Type of the files to compare", default="auto")
    parser.add_argument("--encoding", default="utf-8", help="File encoding for text files")
    parser.add_argument("--chunk-size", type=int, default=8192, help="Chunk size for binary comparison")
    parser.add_argument("--output-format", choices=["text", "json", "html"], default="text",
                        help="Output format for the comparison result")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with detailed logging")
    parser.add_argument("--similarity", action="store_true",
                        help="When comparing binary files, compute and show similarity index")
    parser.add_argument("--num-threads", type=int, default=4, help="Number of threads for parallel processing")
    
    # JSON comparison options
    json_group = parser.add_argument_group('JSON comparison options')
    json_group.add_argument("--json-compare-mode", choices=["exact", "key-based"], default="exact",
                      help="JSON comparison mode: exact (default) or key-based")
    json_group.add_argument("--json-key-field", help="Key field(s) to use for key-based JSON comparison")
    
    # H5 comparison options
    h5_group = parser.add_argument_group('HDF5 comparison options')
    h5_group.add_argument("--h5-table", help="Comma-separated list of table names to compare in HDF5 files")
    h5_group.add_argument("--h5-table-regex", help="Regular expression pattern to match table names in HDF5 files")
    h5_group.add_argument("--h5-structure-only", action="store_true", 
                         help="Only compare HDF5 file structure without comparing content")
    h5_group.add_argument("--h5-show-content-diff", action="store_true",
                         help="Show detailed content differences when content differs")
    h5_group.add_argument("--h5-rtol", type=float, default=1e-5,
                         help="Relative tolerance for numerical comparison in HDF5 files")
    h5_group.add_argument("--h5-atol", type=float, default=1e-8,
                         help="Absolute tolerance for numerical comparison in HDF5 files")
    
    return parser.parse_args()

def detect_file_type(file_path):
    """Detect the type of file based on its extension"""
    ext = file_path.suffix.lower()
    if ext in ['.txt', '.py', '.md', '.json', '.xml', '.html', '.css', '.js']:
        return 'text'
    elif ext == '.json':
        return 'json'
    elif ext in ['.h5', '.hdf5']:
        return 'h5'
    else:
        return 'binary'

def format_result(result, output_format):
    """Format the comparison result according to the specified output format"""
    if output_format == "json":
        return result.to_json()
    elif output_format == "html":
        return result.to_html()
    else:
        return str(result)

def main():
    """Main entry point for the compare-files command"""
    logger = configure_logging()

    try:
        args = parse_arguments()
        
        if args.debug:
            logger.setLevel(logging.DEBUG)
            logger.debug("Debug mode enabled")

        # Adjust for 0-based indexing
        start_line = max(0, args.start_line - 1)
        end_line = None if args.end_line is None else max(0, args.end_line - 1)
        start_column = max(0, args.start_column - 1)
        end_column = None if args.end_column is None else max(0, args.end_column - 1)

        # Resolve file paths
        file1_path = Path(args.file1).resolve()
        file2_path = Path(args.file2).resolve()

        if not file1_path.exists():
            raise ValueError(f"File not found: {file1_path}")
        if not file2_path.exists():
            raise ValueError(f"File not found: {file2_path}")

        # Determine file type
        file_type = args.file_type
        if file_type == "auto":
            file_type = detect_file_type(file1_path)
            logger.info(f"Auto-detected file type: {file_type}")

        # Prepare comparator kwargs
        comparator_kwargs = {
            "encoding": args.encoding,
            "chunk_size": args.chunk_size,
            "verbose": args.verbose or args.debug,
            "num_threads": args.num_threads
        }
        
        # Add file type specific arguments
        if file_type == "json":
            comparator_kwargs["compare_mode"] = args.json_compare_mode
            if args.json_key_field:
                key_fields = [field.strip() for field in args.json_key_field.split(',')]
                comparator_kwargs["key_field"] = key_fields[0] if len(key_fields) == 1 else key_fields
        
        if file_type == "h5":
            if args.h5_table:
                tables = [table.strip() for table in args.h5_table.split(',')]
                comparator_kwargs["tables"] = tables
            if args.h5_table_regex:
                comparator_kwargs["table_regex"] = args.h5_table_regex
            comparator_kwargs["structure_only"] = args.h5_structure_only
            comparator_kwargs["show_content_diff"] = args.h5_show_content_diff
            comparator_kwargs["rtol"] = args.h5_rtol
            comparator_kwargs["atol"] = args.h5_atol
        
        if file_type == "binary":
            comparator_kwargs["similarity"] = args.similarity

        # Create comparator and perform comparison
        comparator = ComparatorFactory.create_comparator(file_type, **comparator_kwargs)
        result = comparator.compare_files(
            file1_path,
            file2_path,
            start_line,
            end_line,
            start_column,
            end_column
        )

        # Output result
        output = format_result(result, args.output_format)
        print(output)

        sys.exit(0 if result.identical else 1)

    except ValueError as ve:
        logger.error(f"ValueError: {ve}")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"An unexpected error occurred")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: src/cli_test_framework/core/__init__.py
================
"""
Core components for the CLI Testing Framework
"""

from .base_runner import BaseRunner
from .parallel_runner import ParallelRunner
from .test_case import TestCase
from .assertions import Assertions

__all__ = [
    'BaseRunner',
    'ParallelRunner', 
    'TestCase',
    'Assertions'
]

================
File: src/cli_test_framework/core/assertions.py
================
import re
from typing import Any, Pattern

class Assertions:
    @staticmethod
    def equals(actual: Any, expected: Any, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected: {expected}, but got: {actual}")
        return True

    @staticmethod
    def contains(container: str, item: str, message: str = "") -> bool:
        """
        Check if the item is contained within the container string.
        This method returns True if the item is found anywhere within the container,
        even if the container contains other information.
        """
        if item not in container:
            raise AssertionError(f"{message} Expected to contain: {item}")
        return True

    @staticmethod
    def matches(text: str, pattern: str, message: str = "") -> bool:
        if not re.search(pattern, text):
            raise AssertionError(f"{message} Text does not match pattern: {pattern}")
        return True

    @staticmethod
    def return_code_equals(actual: int, expected: int, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected return code: {expected}, got: {actual}")
        return True

================
File: src/cli_test_framework/core/base_runner.py
================
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from .test_case import TestCase
from .assertions import Assertions

class BaseRunner(ABC):
    def __init__(self, config_file: str, workspace: Optional[str] = None):
        if workspace:
            self.workspace = Path(workspace)
        else:
            self.workspace = Path(__file__).parent.parent.parent
        self.config_path = self.workspace / config_file
        self.test_cases: List[TestCase] = []
        self.results: Dict[str, Any] = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "details": []
        }
        self.assertions = Assertions()

    @abstractmethod
    def load_test_cases(self) -> None:
        """Load test cases from configuration file"""
        pass

    def run_tests(self) -> bool:
        """Run all test cases and return whether all tests passed"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting test execution... Total tests: {self.results['total']}")
        print("=" * 50)
        
        for i, case in enumerate(self.test_cases, 1):
            print(f"\nRunning test {i}/{self.results['total']}: {case.name}")
            result = self.run_single_test(case)
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"âœ“ Test passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"âœ— Test failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
                
        print("\n" + "=" * 50)
        print(f"Test execution completed. Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0

    @abstractmethod
    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        """Run a single test case and return the result"""
        pass

================
File: src/cli_test_framework/core/parallel_runner.py
================
from abc import ABC
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Union
import time
import threading
from .base_runner import BaseRunner
from .test_case import TestCase
from .process_worker import run_test_in_process

class ParallelRunner(BaseRunner):
    """å¹¶è¡Œæµ‹è¯•è¿è¡Œå™¨åŸºç±»ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹æ‰§è¡Œ"""
    
    def __init__(self, config_file: str, workspace: Optional[str] = None, 
                 max_workers: Optional[int] = None, 
                 execution_mode: str = "thread"):
        """
        åˆå§‹åŒ–å¹¶è¡Œè¿è¡Œå™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            workspace: å·¥ä½œç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
            execution_mode: æ‰§è¡Œæ¨¡å¼ï¼Œ'thread'(çº¿ç¨‹) æˆ– 'process'(è¿›ç¨‹)
        """
        super().__init__(config_file, workspace)
        self.max_workers = max_workers
        self.execution_mode = execution_mode
        self.lock = threading.Lock()  # ç”¨äºŽçº¿ç¨‹å®‰å…¨çš„ç»“æžœæ›´æ–°
        
    def run_tests(self) -> bool:
        """å¹¶è¡Œè¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting parallel test execution... Total tests: {self.results['total']}")
        print(f"Execution mode: {self.execution_mode}, Max workers: {self.max_workers or 'auto'}")
        print("=" * 50)
        
        start_time = time.time()
        
        if self.execution_mode == "process":
            executor_class = ProcessPoolExecutor
        else:
            executor_class = ThreadPoolExecutor
            
        with executor_class(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            if self.execution_mode == "process":
                # è¿›ç¨‹æ¨¡å¼ï¼šä½¿ç”¨ç‹¬ç«‹çš„å·¥ä½œå™¨å‡½æ•°
                future_to_case = {
                    executor.submit(
                        run_test_in_process, 
                        i, 
                        {
                            "name": case.name,
                            "command": case.command,
                            "args": case.args,
                            "expected": case.expected
                        },
                        str(self.workspace) if self.workspace else None
                    ): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            else:
                # çº¿ç¨‹æ¨¡å¼ï¼šä½¿ç”¨å®žä¾‹æ–¹æ³•
                future_to_case = {
                    executor.submit(self._run_test_with_index, i, case): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            
            # æ”¶é›†ç»“æžœ
            for future in as_completed(future_to_case):
                test_index, case = future_to_case[future]
                try:
                    result = future.result()
                    self._update_results(result, test_index, case)
                except Exception as exc:
                    error_result = {
                        "name": case.name,
                        "status": "failed",
                        "message": f"Test execution failed: {str(exc)}",
                        "output": "",
                        "command": "",
                        "return_code": None
                    }
                    self._update_results(error_result, test_index, case)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print("\n" + "=" * 50)
        print(f"Parallel test execution completed in {execution_time:.2f} seconds")
        print(f"Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0
    
    def _run_test_with_index(self, test_index: int, case: TestCase) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è¿”å›žç»“æžœï¼ˆåŒ…å«ç´¢å¼•ä¿¡æ¯ï¼‰"""
        print(f"[Worker] Running test {test_index}: {case.name}")
        result = self.run_single_test(case)
        return result
    
    def _update_results(self, result: Dict[str, Any], test_index: int, case: TestCase) -> None:
        """çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°æµ‹è¯•ç»“æžœ"""
        with self.lock:
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"âœ“ Test {test_index} passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"âœ— Test {test_index} failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
    
    def run_tests_sequential(self) -> bool:
        """å›žé€€åˆ°é¡ºåºæ‰§è¡Œæ¨¡å¼"""
        print("Falling back to sequential execution...")
        return super().run_tests()

================
File: src/cli_test_framework/core/process_worker.py
================
"""
è¿›ç¨‹å·¥ä½œå™¨æ¨¡å—
ç”¨äºŽå¤šè¿›ç¨‹å¹¶è¡Œæµ‹è¯•æ‰§è¡Œï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
"""

import subprocess
import sys
from typing import Dict, Any
from .test_case import TestCase
from .assertions import Assertions

def run_test_in_process(test_index: int, case_data: Dict[str, Any], workspace: str = None) -> Dict[str, Any]:
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹
    
    Args:
        test_index: æµ‹è¯•ç´¢å¼•
        case_data: æµ‹è¯•ç”¨ä¾‹æ•°æ®å­—å…¸
        workspace: å·¥ä½œç›®å½•
    
    Returns:
        æµ‹è¯•ç»“æžœå­—å…¸
    """
    # é‡æ–°åˆ›å»ºTestCaseå¯¹è±¡ï¼ˆé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
    case = TestCase(
        name=case_data["name"],
        command=case_data["command"],
        args=case_data["args"],
        expected=case_data["expected"]
    )
    
    # åˆ›å»ºæ–­è¨€å¯¹è±¡
    assertions = Assertions()
    
    result = {
        "name": case.name,
        "status": "failed",
        "message": "",
        "output": "",
        "command": "",
        "return_code": None
    }

    try:
        command = f"{case.command} {' '.join(case.args)}"
        result["command"] = command
        print(f"  [Process Worker {test_index}] Executing command: {command}")
        
        process = subprocess.run(
            command,
            cwd=workspace if workspace else None,
            capture_output=True,
            text=True,
            check=False,
            shell=True
        )

        output = process.stdout + process.stderr
        result["output"] = output
        result["return_code"] = process.returncode
        
        if output.strip():
            print(f"  [Process Worker {test_index}] Command output for {case.name}:")
            for line in output.splitlines():
                print(f"    {line}")

        # æ£€æŸ¥è¿”å›žç 
        if "return_code" in case.expected:
            print(f"  [Process Worker {test_index}] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
            assertions.return_code_equals(
                process.returncode,
                case.expected["return_code"]
            )

        # æ£€æŸ¥è¾“å‡ºåŒ…å«
        if "output_contains" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output contains for {case.name}...")
            for expected_text in case.expected["output_contains"]:
                assertions.contains(output, expected_text)

        # æ£€æŸ¥æ­£åˆ™åŒ¹é…
        if "output_matches" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output matches regex for {case.name}...")
            assertions.matches(output, case.expected["output_matches"])

        result["status"] = "passed"
        
    except AssertionError as e:
        result["message"] = str(e)
    except Exception as e:
        result["message"] = f"Execution error: {str(e)}"

    return result

================
File: src/cli_test_framework/core/test_case.py
================
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class TestCase:
    name: str
    command: str
    args: List[str]
    expected: Dict[str, Any]
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert test case to dictionary format"""
        print("Convert test case to dictionary format")
        print(self.command)
        return {
            "name": self.name,
            "command": self.command,
            "args": self.args,
            "expected": self.expected
        }

================
File: src/cli_test_framework/file_comparator/__init__.py
================
"""
File comparison module for cli-test-framework.
This module provides functionality for comparing different types of files.
"""

from .factory import ComparatorFactory
from .result import ComparisonResult

__all__ = ['ComparatorFactory', 'ComparisonResult']

================
File: src/cli_test_framework/file_comparator/base_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file base_comparator.py
@brief Base abstract class for file comparison operations
@author Xiaotong Wang
@date 2025
"""

from abc import ABC, abstractmethod
import logging
from pathlib import Path
from .result import ComparisonResult, Difference

class BaseComparator(ABC):
    """
    @brief Base abstract class for all file comparators
    @details This class defines the interface and common functionality for all file comparators.
             It provides basic file comparison operations and logging capabilities.
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False):
        """
        @brief Initialize the base comparator
        @param encoding str: File encoding to use (default: "utf-8")
        @param chunk_size int: Size of chunks for reading large files (default: 8192)
        @param verbose bool: Enable verbose logging (default: False)
        """
        self.encoding = encoding
        self.chunk_size = chunk_size
        self.logger = logging.getLogger(f"file_comparator.{self.__class__.__name__}")
        if verbose:
            self.logger.setLevel(logging.DEBUG)
    
    @abstractmethod
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read file content with specified range
        @param file_path Path: Path to the file to read
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return object: File content in a format suitable for comparison
        """
        pass

    @abstractmethod
    def compare_content(self, content1, content2):
        """
        @brief Compare two content objects and return comparison details
        @param content1 object: First content object to compare
        @param content2 object: Second content object to compare
        @return tuple: (bool, list) - (identical, differences)
        """
        pass

    def compare_files(self, file1, file2, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Compare two files with the specified parameters
        @param file1 Path: Path to the first file
        @param file2 Path: Path to the second file
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return ComparisonResult: Result object containing comparison details
        """
        result = ComparisonResult(
            file1=str(file1),
            file2=str(file2),
            start_line=start_line,
            end_line=end_line,
            start_column=start_column,
            end_column=end_column
        )
        
        try:
            self.logger.info(f"Comparing files: {file1} and {file2}")
            
            # Record file metadata
            file1_path = Path(file1)
            file2_path = Path(file2)
            result.file1_size = file1_path.stat().st_size
            result.file2_size = file2_path.stat().st_size
            
            # Read content with specified ranges
            self.logger.debug(f"Reading content from files")
            content1 = self.read_content(file1, start_line, end_line, start_column, end_column)
            content2 = self.read_content(file2, start_line, end_line, start_column, end_column)
            
            # Compare content
            self.logger.debug(f"Comparing content")
            identical, differences = self.compare_content(content1, content2)
            
            # Update result
            result.identical = identical
            result.differences = differences
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error during comparison: {str(e)}")
            result.error = str(e)
            result.identical = False
            return result

================
File: src/cli_test_framework/file_comparator/binary_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file binary_comparator.py
@brief Binary file comparator implementation with efficient byte-level comparison
@author Xiaotong Wang
@date 2025
"""

import hashlib
from .base_comparator import BaseComparator
from .result import Difference
from concurrent.futures import ThreadPoolExecutor

class BinaryComparator(BaseComparator):
    """
    @brief Comparator for binary files with efficient byte-level comparison
    @details This class implements binary file comparison with support for:
             - Byte-level difference detection
             - Similarity index calculation using LCS
             - Parallel processing for large files
             - File hash calculation
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False, similarity=False, num_threads=4):
        """
        @brief Initialize the binary comparator
        @param encoding str: File encoding (not used for binary files)
        @param chunk_size int: Size of chunks for reading large files
        @param verbose bool: Enable verbose logging
        @param similarity bool: Enable similarity index calculation
        @param num_threads int: Number of threads for parallel processing
        """
        super().__init__(encoding, chunk_size, verbose)
        self.similarity = similarity
        self.num_threads = num_threads

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read binary content with specified range
        @param file_path Path: Path to the binary file to read
        @param start_line int: Starting byte offset (interpreted as bytes for binary files)
        @param end_line int: Ending byte offset (interpreted as bytes for binary files)
        @param start_column int: Ignored for binary files
        @param end_column int: Ignored for binary files
        @return bytes: Binary content within the specified range
        @throws ValueError: If byte offsets are invalid
        @throws FileNotFoundError: If file doesn't exist
        @throws IOError: If there are other file reading errors
        """
        try:
            self.logger.debug(f"Reading binary file: {file_path}")
            
            # For binary files, interpret start_line as byte offset
            start_offset = start_line
            end_offset = end_line
            
            with open(file_path, 'rb') as f:
                if start_offset > 0:
                    f.seek(start_offset)
                
                if end_offset is not None:
                    if end_offset <= start_offset:
                        raise ValueError("End offset must be greater than start offset")
                    bytes_to_read = end_offset - start_offset
                    content = f.read(bytes_to_read)
                else:
                    content = f.read()
                    
            return content
                
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")
        except IOError as e:
            raise ValueError(f"Error reading file {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare binary content efficiently
        @param content1 bytes: First binary content to compare
        @param content2 bytes: Second binary content to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Performs efficient byte-level comparison of binary content.
                 Reports differences with hex context and limits the number
                 of differences to avoid overwhelming output.
        """
        self.logger.debug(f"Comparing binary content")
        
        if len(content1) != len(content2):
            differences = [Difference(
                position="file size",
                expected=f"{len(content1)} bytes",
                actual=f"{len(content2)} bytes",
                diff_type="size"
            )]
            identical = False
        elif content1 == content2:
            differences = []
            identical = True
        else:
            identical = False
            differences = []
            offset = 0
            max_differences = 10  # Limit number of differences reported
        
            for i in range(0, len(content1), self.chunk_size):
                chunk1 = content1[i:i+self.chunk_size]
                chunk2 = content2[i:i+self.chunk_size]
                
                if chunk1 != chunk2:
                    # Find the exact byte position where the difference starts
                    for j in range(len(chunk1)):
                        if j >= len(chunk2) or chunk1[j] != chunk2[j]:
                            diff_pos = i + j
                            # Show a few bytes before and after the difference for context
                            context_size = 8
                            start_ctx = max(0, diff_pos - context_size)
                            end_ctx = min(len(content1), diff_pos + context_size)
                            
                            # Create hex representations of the differing sections
                            expected_bytes = content1[start_ctx:end_ctx]
                            actual_bytes = content2[start_ctx:min(len(content2), end_ctx)]
                            
                            expected_hex = ' '.join(f"{b:02x}" for b in expected_bytes)
                            actual_hex = ' '.join(f"{b:02x}" for b in actual_bytes)
                            
                            differences.append(Difference(
                                position=f"byte {diff_pos}",
                                expected=expected_hex,
                                actual=actual_hex,
                                diff_type="content"
                            ))
                            break
                            
                    if len(differences) >= max_differences:
                        differences.append(Difference(
                            position=None,
                            expected=None,
                            actual=None,
                            diff_type=f"more differences not shown"
                        ))
                        break
        
        return identical, differences

    def compute_lcs_length(self, a: bytes, b: bytes) -> int:
        """
        @brief Compute the length of the longest common subsequence
        @param a bytes: First binary sequence
        @param b bytes: Second binary sequence
        @return int: Length of the longest common subsequence
        @details Uses dynamic programming with memory optimization to compute LCS.
                 Supports parallel processing for large sequences.
        """
        if not a or not b:
            return 0

        def lcs_worker(start, end):
            previous = [0] * (len(b) + 1)
            for i in range(start, end):
                current = [0] * (len(b) + 1)
                for j in range(1, len(b) + 1):
                    if a[i - 1] == b[j - 1]:
                        current[j] = previous[j - 1] + 1
                    else:
                        current[j] = max(previous[j], current[j - 1])
                previous = current
            return previous[len(b)]

        chunk_size = len(a) // self.num_threads
        futures = []

        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            for i in range(self.num_threads):
                start = i * chunk_size
                end = (i + 1) * chunk_size if i != self.num_threads - 1 else len(a)
                futures.append(executor.submit(lcs_worker, start, end))

        lcs_length = sum(f.result() for f in futures)
        return lcs_length

    def compare_files(self, file1, file2, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Compare two binary files with optional similarity calculation
        @param file1 Path: Path to the first binary file
        @param file2 Path: Path to the second binary file
        @param start_line int: Starting byte offset
        @param end_line int: Ending byte offset
        @param start_column int: Ignored for binary files
        @param end_column int: Ignored for binary files
        @return ComparisonResult: Result object containing comparison details
        """
        from pathlib import Path
        from .result import ComparisonResult
        result = ComparisonResult(
            file1=str(file1),
            file2=str(file2),
            start_line=start_line,
            end_line=end_line,
            start_column=start_column,
            end_column=end_column
        )
        try:
            self.logger.info(f"Comparing files: {file1} and {file2}")
            file1_path = Path(file1)
            file2_path = Path(file2)
            result.file1_size = file1_path.stat().st_size
            result.file2_size = file2_path.stat().st_size
            self.logger.debug("Reading content from files")
            content1 = self.read_content(file1, start_line, end_line, start_column, end_column)
            content2 = self.read_content(file2, start_line, end_line, start_column, end_column)
            self.logger.debug("Comparing content")
            identical, differences = self.compare_content(content1, content2)
            result.identical = identical
            result.differences = differences
            if self.similarity:
                if (len(content1) + len(content2)) > 0:
                    lcs_len = self.compute_lcs_length(content1, content2)
                    similarity = 2 * lcs_len / (len(content1) + len(content2))
                else:
                    similarity = 1
                result.similarity = similarity
            return result
        except Exception as e:
            self.logger.error(f"Error during comparison: {str(e)}")
            result.error = str(e)
            result.identical = False
            return result

    def get_file_hash(self, file_path, chunk_size=8192):
        """
        @brief Calculate SHA-256 hash of a file efficiently
        @param file_path Path: Path to the file to hash
        @param chunk_size int: Size of chunks for reading large files
        @return str: Hexadecimal representation of the file's SHA-256 hash
        """
        h = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(chunk_size), b''):
                h.update(chunk)
        return h.hexdigest()

================
File: src/cli_test_framework/file_comparator/csv_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file csv_comparator.py
@brief CSV file comparator implementation with row and column comparison
@author Xiaotong Wang
@date 2025
"""

import csv
import io
from .text_comparator import TextComparator
from .result import Difference

class CsvComparator(TextComparator):
    """
    @brief Comparator for CSV files with row and column comparison
    @details This class extends TextComparator to provide specialized CSV comparison
             capabilities, including:
             - Row count comparison
             - Column count comparison
             - Cell value comparison
             - Configurable delimiter and quote character
    """
    
    def __init__(self, encoding="utf-8", delimiter=",", quotechar='"', chunk_size=8192, verbose=False):
        """
        @brief Initialize CSV comparator with configuration
        @param encoding str: File encoding (default: utf-8)
        @param delimiter str: CSV field delimiter (default: comma)
        @param quotechar str: Character used for quoting fields (default: double quote)
        @param chunk_size int: Size of chunks for reading large files
        @param verbose bool: Enable verbose output
        """
        super().__init__(encoding, chunk_size, verbose)
        self.delimiter = delimiter
        self.quotechar = quotechar
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse CSV content from file
        @param file_path Path: Path to the CSV file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return list: List of rows, where each row is a list of cell values
        @details Reads CSV content and parses it into a structured format,
                 supporting line and column range selection
        """
        # First read the file as text
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Join the lines to create a CSV string
        csv_text = ''.join(text_content)
        
        # Parse the CSV
        csv_data = []
        csv_reader = csv.reader(
            io.StringIO(csv_text), 
            delimiter=self.delimiter,
            quotechar=self.quotechar
        )
        
        for row in csv_reader:
            # Apply column range if specified
            if start_column > 0 or end_column is not None:
                col_start = start_column
                col_end = end_column if end_column is not None else len(row)
                row = row[col_start:col_end+1]
            csv_data.append(row)
            
        return csv_data
    
    def compare_content(self, content1, content2):
        """
        @brief Compare CSV content structurally
        @param content1 list: First CSV data to compare (list of rows)
        @param content2 list: Second CSV data to compare (list of rows)
        @return tuple: (bool, list) - (identical, differences)
        @details Performs structural comparison of CSV data, including:
                 - Row count comparison
                 - Column count comparison per row
                 - Cell value comparison
                 - Limits the number of reported differences
        """
        if content1 == content2:
            return True, []
            
        differences = []
        
        # Check row count
        if len(content1) != len(content2):
            differences.append(Difference(
                position="row count",
                expected=f"{len(content1)} rows",
                actual=f"{len(content2)} rows",
                diff_type="row_count_mismatch"
            ))
        
        # Compare rows
        max_diffs = 10
        for i, (row1, row2) in enumerate(zip(content1, content2)):
            # Check column count in this row
            if len(row1) != len(row2):
                differences.append(Difference(
                    position=f"row {i+1}",
                    expected=f"{len(row1)} columns",
                    actual=f"{len(row2)} columns",
                    diff_type="column_count_mismatch"
                ))
                if len(differences) >= max_diffs:
                    break
            
            # Compare column values
            for j, (cell1, cell2) in enumerate(zip(row1, row2)):
                if cell1 != cell2:
                    differences.append(Difference(
                        position=f"row {i+1}, column {j+1}",
                        expected=cell1,
                        actual=cell2,
                        diff_type="cell_mismatch"
                    ))
                    if len(differences) >= max_diffs:
                        break
            
            if len(differences) >= max_diffs:
                break
        
        # Add a summary if there are more differences
        if len(differences) >= max_diffs:
            differences.append(Difference(
                position=None,
                expected=None,
                actual=None,
                diff_type=f"more differences not shown"
            ))
        
        return False, differences

================
File: src/cli_test_framework/file_comparator/factory.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file factory.py
@brief Factory class for creating file comparators based on file type
@author Xiaotong Wang
@date 2025
"""

import os
import importlib
import pkgutil
from pathlib import Path

class ComparatorFactory:
    """
    @brief Factory class for creating file comparators
    @details This class manages the creation and registration of different types of file comparators.
             It provides a centralized way to create appropriate comparators based on file type
             and handles parameter filtering for different comparator types.
    """
    _comparators = {}
    _initialized = False

    @staticmethod
    def register_comparator(file_type, comparator_class):
        """
        @brief Register a new comparator class for a specific file type
        @param file_type str: Type of file the comparator handles
        @param comparator_class class: Comparator class to register
        """
        ComparatorFactory._comparators[file_type.lower()] = comparator_class

    @staticmethod
    def create_comparator(file_type, **kwargs):
        """
        @brief Create a comparator instance for the specified file type
        @param file_type str: Type of file to compare
        @param **kwargs: Additional arguments to pass to the comparator
        @return BaseComparator: An instance of the appropriate comparator class
        @details Creates and returns a comparator instance based on the file type.
                 If no specific comparator is found, falls back to TextComparator
                 for text files or BinaryComparator for other types.
        """
        if not ComparatorFactory._initialized:
            ComparatorFactory._load_comparators()

        comparator_class = ComparatorFactory._comparators.get(file_type.lower())
        if not comparator_class:
            if file_type.lower() in ['auto', 'text']:
                from .text_comparator import TextComparator
                # Only pass TextComparator supported parameters
                text_kwargs = {k: v for k, v in kwargs.items() 
                              if k in ['encoding', 'chunk_size', 'verbose']}
                return TextComparator(**text_kwargs)
            else:
                from .binary_comparator import BinaryComparator
                # Pass all BinaryComparator supported parameters
                binary_kwargs = {k: v for k, v in kwargs.items()
                               if k in ['chunk_size', 'verbose', 'similarity', 'num_threads']}
                return BinaryComparator(**binary_kwargs)

        # Filter parameters based on comparator type
        if file_type.lower() == 'h5':
            # H5 comparator accepts specific parameters
            h5_kwargs = {k: v for k, v in kwargs.items()
                        if k in ['tables', 'table_regex', 'encoding', 'chunk_size', 'verbose', 'structure_only', 'show_content_diff', 'debug', 'rtol', 'atol']}
            return comparator_class(**h5_kwargs)
        elif file_type.lower() == 'binary':
            # Binary comparator accepts all parameters, including num_threads
            binary_kwargs = {k: v for k, v in kwargs.items()
                           if k in ['chunk_size', 'verbose', 'similarity', 'num_threads']}
            return comparator_class(**binary_kwargs)
        elif file_type.lower() == 'json':
            # JSON comparator accepts specific parameters
            json_kwargs = {k: v for k, v in kwargs.items()
                         if k in ['encoding', 'chunk_size', 'verbose', 'compare_mode', 'key_field']}
            return comparator_class(**json_kwargs)
        else:
            # Other comparators only accept basic parameters
            basic_kwargs = {k: v for k, v in kwargs.items()
                          if k in ['encoding', 'chunk_size', 'verbose']}
            return comparator_class(**basic_kwargs)

    @staticmethod
    def _load_comparators():
        """
        @brief Load and register all available comparators
        @details Automatically discovers and registers comparator classes from the package.
                 This includes both built-in comparators and any additional comparators
                 that follow the naming convention '*_comparator.py'.
        """
        from .text_comparator import TextComparator
        from .binary_comparator import BinaryComparator

        ComparatorFactory.register_comparator('text', TextComparator)
        ComparatorFactory.register_comparator('binary', BinaryComparator)

        package_dir = Path(__file__).parent
        for module_info in pkgutil.iter_modules([str(package_dir)]):
            if module_info.name.endswith('_comparator') and module_info.name not in [
                'base_comparator', 'text_comparator', 'binary_comparator'
            ]:
                try:
                    module = importlib.import_module(f".{module_info.name}", package=__package__)

                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if (isinstance(attr, type) and
                            attr.__module__ == module.__name__ and
                            attr_name.endswith('Comparator')):
                            type_name = attr_name.lower().replace('comparator', '')
                            ComparatorFactory.register_comparator(type_name, attr)
                except ImportError as e:
                    print(f"Failed to import comparator module {module_info.name}: {e}")

        ComparatorFactory._initialized = True

    @staticmethod
    def get_available_comparators():
        """
        @brief Get a list of all registered comparator types
        @return list: List of available comparator type names
        """
        if not ComparatorFactory._initialized:
            ComparatorFactory._load_comparators()
        return sorted(ComparatorFactory._comparators.keys())

# Register built-in comparators
from .json_comparator import JsonComparator
from .xml_comparator import XmlComparator
from .csv_comparator import CsvComparator
from .text_comparator import TextComparator
from .binary_comparator import BinaryComparator

ComparatorFactory.register_comparator('json', JsonComparator)
ComparatorFactory.register_comparator('xml', XmlComparator)
ComparatorFactory.register_comparator('csv', CsvComparator)
ComparatorFactory.register_comparator('text', TextComparator)
ComparatorFactory.register_comparator('binary', BinaryComparator)

================
File: src/cli_test_framework/file_comparator/h5_comparator.py
================
from .base_comparator import BaseComparator
import h5py
import numpy as np
import logging
import re

class H5Comparator(BaseComparator):
    def __init__(self, tables=None, table_regex=None, structure_only=False, show_content_diff=False, debug=False, rtol=1e-5, atol=1e-8, **kwargs):
        """
        Initialize H5 comparator
        :param tables: List of table names to compare. If None, compare all tables
        :param table_regex: Regular expression pattern to match table names
        :param structure_only: If True, only compare file structure without comparing content
        :param show_content_diff: If True, show detailed content differences
        :param debug: If True, enable debug mode
        :param rtol: Relative tolerance for numerical comparison
        :param atol: Absolute tolerance for numerical comparison
        """
        super().__init__(**kwargs)
        self.tables = tables
        self.table_regex = table_regex
        self.structure_only = structure_only
        self.show_content_diff = show_content_diff
        self.rtol = rtol
        self.atol = atol
        
        # Set debug level if verbose is enabled
        if kwargs.get('verbose', False) or debug:
            self.logger.setLevel(logging.DEBUG)
            
        self.logger.debug(f"Initialized H5Comparator with structure_only={structure_only}, show_content_diff={show_content_diff}, rtol={rtol}, atol={atol}")
        if table_regex:
            self.logger.debug(f"Using table regex pattern: {table_regex}")

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """Read H5 file content"""
        content = {}
        
        # Log whether we're in structure-only mode
        self.logger.debug(f"Reading file {file_path} in structure-only mode: {self.structure_only}")
        self.logger.debug(f"Tables parameter: {self.tables}")
        self.logger.debug(f"Table regex parameter: {self.table_regex}")
        
        with h5py.File(file_path, 'r') as f:
            # Function to collect structure information
            def collect_structure(name, obj):
                if isinstance(obj, h5py.Dataset):
                    content[name] = {
                        'type': 'dataset',
                        'shape': obj.shape,
                        'dtype': str(obj.dtype),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for dataset: {name}")
                elif isinstance(obj, h5py.Group) and name:  # Skip root group
                    content[name] = {
                        'type': 'group',
                        'keys': list(obj.keys()),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for group: {name}")
            
            # Function to collect structure and data
            def collect_structure_and_data(name, obj):
                if isinstance(obj, h5py.Dataset):
                    dataset_info = {
                        'type': 'dataset',
                        'shape': obj.shape,
                        'dtype': str(obj.dtype),
                        'attrs': dict(obj.attrs)
                    }
                    
                    # Read data with range constraints
                    try:
                        data = obj[:]
                        if isinstance(data, np.ndarray):
                            if end_line is None:
                                end_line_actual = data.shape[0]
                            else:
                                end_line_actual = min(end_line, data.shape[0])
                                
                            if len(data.shape) == 1:
                                data = data[start_line:end_line_actual]
                            elif len(data.shape) > 1:
                                if end_column is None:
                                    end_column_actual = data.shape[1]
                                else:
                                    end_column_actual = min(end_column, data.shape[1])
                                data = data[start_line:end_line_actual, start_column:end_column_actual]
                        
                        dataset_info['data'] = data
                        self.logger.debug(f"Collected data for dataset: {name}")
                    except Exception as e:
                        self.logger.error(f"Error reading data from {name}: {str(e)}")
                    
                    content[name] = dataset_info
                    
                elif isinstance(obj, h5py.Group) and name:  # Skip root group
                    content[name] = {
                        'type': 'group',
                        'keys': list(obj.keys()),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for group: {name}")
            
            if self.tables or self.table_regex:
                # If specific tables or regex pattern is specified
                if self.table_regex:
                    # If the regex looks like a simple path (no regex metacharacters except . and /),
                    # escape it to treat it as a literal string
                    regex_str = self.table_regex
                    self.logger.debug(f"Original table_regex: {regex_str}")
                    # Check if it contains regex metacharacters other than . and /
                    import string
                    regex_metacharacters = set('[]{}()*+?^$|\\')
                    if not any(char in regex_str for char in regex_metacharacters):
                        # Escape dots and other special characters for literal matching
                        regex_str = re.escape(regex_str)
                        self.logger.debug(f"Treating table_regex as literal path, escaped: {regex_str}")
                    else:
                        self.logger.debug(f"Using table_regex as regular expression: {regex_str}")
                    regex_pattern = re.compile(regex_str)
                else:
                    regex_pattern = None
                
                def should_process(name):
                    if self.tables and name in self.tables:
                        self.logger.debug(f"Matched by tables list: {name}")
                        return True
                    if regex_pattern and regex_pattern.fullmatch(name):
                        self.logger.debug(f"Matched by regex: {name}")
                        return True
                    return False
                
                def process_item(name, item):
                    try:
                        if self.structure_only:
                            collect_structure(name, item)
                        else:
                            collect_structure_and_data(name, item)
                    except Exception as e:
                        self.logger.error(f"Error processing {name}: {str(e)}")
                
                # First try direct path access for table names
                if self.tables:
                    for table_path in self.tables:
                        try:
                            if table_path in f:
                                process_item(table_path, f[table_path])
                            else:
                                self.logger.warning(f"Table {table_path} not found in {file_path}")
                        except Exception as e:
                            self.logger.error(f"Error processing {table_path}: {str(e)}")
                
                # Then process regex pattern if specified
                if regex_pattern:
                    def visit_with_regex(name, obj):
                        self.logger.debug(f"Checking path: {name}")
                        if should_process(name):
                            self.logger.debug(f"Processing matched path: {name}")
                            process_item(name, obj)
                        else:
                            self.logger.debug(f"Skipping path: {name}")
                    f.visititems(visit_with_regex)
            else:
                # If no tables specified, read all datasets
                if self.structure_only:
                    f.visititems(collect_structure)
                else:
                    f.visititems(collect_structure_and_data)
        
        self.logger.debug(f"Read {len(content)} items from {file_path}")
        self.logger.debug(f"Items read: {list(content.keys())}")
        return content

    def compare_content(self, content1, content2):
        """Compare two H5 file contents"""
        identical = True
        differences = []

        # Get all unique table names
        all_tables = set(content1.keys()) | set(content2.keys())
        
        # Debug log
        self.logger.debug(f"Structure-only mode: {self.structure_only}")
        self.logger.debug(f"Number of tables to compare: {len(all_tables)}")
        
        for table_name in all_tables:
            # Debug log
            self.logger.debug(f"Comparing table: {table_name}")
            if table_name in content1 and table_name in content2:
                self.logger.debug(f"Table1 keys: {content1[table_name].keys()}")
                self.logger.debug(f"Table2 keys: {content2[table_name].keys()}")
            
            # Check if table exists in both files
            if table_name not in content1:
                differences.append(self._create_difference(
                    position=table_name,
                    expected="Table exists",
                    actual="Table missing",
                    diff_type="structure"
                ))
                identical = False
                continue
                
            if table_name not in content2:
                differences.append(self._create_difference(
                    position=table_name,
                    expected="Table exists",
                    actual="Table missing",
                    diff_type="structure"
                ))
                identical = False
                continue
            
            table1 = content1[table_name]
            table2 = content2[table_name]
            
            # Compare table type
            if table1.get('type') != table2.get('type'):
                differences.append(self._create_difference(
                    position=f"{table_name}/type",
                    expected=table1.get('type'),
                    actual=table2.get('type'),
                    diff_type="structure"
                ))
                identical = False
                continue
            
            # For datasets, compare shape and dtype
            if table1.get('type') == 'dataset':
                if table1['shape'] != table2['shape']:
                    differences.append(self._create_difference(
                        position=f"{table_name}/shape",
                        expected=str(table1['shape']),
                        actual=str(table2['shape']),
                        diff_type="structure"
                    ))
                    identical = False
                
                if table1['dtype'] != table2['dtype']:
                    differences.append(self._create_difference(
                        position=f"{table_name}/dtype",
                        expected=str(table1['dtype']),
                        actual=str(table2['dtype']),
                        diff_type="structure"
                    ))
                    identical = False
            
            # For groups, compare keys
            elif table1.get('type') == 'group':
                keys1 = set(table1['keys'])
                keys2 = set(table2['keys'])
                if keys1 != keys2:
                    missing_keys = keys1 - keys2
                    extra_keys = keys2 - keys1
                    if missing_keys:
                        differences.append(self._create_difference(
                            position=f"{table_name}/keys",
                            expected=str(sorted(missing_keys)),
                            actual="Keys missing",
                            diff_type="structure"
                        ))
                    if extra_keys:
                        differences.append(self._create_difference(
                            position=f"{table_name}/keys",
                            expected="No extra keys",
                            actual=str(sorted(extra_keys)),
                            diff_type="structure"
                        ))
                    identical = False
            
            # Only compare attributes and data if not in structure-only mode
            if not self.structure_only:
                self.logger.debug(f"Comparing attributes and data for {table_name}")
                
                # Compare attributes
                attr_diff = self._compare_attributes(table1['attrs'], table2['attrs'], table_name)
                if attr_diff:
                    differences.extend(attr_diff)
                    identical = False
                
                # Compare data content
                if 'data' in table1 and 'data' in table2:
                    data1 = table1['data']
                    data2 = table2['data']
                    
                    if isinstance(data1, np.ndarray) and isinstance(data2, np.ndarray):
                        try:
                            # å¯¹äºŽæ•°å€¼ç±»åž‹æ•°æ®ä½¿ç”¨ isclose
                            if np.issubdtype(data1.dtype, np.number) and np.issubdtype(data2.dtype, np.number):
                                equal_mask = np.isclose(data1, data2, equal_nan=True, rtol=self.rtol, atol=self.atol)
                                if not np.all(equal_mask):
                                    diff_indices = np.where(~equal_mask)
                                    if self.show_content_diff:
                                        # Report up to 10 differences
                                        for idx in zip(*diff_indices)[:10]:
                                            position = f"{table_name}[{','.join(map(str, idx))}]"
                                            differences.append(self._create_difference(
                                                position=position,
                                                expected=str(data1[idx]),
                                                actual=str(data2[idx]),
                                                diff_type="content"
                                            ))
                                    else:
                                        # Just report that content differs
                                        differences.append(self._create_difference(
                                            position=table_name,
                                            expected="Same content",
                                            actual="Content differs",
                                            diff_type="content"
                                        ))
                                    identical = False
                            # å¯¹äºŽå­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»åž‹ç›´æŽ¥æ¯”è¾ƒ
                            else:
                                if not np.array_equal(data1, data2):
                                    if self.show_content_diff:
                                        # For non-numeric arrays, find the first difference
                                        diff_indices = np.where(data1 != data2)
                                        for idx in zip(*diff_indices)[:10]:
                                            position = f"{table_name}[{','.join(map(str, idx))}]"
                                            differences.append(self._create_difference(
                                                position=position,
                                                expected=str(data1[idx]),
                                                actual=str(data2[idx]),
                                                diff_type="content"
                                            ))
                                    else:
                                        differences.append(self._create_difference(
                                            position=table_name,
                                            expected="Same content",
                                            actual="Content differs",
                                            diff_type="content"
                                        ))
                                    identical = False
                        except Exception as e:
                            self.logger.error(f"Error comparing data in table {table_name}: {str(e)}")
                            differences.append(self._create_difference(
                                position=table_name,
                                expected=f"Data type: {table1.get('dtype', 'unknown')}",
                                actual=f"Data type: {table2.get('dtype', 'unknown')}",
                                diff_type="error"
                            ))
                            identical = False
        
        return identical, differences

    def _compare_attributes(self, attrs1, attrs2, table_name):
        """Compare HDF5 attributes"""
        differences = []
        
        # Compare attribute keys
        keys1 = set(attrs1.keys())
        keys2 = set(attrs2.keys())
        
        # Check for missing attributes
        for key in keys1 - keys2:
            differences.append(self._create_difference(
                position=f"{table_name}/attrs/{key}",
                expected=str(attrs1[key]),
                actual="Attribute missing",
                diff_type="missing_attribute"
            ))
            
        for key in keys2 - keys1:
            differences.append(self._create_difference(
                position=f"{table_name}/attrs/{key}",
                expected="Attribute missing",
                actual=str(attrs2[key]),
                diff_type="extra_attribute"
            ))
            
        # Compare common attributes
        for key in keys1 & keys2:
            val1 = attrs1[key]
            val2 = attrs2[key]
            
            # Handle numpy arrays in attributes
            if isinstance(val1, np.ndarray) and isinstance(val2, np.ndarray):
                try:
                    if not np.array_equal(val1, val2):
                        differences.append(self._create_difference(
                            position=f"{table_name}/attrs/{key}",
                            expected=str(val1),
                            actual=str(val2),
                            diff_type="attribute"
                        ))
                except Exception as e:
                    self.logger.error(f"Error comparing array attribute {key}: {str(e)}")
                    differences.append(self._create_difference(
                        position=f"{table_name}/attrs/{key}",
                        expected=str(val1),
                        actual=str(val2),
                        diff_type="attribute"
                    ))
            elif isinstance(val1, np.ndarray) or isinstance(val2, np.ndarray):
                # One is array, one is not - they're different
                differences.append(self._create_difference(
                    position=f"{table_name}/attrs/{key}",
                    expected=str(val1),
                    actual=str(val2),
                    diff_type="attribute"
                ))
            else:
                # Regular comparison for non-array values
                if val1 != val2:
                    differences.append(self._create_difference(
                        position=f"{table_name}/attrs/{key}",
                        expected=str(val1),
                        actual=str(val2),
                        diff_type="attribute"
                    ))
                
        return differences

    def _create_difference(self, position, expected, actual, diff_type):
        """Create a Difference object"""
        from .result import Difference
        return Difference(position=position, expected=expected, actual=actual, diff_type=diff_type)

# Register the new comparator
from .factory import ComparatorFactory
ComparatorFactory.register_comparator('h5', H5Comparator)

================
File: src/cli_test_framework/file_comparator/json_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file json_comparator.py
@brief JSON file comparator implementation with support for exact and key-based comparison
@author Xiaotong Wang
@date 2025
"""

import json
from .text_comparator import TextComparator
from .result import Difference

class JsonComparator(TextComparator):
    """
    @brief Comparator for JSON files with support for exact and key-based comparison
    @details This class extends TextComparator to provide specialized JSON comparison
             capabilities, including:
             - Exact comparison of JSON structures
             - Key-based comparison for lists of objects
             - Detailed difference reporting with path information
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False, key_field=None, compare_mode="exact"):
        """
        @brief Initialize the JSON comparator
        @param encoding str: File encoding
        @param chunk_size int: Chunk size for reading files
        @param verbose bool: Enable verbose logging
        @param key_field str or list: Field name(s) to use as key for comparing JSON objects in lists
        @param compare_mode str: Comparison mode: 'exact' (default) or 'key-based'
        """
        super().__init__(encoding, chunk_size, verbose)
        self.key_field = key_field
        self.compare_mode = compare_mode

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse JSON content from file
        @param file_path Path: Path to the JSON file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return dict or list: Parsed JSON content
        @throws ValueError: If JSON is invalid or key fields are missing
        """
        # Read the text content using the parent class method
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Convert to a single string
        json_text = ''.join(text_content)
        try:
            json_data = json.loads(json_text)
            if self.key_field:
                # Only keep the specified key field(s)
                key_fields = self.key_field if isinstance(self.key_field, list) else [self.key_field]
                filtered_data = {key: json_data[key] for key in key_fields if key in json_data}
                return filtered_data
            return json_data
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in {file_path}: {str(e)}")

    def compare_content(self, content1, content2):
        """
        @brief Compare JSON content using the specified comparison mode
        @param content1 dict or list: First JSON content to compare
        @param content2 dict or list: Second JSON content to compare
        @return tuple: (bool, list) - (identical, differences)
        """
        # Quick check for exact equality
        if content1 == content2:
            return True, []
        
        # Different comparison modes
        differences = []
        if self.compare_mode == "key-based" and self.key_field:
            self._compare_json_key_based(content1, content2, "", differences)
        else:
            self._compare_json_exact(content1, content2, "", differences)
            
        return False, differences

    def _compare_json_exact(self, obj1, obj2, path, differences, max_diffs=10):
        """
        @brief Perform exact JSON comparison
        @param obj1: First JSON object to compare
        @param obj2: Second JSON object to compare
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Compares JSON objects recursively, checking for:
                 - Type mismatches
                 - Missing or extra keys in dictionaries
                 - Length mismatches in lists
                 - Value mismatches
        """
        if len(differences) >= max_diffs:
            return

        # Type check
        if type(obj1) != type(obj2):
            differences.append(Difference(
                position=path or "root",
                expected=f"{type(obj1).__name__}: {obj1}",
                actual=f"{type(obj2).__name__}: {obj2}",
                diff_type="type_mismatch"
            ))
            return

        # Dictionary comparison
        if isinstance(obj1, dict):
            keys1 = set(obj1.keys())
            keys2 = set(obj2.keys())

            # Check for missing keys
            for key in keys1 - keys2:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=obj1[key],
                    actual=None,
                    diff_type="missing_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Check for extra keys
            for key in keys2 - keys1:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=None,
                    actual=obj2[key],
                    diff_type="extra_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Compare common keys recursively
            for key in keys1 & keys2:
                new_path = f"{path}.{key}" if path else key
                self._compare_json_exact(obj1[key], obj2[key], new_path, differences, max_diffs)

        # List comparison
        elif isinstance(obj1, list):
            if len(obj1) != len(obj2):
                differences.append(Difference(
                    position=path or "root",
                    expected=f"list with {len(obj1)} items",
                    actual=f"list with {len(obj2)} items",
                    diff_type="length_mismatch"
                ))

            # Compare items by position
            for i in range(min(len(obj1), len(obj2))):
                new_path = f"{path}[{i}]"
                self._compare_json_exact(obj1[i], obj2[i], new_path, differences, max_diffs)

        # Value comparison
        elif obj1 != obj2:
            differences.append(Difference(
                position=path or "root",
                expected=obj1,
                actual=obj2,
                diff_type="value_mismatch"
            ))

    def _compare_json_key_based(self, obj1, obj2, path, differences, max_diffs=10):
        """
        @brief Perform key-based JSON comparison for lists of objects
        @param obj1: First JSON object to compare
        @param obj2: Second JSON object to compare
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Similar to exact comparison but with special handling for lists
                 of objects, using key fields to match items instead of position
        """
        if len(differences) >= max_diffs:
            return

        # Type check
        if type(obj1) != type(obj2):
            differences.append(Difference(
                position=path or "root",
                expected=f"{type(obj1).__name__}: {obj1}",
                actual=f"{type(obj2).__name__}: {obj2}",
                diff_type="type_mismatch"
            ))
            return

        # Dictionary comparison (same as exact comparison)
        if isinstance(obj1, dict):
            keys1 = set(obj1.keys())
            keys2 = set(obj2.keys())

            # Check for missing keys
            for key in keys1 - keys2:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=obj1[key],
                    actual=None,
                    diff_type="missing_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Check for extra keys
            for key in keys2 - keys1:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=None,
                    actual=obj2[key],
                    diff_type="extra_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Compare common keys recursively
            for key in keys1 & keys2:
                new_path = f"{path}.{key}" if path else key
                self._compare_json_key_based(obj1[key], obj2[key], new_path, differences, max_diffs)

        # List comparison - the key difference for key-based comparison
        elif isinstance(obj1, list) and isinstance(obj2, list):
            # Check if we can do key-based comparison
            if self.key_field and all(isinstance(item, dict) for item in obj1 + obj2):
                self._compare_lists_by_key(obj1, obj2, path, differences, max_diffs)
            else:
                # Fall back to position-based comparison if key-based is not possible
                if len(obj1) != len(obj2):
                    differences.append(Difference(
                        position=path or "root",
                        expected=f"list with {len(obj1)} items",
                        actual=f"list with {len(obj2)} items", 
                        diff_type="length_mismatch"
                    ))

                # Compare items by position
                for i in range(min(len(obj1), len(obj2))):
                    new_path = f"{path}[{i}]"
                    self._compare_json_key_based(obj1[i], obj2[i], new_path, differences, max_diffs)

        # Value comparison
        elif obj1 != obj2:
            differences.append(Difference(
                position=path or "root",
                expected=obj1,
                actual=obj2,
                diff_type="value_mismatch"
            ))

    def _compare_lists_by_key(self, list1, list2, path, differences, max_diffs=10):
        """
        @brief Compare two lists of dictionaries using key field(s) to match items
        @param list1 list: First list of dictionaries
        @param list2 list: Second list of dictionaries
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Matches items in lists using specified key fields instead of position,
                 allowing for reordered lists with the same content
        """
        # Convert key_field to list if it's a string
        key_fields = self.key_field if isinstance(self.key_field, list) else [self.key_field]
        
        # Create dictionaries indexed by the key fields
        dict1 = {}
        dict2 = {}
        
        # Function to create compound key from an item
        def get_key(item):
            if not all(k in item for k in key_fields):
                return None  # Skip items without all key fields
            return tuple(str(item.get(k)) for k in key_fields)
        
        # Build dictionaries from lists
        for i, item in enumerate(list1):
            key = get_key(item)
            if key:
                dict1[key] = (i, item)
        
        for i, item in enumerate(list2):
            key = get_key(item)
            if key:
                dict2[key] = (i, item)
        
        # Find keys in the first list that are missing from the second
        for key in set(dict1.keys()) - set(dict2.keys()):
            idx, item = dict1[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            differences.append(Difference(
                position=f"{path}[{idx}] (key: {key_str})",
                expected=item,
                actual=None,
                diff_type="missing_item"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Find keys in the second list that are missing from the first
        for key in set(dict2.keys()) - set(dict1.keys()):
            idx, item = dict2[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            differences.append(Difference(
                position=f"{path}[{idx}] (key: {key_str})",
                expected=None,
                actual=item,
                diff_type="extra_item"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Compare matching items
        for key in set(dict1.keys()) & set(dict2.keys()):
            idx1, item1 = dict1[key]
            idx2, item2 = dict2[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            new_path = f"{path}[key:{key_str}]"
            
            # Skip identical items
            if item1 == item2:
                continue
                
            # Recursive comparison of matched items
            self._compare_json_key_based(item1, item2, new_path, differences, max_diffs)

================
File: src/cli_test_framework/file_comparator/result.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file result.py
@brief Classes for representing file comparison results and differences
@author Xiaotong Wang
@date 2025
"""

class Difference:
    """
    @brief Represents a single difference between files
    @details This class encapsulates information about a single difference found
             during file comparison, including position, expected and actual content,
             and the type of difference.
    """
    
    def __init__(self, position=None, expected=None, actual=None, diff_type="content"):
        """
        @brief Initialize a Difference object
        @param position: Position of the difference (can be line number, byte position, etc.)
        @param expected: Expected content at the position
        @param actual: Actual content found at the position
        @param diff_type: Type of difference ("content", "missing", "extra", etc.)
        """
        self.position = position  # Can be line number, byte position, etc.
        self.expected = expected  # Expected content
        self.actual = actual      # Actual content
        self.diff_type = diff_type  # Type of difference: "content", "missing", "extra", etc.
    
    def __str__(self):
        """
        @brief Convert the difference to a string representation
        @return str: Human-readable description of the difference
        """
        if self.diff_type == "content":
            return f"At {self.position}: expected '{self.expected}', got '{self.actual}'"
        elif self.diff_type == "missing":
            return f"Missing content at {self.position}: '{self.expected}'"
        elif self.diff_type == "extra":
            return f"Extra content at {self.position}: '{self.actual}'"
        else:
            return f"Difference at {self.position}"
    
    def to_dict(self):
        """
        @brief Convert the difference to a dictionary representation
        @return dict: Dictionary containing the difference details
        """
        return {
            "position": self.position,
            "expected": self.expected,
            "actual": self.actual,
            "diff_type": self.diff_type
        }

class ComparisonResult:
    """
    @brief Represents the result of a file comparison
    @details This class encapsulates all information about a file comparison,
             including file paths, comparison range, differences found,
             and additional metadata like file sizes and similarity index.
    """
    
    def __init__(self, file1=None, file2=None, start_line=0, end_line=None, 
                 start_column=0, end_column=None):
        """
        @brief Initialize a ComparisonResult object
        @param file1: Path to the first file
        @param file2: Path to the second file
        @param start_line: Starting line number for comparison (0-based)
        @param end_line: Ending line number for comparison (0-based, None for end of file)
        @param start_column: Starting column number for comparison (0-based)
        @param end_column: Ending column number for comparison (0-based, None for end of line)
        """
        self.file1 = file1
        self.file2 = file2
        self.file1_size = None
        self.file2_size = None
        self.start_line = start_line
        self.end_line = end_line
        self.start_column = start_column
        self.end_column = end_column
        self.identical = None
        self.differences = []
        self.error = None
        self.similarity = None  # Similarity index for binary comparisons
    
    def __str__(self):
        """
        @brief Convert the comparison result to a string representation
        @return str: Human-readable description of the comparison result
        """
        if self.error:
            return f"Error during comparison: {self.error}"
        lines = []
        if self.identical:
            range_str = self._get_range_str()
            lines.append(f"Files are identical{range_str}.")
        else:
            lines.append(f"Files are different. Found {len(self.differences)} differences:")
            for i, diff in enumerate(self.differences, 1):
                lines.append(f"{i}. {diff}")
            if self.similarity is not None:
                lines.append(f"Similarity Index: {self.similarity:.2f}")
        return "\n".join(lines)
    
    def _get_range_str(self):
        """
        @brief Get a string representation of the comparison range
        @return str: Description of the line and column ranges being compared
        """
        parts = []
        if self.start_line > 0 or self.end_line is not None:
            line_range = f"lines {self.start_line+1}"
            if self.end_line is not None:
                line_range += f"-{self.end_line+1}"
            parts.append(line_range)
            
        if self.start_column > 0 or self.end_column is not None:
            col_range = f"columns {self.start_column+1}"
            if self.end_column is not None:
                col_range += f"-{self.end_column+1}"
            parts.append(col_range)
            
        if parts:
            return " in " + ", ".join(parts)
        return ""
    
    def to_dict(self):
        """
        @brief Convert the comparison result to a dictionary representation
        @return dict: Dictionary containing all comparison details
        """
        return {
            "file1": self.file1,
            "file2": self.file2,
            "file1_size": self.file1_size,
            "file2_size": self.file2_size,
            "range": {
                "start_line": self.start_line,
                "end_line": self.end_line,
                "start_column": self.start_column,
                "end_column": self.end_column
            },
            "identical": self.identical,
            "differences": [diff.to_dict() for diff in self.differences],
            "similarity": self.similarity,
            "error": self.error
        }
    
    def to_html(self):
        """
        @brief Convert the comparison result to HTML format
        @details Generates a complete HTML document with styling for displaying
                 the comparison results in a web browser
        @return str: HTML representation of the comparison result
        """
        if self.error:
            return f"<div class='error'>Error during comparison: {self.error}</div>"
            
        html = ["<html><head><style>",
                "body { font-family: Arial, sans-serif; }",
                ".identical { color: green; }",
                ".different { color: red; }",
                ".diff-item { margin: 10px 0; padding: 5px; border: 1px solid #ccc; }",
                "</style></head><body>"]
        
        if self.identical:
            range_str = self._get_range_str()
            html.append(f"<h2 class='identical'>Files are identical{range_str}.</h2>")
        else:
            html.append(f"<h2 class='different'>Files are different. Found {len(self.differences)} differences:</h2>")
            if self.similarity is not None:
                html.append(f"<p>Similarity Index: {self.similarity:.2f}</p>")
            html.append("<div class='diff-list'>")
            
            for i, diff in enumerate(self.differences, 1):
                html.append(f"<div class='diff-item'>")
                html.append(f"<h3>Difference {i}</h3>")
                html.append(f"<p>Position: {diff.position}</p>")
                html.append(f"<p>Expected: <pre>{diff.expected}</pre></p>")
                html.append(f"<p>Actual: <pre>{diff.actual}</pre></p>")
                html.append(f"<p>Type: {diff.diff_type}</p>")
                html.append("</div>")
                
            html.append("</div>")
            
        html.append("</body></html>")
        return "\n".join(html)

================
File: src/cli_test_framework/file_comparator/text_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file text_comparator.py
@brief Text file comparator implementation with line-by-line comparison
@author Xiaotong Wang
@date 2025
"""

import difflib
from .base_comparator import BaseComparator
from .result import Difference

class TextComparator(BaseComparator):
    """
    @brief Comparator for text files with line-by-line comparison
    @details This class implements text file comparison using Python's difflib
             for detailed difference detection. It supports line and column-based
             range selection for comparison.
    """
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read text content with specified range
        @param file_path Path: Path to the text file to read
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return list: List of text lines within the specified range
        @throws ValueError: If line or column ranges are invalid
        @throws UnicodeDecodeError: If file encoding is incorrect
        @throws FileNotFoundError: If file doesn't exist
        @throws IOError: If there are other file reading errors
        """
        try:
            self.logger.debug(f"Reading text file: {file_path}")
            with open(file_path, 'r', encoding=self.encoding) as f:
                lines = f.readlines()
                
            if start_line < 0:
                raise ValueError("Start line cannot be negative")
                
            if end_line is not None:
                if end_line < start_line:
                    raise ValueError("End line cannot be before start line")
                if end_line >= len(lines):
                    self.logger.warning(f"End line {end_line} exceeds file length {len(lines)}, capping at {len(lines)-1}")
                    end_line = len(lines) - 1
            else:
                end_line = len(lines) - 1
                
            if start_line >= len(lines):
                raise ValueError(f"Start line {start_line} is beyond file length {len(lines)}")
                
            selected_lines = lines[start_line:end_line+1]
            
            if start_column < 0:
                raise ValueError("Start column cannot be negative")
                
            if start_column > 0 or end_column is not None:
                self.logger.debug(f"Applying column range: {start_column} to {end_column}")
                processed_lines = []
                for line in selected_lines:
                    if end_column is not None and end_column < start_column:
                        raise ValueError("End column cannot be before start column")
                    # Make sure we don't exceed line length
                    effective_end = end_column
                    if effective_end is not None and effective_end >= len(line):
                        effective_end = len(line) - 1
                    # Apply column range, handle if start_column is beyond line length
                    if start_column >= len(line):
                        processed_lines.append("")
                    else:
                        processed_lines.append(line[start_column:None if effective_end is None else effective_end+1])
                return processed_lines
            
            return selected_lines
            
        except UnicodeDecodeError as e:
            raise ValueError(f"File encoding error for {file_path}. Try specifying a different encoding. Error: {str(e)}")
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")
        except IOError as e:
            raise ValueError(f"Error reading file {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare text content and return detailed differences
        @param content1 list: First list of text lines to compare
        @param content2 list: Second list of text lines to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Uses difflib to generate a detailed comparison of the text content.
                 Returns a tuple containing a boolean indicating if the content is identical
                 and a list of Difference objects describing any differences found.
                 Limits the number of differences reported to 10 to avoid overwhelming output.
        """
        self.logger.debug(f"Comparing text content")
        
        if content1 == content2:
            return True, []
            
        differences = []
        
        # Use difflib for more detailed comparison
        diff = list(difflib.unified_diff(content1, content2, n=0))
        
        # Process the diff output to create structured differences
        line_diffs = []
        for line in diff[2:]:  # Skip the first two header lines
            if line.startswith('@@'):
                continue
            elif line.startswith('-'):
                line_diffs.append(('remove', line[1:]))
            elif line.startswith('+'):
                line_diffs.append(('add', line[1:]))
            else:
                line_diffs.append(('context', line[1:]))
        
        # Convert diff to our difference format
        line_num1 = 0
        line_num2 = 0
        for i, (action, line) in enumerate(line_diffs):
            if action == 'remove':
                # Look ahead for a corresponding 'add'
                add_match = None
                for j in range(i+1, min(i+5, len(line_diffs))):
                    if line_diffs[j][0] == 'add':
                        add_match = line_diffs[j][1]
                        del line_diffs[j]
                        break
                
                if add_match is not None:
                    # Content difference
                    differences.append(Difference(
                        position=f"line {line_num1+1}",
                        expected=line,
                        actual=add_match,
                        diff_type="content"
                    ))
                else:
                    # Missing line
                    differences.append(Difference(
                        position=f"line {line_num1+1}",
                        expected=line,
                        actual=None,
                        diff_type="missing"
                    ))
                line_num1 += 1
                
            elif action == 'add':
                # Extra line
                differences.append(Difference(
                    position=f"line {line_num2+1}",
                    expected=None,
                    actual=line,
                    diff_type="extra"
                ))
                line_num2 += 1
                
            elif action == 'context':
                line_num1 += 1
                line_num2 += 1
        
        # Limit the number of differences reported
        max_diffs = 10
        if len(differences) > max_diffs:
            differences = differences[:max_diffs]
            differences.append(Difference(
                position=None,
                expected=None,
                actual=None,
                diff_type=f"more differences not shown (total: {len(differences)})"
            ))
            
        return False, differences

================
File: src/cli_test_framework/file_comparator/xml_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file xml_comparator.py
@brief XML file comparator implementation with structural comparison
@author Xiaotong Wang
@date 2025
"""

import xml.etree.ElementTree as ET
from .text_comparator import TextComparator
from .result import Difference

class XmlComparator(TextComparator):
    """
    @brief Comparator for XML files with structural comparison
    @details This class extends TextComparator to provide specialized XML comparison
             capabilities, including:
             - Tag comparison
             - Attribute comparison
             - Text content comparison
             - Child element comparison
    """
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse XML content from file
        @param file_path Path: Path to the XML file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return ET.Element: Parsed XML element tree
        @throws ValueError: If XML is invalid
        """
        # First read the file as text
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Join the lines
        xml_text = ''.join(text_content)
        try:
            return ET.fromstring(xml_text)
        except ET.ParseError as e:
            raise ValueError(f"Invalid XML in {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare XML content structurally
        @param content1 ET.Element: First XML element to compare
        @param content2 ET.Element: Second XML element to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Performs structural comparison of XML elements, including tags,
                 attributes, text content, and child elements
        """
        # Convert back to strings for comparison
        xml_str1 = ET.tostring(content1, encoding='unicode')
        xml_str2 = ET.tostring(content2, encoding='unicode')
        
        if xml_str1 == xml_str2:
            return True, []
            
        # Use a recursive function to find differences in XML structures
        differences = []
        self._compare_elements(content1, content2, "", differences)
        
        return False, differences
    
    def _compare_elements(self, elem1, elem2, path, differences, max_diffs=10):
        """
        @brief Recursively compare XML elements and collect differences
        @param elem1 ET.Element: First XML element to compare
        @param elem2 ET.Element: Second XML element to compare
        @param path str: Current path in the XML structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Compares XML elements recursively, checking for:
                 - Tag mismatches
                 - Missing or extra attributes
                 - Text content differences
                 - Child element count mismatches
                 - Child element differences
        """
        if len(differences) >= max_diffs:
            return
            
        # Compare tags
        if elem1.tag != elem2.tag:
            differences.append(Difference(
                position=path or "/",
                expected=elem1.tag,
                actual=elem2.tag,
                diff_type="tag_mismatch"
            ))
            return  # If tags don't match, don't compare further
            
        # Compare attributes
        attrib1 = set(elem1.attrib.items())
        attrib2 = set(elem2.attrib.items())
        
        for attr, value in attrib1 - attrib2:
            differences.append(Difference(
                position=f"{path}/@{attr}" if path else f"/@{attr}",
                expected=value,
                actual="missing attribute",
                diff_type="missing_attribute"
            ))
            if len(differences) >= max_diffs:
                return
                
        for attr, value in attrib2 - attrib1:
            differences.append(Difference(
                position=f"{path}/@{attr}" if path else f"/@{attr}",
                expected="missing attribute",
                actual=value,
                diff_type="extra_attribute"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Compare text content if leaf nodes
        if len(elem1) == 0 and len(elem2) == 0:
            text1 = elem1.text.strip() if elem1.text else ""
            text2 = elem2.text.strip() if elem2.text else ""
            
            if text1 != text2:
                differences.append(Difference(
                    position=path or "/",
                    expected=text1,
                    actual=text2,
                    diff_type="text_mismatch"
                ))
                return
                
        # Compare children elements
        children1 = list(elem1)
        children2 = list(elem2)
        
        if len(children1) != len(children2):
            differences.append(Difference(
                position=path or "/",
                expected=f"{len(children1)} child elements",
                actual=f"{len(children2)} child elements",
                diff_type="children_count_mismatch"
            ))
            
        # Compare matching children
        for i, (child1, child2) in enumerate(zip(children1, children2)):
            new_path = f"{path}/{child1.tag}[{i}]" if path else f"/{child1.tag}[{i}]"
            self._compare_elements(child1, child2, new_path, differences, max_diffs)

================
File: src/cli_test_framework/runners/__init__.py
================
"""
Test runners for the CLI Testing Framework
"""

from .json_runner import JSONRunner
from .parallel_json_runner import ParallelJSONRunner
from .yaml_runner import YAMLRunner

__all__ = [
    'JSONRunner',
    'ParallelJSONRunner',
    'YAMLRunner'
]

================
File: src/cli_test_framework/runners/json_runner.py
================
from typing import Optional
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
from typing import Dict, Any

class JSONRunner(BaseRunner):
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self) -> None:
        """Load test cases from a JSON file."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                # ä½¿ç”¨æ™ºèƒ½å‘½ä»¤è§£æžï¼Œæ­£ç¡®å¤„ç†åŒ…å«ç©ºæ ¼çš„è·¯å¾„
                case["command"] = self.path_resolver.parse_command_string(case["command"])
                
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
            # print(self.test_cases)
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",  # æ·»åŠ å‘½ä»¤è¾“å‡ºå­—æ®µ
            "command": "",  # æ·»åŠ æ‰§è¡Œçš„å‘½ä»¤å­—æ®µ
            "return_code": None  # æ·»åŠ è¿”å›žç å­—æ®µ
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command  # ä¿å­˜æ‰§è¡Œçš„å‘½ä»¤
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output  # ä¿å­˜å‘½ä»¤çš„å®Œæ•´è¾“å‡º
            result["return_code"] = process.returncode  # ä¿å­˜è¿”å›žç 
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/runners/parallel_json_runner.py
================
from typing import Optional, Dict, Any
from ..core.parallel_runner import ParallelRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
import threading

class ParallelJSONRunner(ParallelRunner):
    """å¹¶è¡ŒJSONæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None,
                 max_workers: Optional[int] = None, execution_mode: str = "thread"):
        """
        åˆå§‹åŒ–å¹¶è¡ŒJSONè¿è¡Œå™¨
        
        Args:
            config_file: JSONé…ç½®æ–‡ä»¶è·¯å¾„
            workspace: å·¥ä½œç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°
            execution_mode: æ‰§è¡Œæ¨¡å¼ï¼Œ'thread' æˆ– 'process'
        """
        super().__init__(config_file, workspace, max_workers, execution_mode)
        self.path_resolver = PathResolver(self.workspace)
        self._print_lock = threading.Lock()  # ç”¨äºŽæŽ§åˆ¶è¾“å‡ºé¡ºåº

    def load_test_cases(self) -> None:
        """ä»ŽJSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.parse_command_string(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            
            # çº¿ç¨‹å®‰å…¨çš„è¾“å‡º
            with self._print_lock:
                print(f"  [Worker] Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            # çº¿ç¨‹å®‰å…¨çš„è¾“å‡º
            if output.strip():
                with self._print_lock:
                    print(f"  [Worker] Command output for {case.name}:")
                    for line in output.splitlines():
                        print(f"    {line}")

            # æ£€æŸ¥è¿”å›žç 
            if "return_code" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # æ£€æŸ¥è¾“å‡ºåŒ…å«
            if "output_contains" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output contains for {case.name}...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # æ£€æŸ¥æ­£åˆ™åŒ¹é…
            if "output_matches" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output matches regex for {case.name}...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/runners/yaml_runner.py
================
from typing import Optional, Dict, Any
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import subprocess
import sys

class YAMLRunner(BaseRunner):
    def __init__(self, config_file="test_cases.yaml", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self):
        """Load test cases from a YAML file."""
        try:
            import yaml
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")
                
                case["command"] = self.path_resolver.parse_command_string(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))
                
            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """Run a single test case and return the result"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/utils/__init__.py
================
# File: /python-test-framework/python-test-framework/src/utils/__init__.py

"""
Utility functions for the CLI Testing Framework
"""

from .path_resolver import PathResolver

__all__ = [
    'PathResolver'
]

================
File: src/cli_test_framework/utils/path_resolver.py
================
from pathlib import Path
from typing import List
import shlex
import os
import shutil

class PathResolver:
    def __init__(self, workspace: Path):
        self.workspace = workspace

    def resolve_paths(self, args: List[str]) -> List[str]:
        resolved_args = []
        for arg in args:
            if not arg.startswith("--"):
                # Only prepend workspace if the path is relative
                if not Path(arg).is_absolute():
                    resolved_args.append(str(self.workspace / arg))
                else:
                    resolved_args.append(arg)
            else:
                resolved_args.append(arg)
        return resolved_args

    def resolve_command(self, command: str) -> str:
        """
        è§£æžå‘½ä»¤è·¯å¾„
        - ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚echo, ping, dirç­‰ï¼‰ä¿æŒåŽŸæ ·
        - å·²å®‰è£…çš„å‘½ä»¤ï¼ˆåœ¨PATHä¸­å¯æ‰¾åˆ°ï¼‰ä¿æŒåŽŸæ ·
        - ç›¸å¯¹è·¯å¾„çš„å¯æ‰§è¡Œæ–‡ä»¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        """
        # å¦‚æžœæ˜¯ç»å¯¹è·¯å¾„ï¼Œä¿æŒåŽŸæ ·
        if Path(command).is_absolute():
            return command
        
        # æ£€æŸ¥å‘½ä»¤æ˜¯å¦åœ¨ç³»ç»ŸPATHä¸­
        if shutil.which(command) is not None:
            return command
        
        # å¸¸è§çš„ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨ï¼ˆä½œä¸ºå¤‡ç”¨ï¼Œä»¥é˜²shutil.whichåœ¨æŸäº›æƒ…å†µä¸‹å¤±æ•ˆï¼‰
        system_commands = {
            'echo', 'ping', 'dir', 'ls', 'cat', 'grep', 'find', 'sort', 
            'head', 'tail', 'wc', 'curl', 'wget', 'git', 'python', 'node',
            'npm', 'pip', 'java', 'javac', 'gcc', 'make', 'cmake', 'docker',
            'kubectl', 'helm', 'terraform', 'ansible', 'ssh', 'scp', 'rsync'
        }
        
        # å¦‚æžœåœ¨ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨ä¸­ï¼Œä¿æŒåŽŸæ ·
        if command in system_commands:
            return command
        
        # å¦åˆ™å½“ä½œç›¸å¯¹è·¯å¾„å¤„ç†
        return str(self.workspace / command)

    def parse_command_string(self, command_string: str) -> str:
        """
        æ™ºèƒ½è§£æžå‘½ä»¤å­—ç¬¦ä¸²ï¼Œæ­£ç¡®å¤„ç†åŒ…å«ç©ºæ ¼çš„è·¯å¾„
        
        Args:
            command_string: åŽŸå§‹å‘½ä»¤å­—ç¬¦ä¸²ï¼Œå¦‚ "python ./script.py" æˆ– r"C:\\Program Files (x86)\\python.exe script.py"
            
        Returns:
            è§£æžåŽçš„å®Œæ•´å‘½ä»¤å­—ç¬¦ä¸²
        """
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æžœå‘½ä»¤å­—ç¬¦ä¸²åŒ…å«å¼•å·ï¼Œä½¿ç”¨shlexè§£æž
        if '"' in command_string or "'" in command_string:
            try:
                # å¯¹äºŽæ‰€æœ‰ç³»ç»Ÿéƒ½ä½¿ç”¨posix=Trueæ¥æ­£ç¡®å¤„ç†å¼•å·
                # è¿™æ ·å¯ä»¥åŽ»æŽ‰å¼•å·ï¼Œä½†ä¿ç•™è·¯å¾„å†…å®¹
                parts = shlex.split(command_string, posix=True)
                
                if not parts:
                    return command_string
                
                # ç¬¬ä¸€éƒ¨åˆ†æ˜¯å‘½ä»¤ï¼Œå…¶ä½™æ˜¯å‚æ•°
                command_part = parts[0]
                remaining_parts = parts[1:]
                
                # è§£æžå‘½ä»¤éƒ¨åˆ†ï¼ˆå¦‚æžœæ˜¯ç»å¯¹è·¯å¾„ï¼Œä¿æŒåŽŸæ ·ï¼›å¦åˆ™è§£æžï¼‰
                if Path(command_part).is_absolute():
                    resolved_command = command_part
                else:
                    resolved_command = self.resolve_command(command_part)
                
                # è§£æžå‚æ•°éƒ¨åˆ†
                resolved_parts = []
                for part in remaining_parts:
                    if part.startswith('-'):
                        # é€‰é¡¹å‚æ•°ï¼Œä¿æŒåŽŸæ ·
                        resolved_parts.append(part)
                    elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                        # çœ‹èµ·æ¥åƒæ–‡ä»¶è·¯å¾„
                        if not Path(part).is_absolute():
                            resolved_parts.append(str(self.workspace / part))
                        else:
                            resolved_parts.append(part)
                    else:
                        # å…¶ä»–å‚æ•°ï¼Œä¿æŒåŽŸæ ·
                        resolved_parts.append(part)
                
                return f"{resolved_command} {' '.join(resolved_parts)}"
                
            except ValueError:
                # shlexè§£æžå¤±è´¥ï¼Œå›žé€€åˆ°ç®€å•å¤„ç†
                pass
        
        # ç®€å•æƒ…å†µï¼šæ²¡æœ‰å¼•å·çš„å‘½ä»¤å­—ç¬¦ä¸²
        # å…ˆå°è¯•è¯†åˆ«æ˜¯å¦ä»¥ç»å¯¹è·¯å¾„å¼€å¤´
        if self._starts_with_absolute_path(command_string):
            # å¤„ç†ä»¥ç»å¯¹è·¯å¾„å¼€å¤´çš„å‘½ä»¤
            return self._parse_absolute_path_command(command_string)
        else:
            # æ™®é€šå‘½ä»¤å¤„ç†
            parts = command_string.split()
            if not parts:
                return command_string
            
            if len(parts) == 1:
                return self.resolve_command(parts[0])
            else:
                command_part = parts[0]
                remaining_parts = parts[1:]
                
                resolved_command = self.resolve_command(command_part)
                resolved_parts = []
                
                for part in remaining_parts:
                    if part.startswith('-'):
                        resolved_parts.append(part)
                    elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                        if not Path(part).is_absolute():
                            resolved_parts.append(str(self.workspace / part))
                        else:
                            resolved_parts.append(part)
                    else:
                        resolved_parts.append(part)
                
                return f"{resolved_command} {' '.join(resolved_parts)}"
    
    def _starts_with_absolute_path(self, command_string: str) -> bool:
        """æ£€æŸ¥å‘½ä»¤å­—ç¬¦ä¸²æ˜¯å¦ä»¥ç»å¯¹è·¯å¾„å¼€å¤´"""
        if os.name == 'nt':  # Windows
            # Windowsç»å¯¹è·¯å¾„æ¨¡å¼ï¼šC:\... æˆ– \\server\...
            return (len(command_string) >= 3 and 
                    command_string[1:3] == ':\\') or command_string.startswith('\\\\')
        else:  # Unix/Linux
            return command_string.startswith('/')
    
    def _parse_absolute_path_command(self, command_string: str) -> str:
        """è§£æžä»¥ç»å¯¹è·¯å¾„å¼€å¤´çš„å‘½ä»¤å­—ç¬¦ä¸²"""
        # å¯¹äºŽWindowsè·¯å¾„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ç©ºæ ¼
        if os.name == 'nt':
            # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª.exeæˆ–.batç­‰å¯æ‰§è¡Œæ–‡ä»¶æ‰©å±•å
            exe_extensions = ['.exe', '.bat', '.cmd', '.com']
            
            for ext in exe_extensions:
                if ext in command_string:
                    # æ‰¾åˆ°å¯æ‰§è¡Œæ–‡ä»¶çš„ç»“æŸä½ç½®
                    ext_pos = command_string.find(ext)
                    if ext_pos != -1:
                        command_end = ext_pos + len(ext)
                        command_part = command_string[:command_end]
                        remaining = command_string[command_end:].strip()
                        
                        if remaining:
                            # è§£æžå‰©ä½™å‚æ•°
                            remaining_parts = remaining.split()
                            resolved_parts = []
                            
                            for part in remaining_parts:
                                if part.startswith('-'):
                                    resolved_parts.append(part)
                                elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                                    if not Path(part).is_absolute():
                                        resolved_parts.append(str(self.workspace / part))
                                    else:
                                        resolved_parts.append(part)
                                else:
                                    resolved_parts.append(part)
                            
                            return f"{command_part} {' '.join(resolved_parts)}"
                        else:
                            return command_part
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°å¯æ‰§è¡Œæ–‡ä»¶æ‰©å±•åï¼Œå›žé€€åˆ°ç®€å•åˆ†å‰²
        parts = command_string.split()
        if not parts:
            return command_string
        
        # å‡è®¾ç¬¬ä¸€ä¸ªéƒ¨åˆ†æ˜¯å‘½ä»¤
        command_part = parts[0]
        remaining_parts = parts[1:]
        
        resolved_parts = []
        for part in remaining_parts:
            if part.startswith('-'):
                resolved_parts.append(part)
            elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                if not Path(part).is_absolute():
                    resolved_parts.append(str(self.workspace / part))
                else:
                    resolved_parts.append(part)
            else:
                resolved_parts.append(part)
        
        return f"{command_part} {' '.join(resolved_parts)}"

================
File: src/cli_test_framework/utils/report_generator.py
================
class ReportGenerator:
    def __init__(self, results: dict, file_path: str):
        self.results = results
        self.file_path = file_path

    def generate_report(self) -> str:
        report = "Test Results Summary:\n"
        report += f"Total Tests: {self.results['total']}\n"
        report += f"Passed: {self.results['passed']}\n"
        report += f"Failed: {self.results['failed']}\n\n"
        
        report += "Detailed Results:\n"
        for detail in self.results['details']:
            status_icon = "âœ“" if detail['status'] == 'passed' else "âœ—"
            report += f"{status_icon} {detail['name']}\n"
            if detail.get('message'):
                report += f"   -> {detail['message']}\n"
        
        # æ·»åŠ å¤±è´¥æ¡ˆä¾‹çš„è¯¦ç»†è¾“å‡ºä¿¡æ¯
        failed_tests = [detail for detail in self.results['details'] if detail['status'] == 'failed']
        if failed_tests:
            report += "\n" + "="*50 + "\n"
            report += "FAILED TEST CASES DETAILS:\n"
            report += "="*50 + "\n\n"
            
            for i, failed_test in enumerate(failed_tests, 1):
                report += f"{i}. Test: {failed_test['name']}\n"
                report += "-" * 40 + "\n"
                
                # æ·»åŠ æ‰§è¡Œçš„å‘½ä»¤
                if failed_test.get('command'):
                    report += f"Command: {failed_test['command']}\n"
                
                # æ·»åŠ è¿”å›žç 
                if failed_test.get('return_code') is not None:
                    report += f"Return Code: {failed_test['return_code']}\n"
                
                # æ·»åŠ å¤±è´¥åŽŸå› 
                if failed_test.get('message'):
                    report += f"Error Message: {failed_test['message']}\n"
                
                # æ·»åŠ å‘½ä»¤çš„å®Œæ•´è¾“å‡ºï¼ˆè¿™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼‰
                if failed_test.get('output'):
                    report += f"\nCommand Output:\n"
                    report += "=" * 30 + "\n"
                    report += f"{failed_test['output']}\n"
                    report += "=" * 30 + "\n"
                
                # æ·»åŠ é”™è¯¯å †æ ˆä¿¡æ¯ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
                if failed_test.get('error_trace'):
                    report += f"Error Trace:\n{failed_test['error_trace']}\n"
                
                # æ·»åŠ æ‰§è¡Œæ—¶é—´ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
                if failed_test.get('duration'):
                    report += f"Duration: {failed_test['duration']}s\n"
                
                report += "\n"
        
        return report

    def save_report(self) -> None:
        report = self.generate_report()
        with open(self.file_path, 'w', encoding='utf-8') as f:
            f.write(report)

    def print_report(self) -> None:
        report = self.generate_report()
        print(report)

================
File: tests/__init__.py
================
# File: /python-test-framework/python-test-framework/tests/__init__.py

# This file is intentionally left blank.

================
File: tests/fixtures/test_cases.json
================
{
  "test_cases": [
    {
      "name": "æµ‹è¯•Pythonç‰ˆæœ¬",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    },
    {
      "name": "æµ‹è¯•ç›®å½•åˆ—è¡¨",
      "command": "dir",
      "args": ["."],
      "expected": {
        "return_code": 0,
        "output_contains": ["src"]
      }
    },
    {
      "name": "æµ‹è¯•echoå‘½ä»¤",
      "command": "echo",
      "args": ["Hello World"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Hello World"]
      }
    },
    {
      "name": "æµ‹è¯•pingæœ¬åœ°å›žçŽ¯",
      "command": "ping",
      "args": ["-n", "1", "127.0.0.1"],
      "expected": {
        "return_code": 0,
        "output_contains": ["127.0.0.1"]
      }
    },
    {
      "name": "æµ‹è¯•æ—¶é—´å‘½ä»¤",
      "command": "echo",
      "args": ["%time%"],
      "expected": {
        "return_code": 0
      }
    },
    {
      "name": "æµ‹è¯•æ–‡ä»¶å­˜åœ¨æ€§",
      "command": "dir",
      "args": ["src"],
      "expected": {
        "return_code": 0,
        "output_contains": ["core", "runners"]
      }
    }
  ]
}

================
File: tests/fixtures/test_cases.yaml
================
name: Sample Test Case
command: python script.py
args:
  - --input
  - input.txt
expected:
  return_code: 0
  output_contains:
    - "Success"
    - "Processed"

================
File: tests/fixtures/test_cases1.json
================
{
    "test_cases": [
        {
            "name": "text_identical_default",
            "description": "é»˜è®¤æ–‡æœ¬æ¯”è¾ƒï¼ˆç›¸åŒæ–‡ä»¶ï¼‰",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "text_different_range",
            "description": "å¸¦è¡ŒèŒƒå›´é™åˆ¶çš„æ–‡æœ¬æ¯”è¾ƒ",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf", 
                "./test/1_copy.bdf",
                "--start-line=93",
                "--end-line=96",
                "--start-column=1",
                "--end-column=30"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_exact_match",
            "description": "ä¸¥æ ¼JSONæ¯”è¾ƒï¼ˆç›¸åŒæ–‡ä»¶ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/1_copy.json",
                "--file-type=json",
                "--json-compare-mode=exact"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_key_based_match",
            "description": "JSONé”®å€¼æ¯”è¾ƒï¼ˆç»“æž„ä¸åŒä½†å…³é”®å­—æ®µåŒ¹é…ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--file-type=json",
                "--json-compare-mode=key-based",
                "--json-key-field=\"phone\""
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "binary_comparison",
            "description": "äºŒè¿›åˆ¶æ–‡ä»¶æ¯”è¾ƒï¼ˆå¸¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/2.bdf",
                "--file-type=binary",
                "--similarity",
                "--chunk-size=4096"
            ],
            "expected": {
                "output_contains": ["Similarity Index"]
                
            }
        },
        {
            "name": "multi_thread_comparison",
            "description": "å¤šçº¿ç¨‹æ–‡æœ¬æ¯”è¾ƒ",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/1_copy.bdf",
                "--num-threads=8",
                "--verbose"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "different_output_format",
            "description": "JSONè¾“å‡ºæ ¼å¼æµ‹è¯•",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--output-format=json"
            ],
            "expected": {
                "output_contains": ["\"position\""]
                
            }
        },
        {
            "name": "auto_detect_filetype",
            "description": "è‡ªåŠ¨æ–‡ä»¶ç±»åž‹æ£€æµ‹",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Auto-detected file type: text"]
                
            }
        },
        {
            "name": "h5_comparison",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒç‰¹å®šè¡¨æ ¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODAL/DISPLACEMENT"],
            "expected": {
                "output_contains": ["Files are different"]
                
            }
        },
        {
            "name": "h5_comparison_all_tables",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒæ‰€æœ‰è¡¨æ ¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5"],
            "expected": {
                "output_contains": ["Files are different."]
                
            }
        },
        {
            "name": "h5_comparison_with_wrong_table",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œé”™è¯¯è¡¨æ ¼è·¯å¾„",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODALl/DISPLACEMENT"],
            "expected": {
                "matches": ["WARNING - Table .* not found"]                
            }
        },
        {
            "name": "h5_comparison_with_strucutre_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œç»“æž„æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-structure-only"],
            "expected": {
                "output_contains": ["Files are identical."]                
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œå†…å®¹æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-show-content-diff"],
            "expected": {
                "output_contains": ["Difference at"]
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œå†…å®¹æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-rtol=1e-5", "--h5-atol=1e-8"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        },
        {
            "name": "h5_comparison_with_table_regex",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œè¡¨æ ¼æ­£åˆ™è¡¨è¾¾å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-table-regex=NASTRAN/RESULT/\\b\\w*al\\b/STRESS"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        }
    ]
}

================
File: tests/performance_test.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œæµ‹è¯•æ€§èƒ½éªŒè¯è„šæœ¬
å¿«é€ŸéªŒè¯å¹¶è¡Œæµ‹è¯•åŠŸèƒ½å’Œæ€§èƒ½æå‡
"""

import sys
import time
import json
import tempfile
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.runners.json_runner import JSONRunner
from src.runners.parallel_json_runner import ParallelJSONRunner

def create_test_config(num_tests=10):
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    test_cases = []
    
    for i in range(num_tests):
        test_cases.append({
            "name": f"æµ‹è¯•ç”¨ä¾‹ {i+1}",
            "command": "echo",
            "args": [f"test_{i+1}"],
            "expected": {
                "return_code": 0,
                "output_contains": [f"test_{i+1}"]
            }
        })
    
    return {"test_cases": test_cases}

def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("=" * 60)
    print("å¹¶è¡Œæµ‹è¯•æ¡†æž¶æ€§èƒ½éªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•é…ç½®
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "perf_test.json")
    
    # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ï¼ˆå¯ä»¥è°ƒæ•´æ•°é‡ï¼‰
    num_tests = 8
    config = create_test_config(num_tests)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"åˆ›å»ºäº† {num_tests} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"æµ‹è¯•é…ç½®æ–‡ä»¶: {config_file}")
    
    results = {}
    
    # 1. é¡ºåºæ‰§è¡Œæµ‹è¯•
    print(f"\n1. é¡ºåºæ‰§è¡Œæµ‹è¯•...")
    start_time = time.time()
    sequential_runner = JSONRunner(config_file, temp_dir)
    seq_success = sequential_runner.run_tests()
    seq_time = time.time() - start_time
    results['sequential'] = {'time': seq_time, 'success': seq_success}
    
    # 2. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼‰
    print(f"\n2. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼Œ4ä¸ªå·¥ä½œçº¿ç¨‹ï¼‰...")
    start_time = time.time()
    parallel_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=4, 
        execution_mode="thread"
    )
    par_success = parallel_runner.run_tests()
    par_time = time.time() - start_time
    results['parallel_thread'] = {'time': par_time, 'success': par_success}
    
    # 3. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼‰
    print(f"\n3. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼Œ2ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
    start_time = time.time()
    process_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=2, 
        execution_mode="process"
    )
    proc_success = process_runner.run_tests()
    proc_time = time.time() - start_time
    results['parallel_process'] = {'time': proc_time, 'success': proc_success}
    
    # æ€§èƒ½åˆ†æž
    print("\n" + "=" * 60)
    print("æ€§èƒ½åˆ†æžç»“æžœ:")
    print("=" * 60)
    
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°é‡:      {num_tests}")
    print(f"é¡ºåºæ‰§è¡Œæ—¶é—´:      {seq_time:.2f} ç§’")
    print(f"å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹):    {par_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x)")
    print(f"å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹):    {proc_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/proc_time:.2f}x)")
    
    # éªŒè¯ç»“æžœä¸€è‡´æ€§
    print(f"\nç»“æžœéªŒè¯:")
    print(f"é¡ºåºæ‰§è¡ŒæˆåŠŸ:      {seq_success}")
    print(f"å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹)æˆåŠŸ: {par_success}")
    print(f"å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹)æˆåŠŸ: {proc_success}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)
    
    # æ€»ç»“
    if all(results[key]['success'] for key in results):
        print(f"\nâœ“ æ‰€æœ‰æµ‹è¯•æ¨¡å¼éƒ½æˆåŠŸæ‰§è¡Œ")
        if par_time < seq_time:
            print(f"âœ“ å¹¶è¡Œæ‰§è¡Œç¡®å®žæå‡äº†æ€§èƒ½")
        else:
            print(f"âš  åœ¨å½“å‰æµ‹è¯•è§„æ¨¡ä¸‹ï¼Œå¹¶è¡Œä¼˜åŠ¿ä¸æ˜Žæ˜¾")
    else:
        print(f"\nâœ— éƒ¨åˆ†æµ‹è¯•æ¨¡å¼æ‰§è¡Œå¤±è´¥")
        return False
    
    return True

if __name__ == "__main__":
    try:
        success = run_performance_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)

================
File: tests/test_comprehensive_space.py
================
#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•åŒ…å«ç©ºæ ¼çš„è·¯å¾„è§£æžåŠŸèƒ½
"""

import os
import sys
import tempfile
import json
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.runners.json_runner import JSONRunner

def test_comprehensive_space_handling():
    """å…¨é¢æµ‹è¯•åŒ…å«ç©ºæ ¼çš„è·¯å¾„å¤„ç†"""
    print("=" * 60)
    print("å…¨é¢æµ‹è¯•åŒ…å«ç©ºæ ¼çš„è·¯å¾„è§£æžåŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæµ‹è¯•æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "comprehensive_test.json")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®ï¼ŒåŒ…å«å„ç§å¯èƒ½çš„ç©ºæ ¼è·¯å¾„åœºæ™¯
    test_config = {
        "test_cases": [
            {
                "name": "ç®€å•å‘½ä»¤",
                "command": "echo hello",
                "args": [],
                "expected": {
                    "return_code": 0,
                    "output_contains": ["hello"]
                }
            },
            {
                "name": "å¸¦å¼•å·çš„Windowsè·¯å¾„",
                "command": '"C:\\Program Files (x86)\\Python\\python.exe" --version',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "ä¸å¸¦å¼•å·çš„Windowsè·¯å¾„",
                "command": "C:\\Program Files (x86)\\Python\\python.exe --version",
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "ç›¸å¯¹è·¯å¾„è„šæœ¬",
                "command": "python script.py",
                "args": ["--verbose"],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "å¤æ‚å‘½ä»¤å¸¦å‚æ•°",
                "command": "node app.js",
                "args": ["--port", "3000", "--env", "development"],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "å¸¦ç©ºæ ¼çš„Unixè·¯å¾„",
                "command": '"/usr/local/bin/my app" --help',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            }
        ]
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    print(f"æµ‹è¯•é…ç½®æ–‡ä»¶: {config_file}")
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    print()
    
    # æµ‹è¯•åŠ è½½é…ç½®
    try:
        runner = JSONRunner(config_file, temp_dir)
        runner.load_test_cases()
        
        print(f"æˆåŠŸåŠ è½½ {len(runner.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹:")
        print()
        
        for i, case in enumerate(runner.test_cases, 1):
            print(f"{i}. {case.name}")
            print(f"   åŽŸå§‹å‘½ä»¤: {test_config['test_cases'][i-1]['command']}")
            print(f"   è§£æžåŽå‘½ä»¤: {case.command}")
            print(f"   å‚æ•°: {case.args}")
            print()
        
        print("=" * 60)
        print("å‘½ä»¤è§£æžæµ‹è¯•å®Œæˆï¼")
        print("æ‰€æœ‰åŒ…å«ç©ºæ ¼çš„è·¯å¾„éƒ½å·²æ­£ç¡®è§£æžã€‚")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # æ¸…ç†
    import shutil
    shutil.rmtree(temp_dir)

if __name__ == "__main__":
    test_comprehensive_space_handling()

================
File: tests/test_parallel_runner.py
================
import unittest
import tempfile
import json
import os
import sys
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner

class TestParallelRunner(unittest.TestCase):
    """å¹¶è¡Œè¿è¡Œå™¨æµ‹è¯•ç±»"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•çŽ¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.config_file = os.path.join(self.temp_dir, "test_config.json")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = {
            "test_cases": [
                {
                    "name": "æµ‹è¯•1",
                    "command": "echo",
                    "args": ["test1"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test1"]
                    }
                },
                {
                    "name": "æµ‹è¯•2",
                    "command": "echo",
                    "args": ["test2"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test2"]
                    }
                },
                {
                    "name": "æµ‹è¯•3",
                    "command": "echo",
                    "args": ["test3"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test3"]
                    }
                },
                {
                    "name": "æµ‹è¯•4",
                    "command": "echo",
                    "args": ["test4"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test4"]
                    }
                }
            ]
        }
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•çŽ¯å¢ƒ"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def test_parallel_vs_sequential_performance(self):
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œç›¸æ¯”é¡ºåºæ‰§è¡Œçš„æ€§èƒ½æå‡"""
        # é¡ºåºæ‰§è¡Œ
        sequential_runner = JSONRunner(self.config_file, self.temp_dir)
        start_time = time.time()
        seq_success = sequential_runner.run_tests()
        seq_time = time.time() - start_time
        
        # å¹¶è¡Œæ‰§è¡Œ
        parallel_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        start_time = time.time()
        par_success = parallel_runner.run_tests()
        par_time = time.time() - start_time
        
        # éªŒè¯ç»“æžœ
        self.assertTrue(seq_success, "é¡ºåºæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertTrue(par_success, "å¹¶è¡Œæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertEqual(
            sequential_runner.results["total"], 
            parallel_runner.results["total"],
            "æµ‹è¯•æ€»æ•°åº”è¯¥ç›¸åŒ"
        )
        self.assertEqual(
            sequential_runner.results["passed"], 
            parallel_runner.results["passed"],
            "é€šè¿‡çš„æµ‹è¯•æ•°åº”è¯¥ç›¸åŒ"
        )
        
        print(f"\næ€§èƒ½æ¯”è¾ƒ:")
        print(f"é¡ºåºæ‰§è¡Œæ—¶é—´: {seq_time:.3f}ç§’")
        print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {par_time:.3f}ç§’")
        if par_time > 0:
            print(f"åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x")
    
    def test_thread_vs_process_mode(self):
        """æµ‹è¯•çº¿ç¨‹æ¨¡å¼å’Œè¿›ç¨‹æ¨¡å¼"""
        # çº¿ç¨‹æ¨¡å¼
        thread_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        thread_success = thread_runner.run_tests()
        
        # è¿›ç¨‹æ¨¡å¼
        process_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="process"
        )
        process_success = process_runner.run_tests()
        
        # éªŒè¯ç»“æžœ
        self.assertTrue(thread_success, "çº¿ç¨‹æ¨¡å¼åº”è¯¥æˆåŠŸ")
        self.assertTrue(process_success, "è¿›ç¨‹æ¨¡å¼åº”è¯¥æˆåŠŸ")
        self.assertEqual(
            thread_runner.results["passed"], 
            process_runner.results["passed"],
            "ä¸¤ç§æ¨¡å¼çš„é€šè¿‡æµ‹è¯•æ•°åº”è¯¥ç›¸åŒ"
        )
    
    def test_max_workers_configuration(self):
        """æµ‹è¯•ä¸åŒçš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°é…ç½®"""
        for max_workers in [1, 2, 4]:
            with self.subTest(max_workers=max_workers):
                runner = ParallelJSONRunner(
                    self.config_file, 
                    self.temp_dir, 
                    max_workers=max_workers, 
                    execution_mode="thread"
                )
                success = runner.run_tests()
                self.assertTrue(success, f"max_workers={max_workers}æ—¶åº”è¯¥æˆåŠŸ")
                self.assertEqual(runner.results["passed"], 4, "åº”è¯¥é€šè¿‡4ä¸ªæµ‹è¯•")
    
    def test_fallback_to_sequential(self):
        """æµ‹è¯•å›žé€€åˆ°é¡ºåºæ‰§è¡Œ"""
        runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        
        # æµ‹è¯•å›žé€€åŠŸèƒ½
        success = runner.run_tests_sequential()
        self.assertTrue(success, "å›žé€€åˆ°é¡ºåºæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertEqual(runner.results["passed"], 4, "åº”è¯¥é€šè¿‡4ä¸ªæµ‹è¯•")

if __name__ == '__main__':
    unittest.main()

================
File: tests/test_parallel_space.py
================
#!/usr/bin/env python3
"""
æµ‹è¯•å¹¶è¡Œè¿è¡Œå™¨çš„ç©ºæ ¼è·¯å¾„å¤„ç†
"""

import os
import sys
import tempfile
import json

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.runners.parallel_json_runner import ParallelJSONRunner

def test_parallel_space_handling():
    """æµ‹è¯•å¹¶è¡Œè¿è¡Œå™¨çš„ç©ºæ ¼è·¯å¾„å¤„ç†"""
    print("=" * 60)
    print("æµ‹è¯•å¹¶è¡Œè¿è¡Œå™¨çš„ç©ºæ ¼è·¯å¾„å¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæµ‹è¯•æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "parallel_space_test.json")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        "test_cases": [
            {
                "name": "ç®€å•å‘½ä»¤",
                "command": "echo hello",
                "args": [],
                "expected": {
                    "return_code": 0,
                    "output_contains": ["hello"]
                }
            },
            {
                "name": "å¸¦å¼•å·çš„è·¯å¾„",
                "command": '"C:\\Program Files (x86)\\Python\\python.exe" --version',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "ä¸å¸¦å¼•å·çš„è·¯å¾„",
                "command": "C:\\Program Files (x86)\\Python\\python.exe --version",
                "args": [],
                "expected": {
                    "return_code": 0
                }
            }
        ]
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    print(f"æµ‹è¯•é…ç½®æ–‡ä»¶: {config_file}")
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    print()
    
    # æµ‹è¯•å¹¶è¡Œè¿è¡Œå™¨
    try:
        runner = ParallelJSONRunner(
            config_file, 
            temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        runner.load_test_cases()
        
        print(f"æˆåŠŸåŠ è½½ {len(runner.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹:")
        for i, case in enumerate(runner.test_cases, 1):
            print(f"{i}. {case.name}")
            print(f"   è§£æžåŽå‘½ä»¤: {case.command}")
            print(f"   å‚æ•°: {case.args}")
        
        print("\n" + "=" * 60)
        print("å¹¶è¡Œè¿è¡Œå™¨ç©ºæ ¼è·¯å¾„è§£æžæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # æ¸…ç†
    import shutil
    shutil.rmtree(temp_dir)

if __name__ == "__main__":
    test_parallel_space_handling()

================
File: tests/test_report.txt
================
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Detailed Results:
âœ“ text_identical_default
âœ“ text_different_range
âœ“ json_exact_match
âœ“ json_key_based_match
âœ“ binary_comparison
âœ“ multi_thread_comparison
âœ“ different_output_format
âœ“ auto_detect_filetype
âœ“ h5_comparison
âœ“ h5_comparison_all_tables
âœ“ h5_comparison_with_wrong_table
âœ“ h5_comparison_with_strucutre_mode
âœ“ h5_comparison_with_content_mode
âœ“ h5_comparison_with_content_mode
âœ“ h5_comparison_with_table_regex

================
File: tests/test_runners.py
================
import unittest
from src.runners.json_runner import JSONRunner
from src.runners.yaml_runner import YAMLRunner

class TestJSONRunner(unittest.TestCase):
    def setUp(self):
        self.runner = JSONRunner("tests/fixtures/test_cases.json")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

class TestYAMLRunner(unittest.TestCase):
    def setUp(self):
        self.runner = YAMLRunner("tests/fixtures/test_cases.yaml")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

if __name__ == "__main__":
    unittest.main()

================
File: tests/test1.py
================
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import unittest
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator

def main():
    runner = JSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases1.json",
        workspace="D:/Document/xcode/Compare-File-Tool"
    )
    success = runner.run_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_generator = ReportGenerator(
        runner.results, 
        "D:/Document/xcode/cli-test-framework/tests/test_report.txt"
    )
    report_generator.print_report()  # æ‰“å°åˆ°æŽ§åˆ¶å°
    report_generator.save_report()   # ä¿å­˜åˆ°æ–‡ä»¶
    
    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: D:/Document/xcode/cli-test-framework/tests/test_report.txt")
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: User_Manual.md
================
# ðŸ“˜ CLI-Test-Framework ä½¿ç”¨æ‰‹å†Œ

æœ¬æ¡†æž¶æ˜¯ä¸€ä¸ªç”¨äºŽå‘½ä»¤è¡Œå·¥å…·çš„è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æž¶ï¼Œæ”¯æŒ **JSON/YAML æµ‹è¯•é…ç½®æ–‡ä»¶**ã€**é¡ºåºæ‰§è¡Œ**ä¸Ž**å¹¶è¡Œæ‰§è¡Œï¼ˆçº¿ç¨‹/è¿›ç¨‹ï¼‰**ï¼Œå¯è‡ªåŠ¨æ¯”å¯¹è¾“å‡ºã€è¿”å›žç ï¼Œå¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šã€‚

------

## âœ… å®‰è£…æ–¹å¼

### ä»Ž PyPI å®‰è£…ï¼š

```bash
pip install cli-test-framework
```

------

## ðŸ“‚ é¡¹ç›®ç»“æž„æŽ¨è

```bash
your_project/
â”œâ”€â”€ test_cases.json          # æµ‹è¯•ç”¨ä¾‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ test_report.txt          # æµ‹è¯•æŠ¥å‘Šï¼ˆå¯é€‰è¾“å‡ºï¼‰
â””â”€â”€ run_tests.py             # æµ‹è¯•æ‰§è¡Œè„šæœ¬
```

------

## ðŸ§ª ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹ï¼ˆJSON æ ¼å¼ï¼‰

```json
{
  "test_cases": [
    {
      "name": "ç‰ˆæœ¬æ£€æŸ¥æµ‹è¯•",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    }
  ]
}
```

------

## ðŸš€ å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹

### é¡ºåºæ‰§è¡Œæµ‹è¯•

```python
from cli_test_framework.runners import JSONRunner

runner = JSONRunner(config_file="test_cases.json", workspace=".")
runner.run_tests()
```

### å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼‰

```python
from cli_test_framework.runners import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,
    execution_mode="thread"  # å¯ä¸º "thread" æˆ– "process"
)
runner.run_tests()
```

------

## ðŸ“„ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š

```python
from cli_test_framework.utils import ReportGenerator

report = ReportGenerator(runner.results, "test_report.txt")
report.print_report()  # æ‰“å°è‡³ç»ˆç«¯
report.save_report()   # ä¿å­˜è‡³æ–‡ä»¶
```

------

## âš™ï¸ æ”¯æŒçš„å­—æ®µè¯´æ˜Ž

| å­—æ®µ              | ç±»åž‹      | è¯´æ˜Ž                               |
| ----------------- | --------- | ---------------------------------- |
| `name`            | str       | æµ‹è¯•åç§°                           |
| `command`         | str       | è¦æ‰§è¡Œçš„å‘½ä»¤ï¼ˆå¯ä¸ºç³»ç»Ÿå‘½ä»¤æˆ–è„šæœ¬ï¼‰ |
| `args`            | List[str] | å‘½ä»¤å‚æ•°                           |
| `expected`        | dict      | é¢„æœŸç»“æžœ                           |
| `return_code`     | int       | é¢„æœŸè¿”å›žå€¼ï¼ˆå¯é€‰ï¼‰                 |
| `output_contains` | List[str] | è¾“å‡ºä¸­å¿…é¡»åŒ…å«çš„å†…å®¹ï¼ˆå¯é€‰ï¼‰       |
| `output_matches`  | str       | è¾“å‡ºéœ€åŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¯é€‰ï¼‰     |



------

## ðŸ§  å¹¶è¡Œæ‰§è¡Œè¯´æ˜Ž

### æ‰§è¡Œæ¨¡å¼é€‰é¡¹ï¼š

| æ¨¡å¼      | è¯´æ˜Ž                              | é€‚ç”¨åœºæ™¯        |
| --------- | --------------------------------- | --------------- |
| `thread`  | å¤šçº¿ç¨‹ï¼Œé€‚åˆ I/O å¯†é›†åž‹æµ‹è¯•       | ç½‘ç»œ/æ–‡ä»¶æ“ä½œ   |
| `process` | å¤šè¿›ç¨‹ï¼Œé€‚åˆ CPU å¯†é›†åž‹ã€éš”ç¦»éœ€æ±‚ | é‡è®¡ç®—/å´©æºƒæµ‹è¯• |



### è®¾ç½®æœ€å¤§å¹¶å‘æ•°ï¼š

```python
import os
max_workers = os.cpu_count() * 2  # æŽ¨èå€¼
```

------

## ðŸ“¦ é«˜çº§ç”¨æ³•

- æ”¯æŒ YAML æµ‹è¯•æ–‡ä»¶ï¼šä½¿ç”¨ `YAMLRunner`
- è‡ªå®šä¹‰æ–­è¨€æ¨¡å—ï¼šç»§æ‰¿ `Assertions` ç±»æ·»åŠ æ–°è§„åˆ™
- è‡ªå®šä¹‰æµ‹è¯•æ ¼å¼ï¼šç»§æ‰¿ `BaseRunner`

------

## ðŸ›  å¸¸è§é—®é¢˜æŽ’æŸ¥

| é—®é¢˜                     | å¯èƒ½åŽŸå›                                     |
| ------------------------ | ------------------------------------------- |
| å‘½ä»¤æœªæ‰§è¡ŒæˆåŠŸ           | command è·¯å¾„é”™è¯¯ / çŽ¯å¢ƒæœªæ¿€æ´»               |
| output_contains æ–­è¨€å¤±è´¥ | è¾“å‡ºä¸º stderr è€Œéž stdout                   |
| å¹¶è¡Œæ¨¡å¼ä¸‹æŠ¥é”™           | å¯èƒ½ä¸º `args` æˆ– `command` å«ä¸å¯åºåˆ—åŒ–å¯¹è±¡ |



------

## ðŸ§ª å¿«é€Ÿæ€§èƒ½å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰

è¿è¡Œå†…ç½®æ€§èƒ½æµ‹è¯•è„šæœ¬ï¼š

```bash
python tests/performance_test.py
```

è¾“å‡ºç¤ºä¾‹ï¼š

```makefile
é¡ºåºæ‰§è¡Œæ—¶é—´:      2.34 ç§’
å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹):    0.88 ç§’ (åŠ é€Ÿæ¯”: 2.66x)
```

------

## ðŸ“Ž é™„åŠ è¯´æ˜Ž

- **æ”¯æŒå¹³å°**ï¼šWindows / Linux / macOS
- **Python ç‰ˆæœ¬**ï¼š3.6+
- **ä¾èµ–é¡¹**ï¼š
  - `PyYAML`ï¼ˆä»…ç”¨äºŽ YAMLRunnerï¼‰
  - å…¶ä½™ä½¿ç”¨æ ‡å‡†åº“ï¼ˆæ— é¢å¤–ä¾èµ–ï¼‰

------

## ðŸ“¬ è”ç³»æ–¹å¼ï¼ˆå¯é€‰ï¼‰

ä½œè€…ï¼šXiaotong Wang
 é‚®ç®±ï¼šxiaotongwang98@gmail.com
 GitHubï¼š`https://github.com/ozil111/cli-test-framework`
