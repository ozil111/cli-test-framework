This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-27T05:55:03.407Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
Example_usage.py
parallel_example.py
PARALLEL_TESTING_GUIDE.md
README_cn.md
README.md
requirements.txt
setup.py
src/__init__.py
src/core/__init__.py
src/core/assertions.py
src/core/base_runner.py
src/core/parallel_runner.py
src/core/process_worker.py
src/core/test_case.py
src/runners/__init__.py
src/runners/json_runner.py
src/runners/parallel_json_runner.py
src/runners/yaml_runner.py
src/utils/__init__.py
src/utils/path_resolver.py
src/utils/report_generator.py
tests/__init__.py
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json
tests/performance_test.py
tests/test_parallel_runner.py
tests/test_report.txt
tests/test_runners.py
tests/test1.py

================================================================
Files
================================================================

================
File: Example_usage.py
================
# Example usage
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator
import sys

def main():
    runner = JSONRunner(config_file="D:/Document/xcode/Compare-File-Tool/test_script/test_cases.json", workspace="D:/Document/xcode/Compare-File-Tool")
    success = runner.run_tests()
    
    # Generate and save the report
    report_generator = ReportGenerator(runner.results, "D:/Document/xcode/Compare-File-Tool/test_script/test_report.txt")
    report_generator.print_report()
    report_generator.save_report()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: parallel_example.py
================
# 并行测试示例用法
from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner
from src.utils.report_generator import ReportGenerator
import sys
import time

def run_sequential_test():
    """运行顺序测试"""
    print("=" * 60)
    print("运行顺序测试...")
    print("=" * 60)
    
    start_time = time.time()
    runner = JSONRunner(config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", workspace=".")
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\n顺序测试完成，耗时: {end_time - start_time:.2f} 秒")
    return success, runner.results, end_time - start_time

def run_parallel_test(max_workers=None, execution_mode="thread"):
    """运行并行测试"""
    print("=" * 60)
    print(f"运行并行测试 (模式: {execution_mode}, 工作线程: {max_workers or 'auto'})...")
    print("=" * 60)
    
    start_time = time.time()
    runner = ParallelJSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", 
        workspace=".",
        max_workers=max_workers,
        execution_mode=execution_mode
    )
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\n并行测试完成，耗时: {end_time - start_time:.2f} 秒")
    return success, runner.results, end_time - start_time

def main():
    """主函数：比较顺序和并行测试的性能"""
    
    # 运行顺序测试
    seq_success, seq_results, seq_time = run_sequential_test()
    
    # 运行并行测试（线程模式）
    par_success, par_results, par_time = run_parallel_test(max_workers=4, execution_mode="thread")
    
    # 运行并行测试（进程模式）
    proc_success, proc_results, proc_time = run_parallel_test(max_workers=2, execution_mode="process")
    
    # 性能比较
    print("\n" + "=" * 60)
    print("性能比较结果:")
    print("=" * 60)
    print(f"顺序执行时间:     {seq_time:.2f} 秒")
    print(f"并行执行时间(线程): {par_time:.2f} 秒 (加速比: {seq_time/par_time:.2f}x)")
    print(f"并行执行时间(进程): {proc_time:.2f} 秒 (加速比: {seq_time/proc_time:.2f}x)")
    
    # 生成报告
    if par_success:
        report_generator = ReportGenerator(par_results, "parallel_test_report.txt")
        report_generator.print_report()
        report_generator.save_report()
        print(f"\n并行测试报告已保存到: parallel_test_report.txt")
    
    # 返回最终结果
    return seq_success and par_success and proc_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

================
File: PARALLEL_TESTING_GUIDE.md
================
# 并行测试功能使用指南

## 概述

你的测试框架现在支持并行测试执行，可以显著提升测试执行效率。框架提供了两种并行执行模式：**线程模式**和**进程模式**。

## 功能特性

### ✅ 已实现的功能

- **多线程并行执行**：适用于I/O密集型测试
- **多进程并行执行**：适用于CPU密集型测试，提供完全隔离的执行环境
- **可配置并发数**：支持自定义最大工作线程/进程数
- **线程安全设计**：确保测试结果的正确性和输出的清晰性
- **性能监控**：提供执行时间统计和加速比分析
- **向后兼容**：完全兼容现有的顺序执行代码

### 📊 性能提升

根据测试结果，并行执行可以带来显著的性能提升：

- **线程模式**：通常可获得 **2-4倍** 的加速比
- **进程模式**：适合需要完全隔离的场景，但启动开销较大

## 使用方法

### 基本用法

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# 创建并行运行器
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # 最大并发数
    execution_mode="thread"  # 执行模式：thread 或 process
)

# 运行测试
success = runner.run_tests()
```

### 配置选项

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `config_file` | str | "test_cases.json" | 测试配置文件路径 |
| `workspace` | str | None | 工作目录 |
| `max_workers` | int | None (自动) | 最大并发数 |
| `execution_mode` | str | "thread" | 执行模式：thread/process |

### 执行模式选择

#### 线程模式 (thread)
- **适用场景**：I/O密集型测试（如网络请求、文件操作）
- **优势**：启动快，内存共享，适合大多数测试场景
- **推荐并发数**：CPU核心数 × 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### 进程模式 (process)
- **适用场景**：需要完全隔离的测试，CPU密集型任务
- **优势**：完全隔离，避免GIL限制
- **推荐并发数**：CPU核心数

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json", 
    max_workers=2,
    execution_mode="process"
)
```

## 示例代码

### 性能比较示例

```python
# 运行性能比较
python parallel_example.py

# 输出示例：
# 顺序执行时间:     0.12 秒
# 并行执行时间(线程): 0.03 秒 (加速比: 3.58x)
# 并行执行时间(进程): 0.10 秒 (加速比: 1.15x)
```

### 快速验证

```python
# 快速验证并行功能
python test_parallel_simple.py

# 快速性能测试
python performance_test.py
```

## 最佳实践

### 1. 选择合适的并发数

```python
import os

# CPU密集型任务
max_workers = os.cpu_count()

# I/O密集型任务  
max_workers = os.cpu_count() * 2
```

### 2. 测试用例设计原则

- ✅ **确保测试独立性**：测试用例之间不应有依赖关系
- ✅ **避免共享资源冲突**：不同测试不应操作相同的文件或端口
- ✅ **使用相对路径**：框架会自动处理路径解析

### 3. 错误处理

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # 检查失败的测试
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"失败的测试: {detail['name']}")
                print(f"错误信息: {detail['message']}")
                
except Exception as e:
    print(f"执行出错: {e}")
    # 回退到顺序执行
    runner.run_tests_sequential()
```

## 技术实现

### 架构设计

```
ParallelRunner (基类)
├── 线程安全的结果收集
├── 可配置的执行模式
└── 异常处理机制

ParallelJSONRunner (实现类)
├── 继承 ParallelRunner
├── JSON配置解析
└── 路径解析功能

进程工作器 (process_worker.py)
├── 独立进程执行
├── 避免序列化问题
└── 完全隔离环境
```

### 线程安全机制

- **结果收集锁**：`threading.Lock()` 保护共享结果数据
- **输出控制锁**：避免并发输出混乱
- **异常隔离**：单个测试失败不影响其他测试

## 故障排除

### 常见问题

1. **进程模式序列化错误**
   - 原因：对象包含不可序列化的属性（如锁）
   - 解决：使用独立的进程工作器函数

2. **路径解析错误**
   - 原因：系统命令被当作相对路径处理
   - 解决：更新 `PathResolver` 的系统命令列表

3. **性能提升不明显**
   - 原因：测试用例执行时间太短，并行开销大于收益
   - 解决：增加测试用例数量或使用更复杂的测试

### 调试技巧

```python
# 启用详细输出
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=1,  # 设为1便于调试
    execution_mode="thread"
)

# 查看详细结果
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 版本兼容性

- **Python版本**：3.6+
- **依赖项**：无额外依赖，使用标准库
- **向后兼容**：完全兼容现有的 `JSONRunner` 代码

## 总结

并行测试功能为你的测试框架带来了显著的性能提升，特别适合：

- 🚀 **大规模测试套件**：数十个或数百个测试用例
- 🌐 **I/O密集型测试**：网络请求、文件操作等
- ⚡ **CI/CD流水线**：缩短构建时间
- 🔄 **回归测试**：快速验证代码变更

通过合理配置并发参数和选择适当的执行模式，你可以在保证测试可靠性的同时，大幅提升测试执行效率！

================
File: README_cn.md
================
# Command line 测试框架开发文档

## 1. 概述
本测试框架是一个轻量级、可扩展的自动化测试解决方案，支持通过JSON/YAML格式定义测试用例，提供完整的测试执行、结果验证和报告生成功能。核心目标是为命令行工具和脚本提供标准化的测试管理能力，支持跨平台测试场景。

## 2. 功能特点
- **模块化架构**：核心组件解耦设计（运行器/断言/报告）
- **多格式支持**：原生支持JSON/YAML测试用例格式
- **并行测试执行**：支持多线程和多进程并行测试，显著提升执行效率
- **智能路径解析**：自动处理相对路径与绝对路径转换
- **丰富断言机制**：包含返回值校验、输出内容匹配、正则表达式验证
- **可扩展接口**：通过继承BaseRunner可快速实现新测试格式支持
- **执行环境隔离**：独立子进程运行保证测试隔离性
- **诊断报告**：提供通过率统计和失败详情定位

## 3. 使用说明

### 环境要求
```bash
pip install -r requirements.txt
Python >= 3.6
```

### 快速开始

1. 创建测试用例文件（示例见`tests/fixtures/`）
2. 编写执行脚本：

**顺序执行**：
```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

**并行执行**：
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# 使用多线程并行执行
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # 最大并发数
    execution_mode="thread"  # 执行模式：thread 或 process
)
success = runner.run_tests()

# 性能比较示例
python parallel_example.py
```

### 测试用例格式示例

**JSON格式**：

```json
{
  "test_cases": [
    {
      "name": "文件比对测试",
      "command": "diff",
      "args": ["file1.txt", "file2.txt"],
      "expected": {
        "return_code": 0,
        "output_contains": ["identical"]
      }
    }
  ]
}
```

**YAML格式**：

```yaml
test_cases:
  - name: 目录扫描测试
    command: ls
    args: 
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\.md$"
```

## 4. 系统流程

### 架构模块

```mermaid
graph TD
    A[测试用例] --> B[Runner]
    B --> C[路径解析]
    B --> D[子进程执行]
    D --> E[断言验证]
    E --> F[结果收集]
    F --> G[报告生成]
```

### 核心模块说明

1. **Test Runner**

   - 加载测试配置
   - 管理测试生命周期
   - 协调各组件协作

2. **PathResolver**

   ```python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   智能处理路径参数，自动将相对路径转换为基于workspace的绝对路径

3. **Assertion Engine**

   - 返回值校验（return_code）
   - 输出内容匹配（contains/matches）
   - 异常捕获机制

4. **Report Generator**

   - 实时统计测试进度
   - 生成带错误定位的详细报告
   - 支持控制台输出和文件保存

## 5. 代码实现详解

### 核心类说明

**TestCase 数据类**：

```python
@dataclass
class TestCase:
    name: str          # 测试名称
    command: str       # 执行命令/程序
    args: List[str]    # 参数列表
    expected: Dict[str, Any]  # 预期结果
```

**BaseRunner 抽象类**：

```python
def run_tests(self) -> bool:
    self.load_test_cases()
    for case in self.test_cases:
        result = self.run_single_test(case)
        # 结果收集逻辑...
    return self.results["failed"] == 0
```

**JSONRunner 实现**：

```python
def load_test_cases(self):
    with open(config_path) as f:
        cases = json.load(f)["test_cases"]
        # 字段校验和路径预处理
        case["command"] = self.path_resolver.resolve_command(case["command"])
```

**断言子系统**：

```python
class Assertions:
    @staticmethod
    def matches(text, pattern):
        if not re.search(pattern, text):
            raise AssertionError(f"Pattern mismatch: {pattern}")
```

## 6. 健壮性设计

### 可靠性保障措施

- **输入验证**：强制校验测试用例必填字段

- **错误隔离**：单个测试失败不影响后续执行

- **子进程防护**：

  ```python
  subprocess.run(..., check=False, shell=True)
  ```

- **路径安全**：自动处理路径分隔符差异

- **异常捕获**：三级错误处理（断言错误/执行错误/系统错误）

- **结果完整性**：确保所有测试结果都被记录

## 7. 扩展性设计

### 扩展方向

1. **格式扩展**：实现XMLRunner/TomlRunner
2. **执行器扩展**：支持Docker/SSH远程执行
3. **断言扩展**：添加性能指标断言
4. **报告格式**：支持HTML/PDF报告生成
5. **分布式执行**：多进程并行测试

扩展示例（新增XML运行器）：

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # 解析XML结构并转换为TestCase对象
```

## 8. 并行测试功能

### 8.1 并行测试概述

框架支持两种并行执行模式：
- **线程模式（thread）**：适用于I/O密集型测试，共享内存空间
- **进程模式（process）**：适用于CPU密集型测试，完全隔离执行环境

### 8.2 并行测试配置

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# 线程模式配置
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # 最大并发线程数
    execution_mode="thread"  # 线程模式
)

# 进程模式配置
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=2,           # 最大并发进程数
    execution_mode="process" # 进程模式
)
```

### 8.3 性能优势

并行测试可以显著提升测试执行效率：

```bash
# 运行性能比较示例
python parallel_example.py

# 典型输出：
# 顺序执行时间:     12.45 秒
# 并行执行时间(线程): 3.21 秒 (加速比: 3.88x)
# 并行执行时间(进程): 4.12 秒 (加速比: 3.02x)
```

### 8.4 线程安全设计

- **结果收集**：使用线程锁确保测试结果安全更新
- **输出控制**：通过打印锁避免输出混乱
- **异常处理**：每个工作线程独立处理异常

### 8.5 最佳实践

1. **选择合适的并发数**：
   - CPU密集型：`max_workers = CPU核心数`
   - I/O密集型：`max_workers = CPU核心数 * 2-4`

2. **选择合适的执行模式**：
   - 测试间无依赖：推荐线程模式
   - 需要完全隔离：推荐进程模式

3. **测试用例设计**：
   - 确保测试用例间无依赖关系
   - 避免共享资源冲突

### 8.6 单元测试

运行并行功能的单元测试：

```bash
python -m pytest tests/test_parallel_runner.py -v
```

## 9. 示例演示

### 输入样例

```json
{
  "test_cases": [
    {
      "name": "版本检查测试",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "output_matches": "Python 3\\.[89]\\.",
        "return_code": 0
      }
    }
  ]
}
```

### 输出报告

```
Test Results Summary:
Total Tests: 1
Passed: 1 (100.0%)
Failed: 0 (0.0%)

Detailed Results:
✓ 版本检查测试
```

## 9. 注意事项

1. **路径处理**：
   - 使用`--`开头的参数不会被路径转换
   - Windows路径需使用`/`或转义`\\`
2. **命令限制**：
   - 仅支持单命令执行
   - 复杂管道需封装为脚本
3. **安全规范**：
   - 不要以root权限运行
   - 禁止执行不可信测试用例
4. **性能注意**：
   - 单个测试超时默认无限制
   - 建议I/O密集型测试自行控制并发
5. **环境依赖**：
   - 需预先安装被测程序
   - Python路径需在系统PATH中

================
File: README.md
================
# Command line Testing Framework Development Documentation

## 1. Overview

This testing framework is a lightweight and extensible automated testing solution that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The core objective is to provide standardized test management capabilities for command-line tools and scripts, supporting cross-platform testing scenarios.

## 2. Features

- **Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **Multi-Format Support**: Native support for JSON/YAML test case formats
- **Intelligent Path Resolution**: Automatic handling of relative and absolute path conversions
- **Rich Assertion Mechanism**: Includes return value validation, output content matching, regular expression verification
- **Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **Diagnostic Reports**: Provides pass rate statistics and failure detail localization

## 3. Usage Instructions

### Environment Requirements

```Bash
pip install -r requirements.txt
Python >= 3.6
```

### Quick Start

1. Create test case files (examples in `tests/fixtures/`)
2. Write an execution script:

```Python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Test Case Format Examples

**JSON Format**:

```JSON
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "diff",
            "args": ["file1.txt", "file2.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["identical"]
            }
        }
    ]
}
```

**YAML Format**:

```YAML
test_cases:
    - name: Directory Scan Test
      command: ls
      args:
        - -l
        - docs/
      expected:
        return_code: 0
        output_matches: ".*\.md$"
```

## 4. System Flow

### Architecture Modules

```mermaid
graph TD
    A[Test Cases] --> B[Runner]
    B --> C[Path Resolution]
    B --> D[Sub-process Execution]
    D --> E[Assertion Verification]
    E --> F[Result Collection]
    F --> G[Report Generation]
```

### Core Module Description

1. **Test Runner**

   - Loads test configurations
   - Manages test lifecycle
   - Coordinates component collaboration

2. **PathResolver**

   ```Python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   Intelligently handles path parameters, automatically converting relative paths to absolute paths based on the workspace.

3. **Assertion Engine**

   - Return value validation (return_code)
   - Output content matching (contains/matches)
   - Exception capture mechanism

4. **Report Generator**

   - Real-time test progress statistics
   - Generates detailed reports with error localization
   - Supports console output and file saving

## 5. Detailed Code Implementation

### Core Class Description

**TestCase Data Class**:

```Python
@dataclass
class TestCase:
    name: str          # Test name
    command: str       # Execution command/program
    args: List[str]    # Argument list
    expected: Dict[str, Any] # Expected results
```

**BaseRunner Abstract Class**:

```Python
def run_tests(self) -> bool:
    self.load_test_cases()
    for case in self.test_cases:
        result = self.run_single_test(case)
        # Result collection logic...
    return self.results["failed"] == 0
```

**JSONRunner Implementation**:

```Python
def load_test_cases(self):
    with open(config_path) as f:
        cases = json.load(f)["test_cases"]
        # Field validation and path preprocessing
        case["command"] = self.path_resolver.resolve_command(case["command"])
```

**Assertion Subsystem**:


```Python
class Assertions:
    @staticmethod
    def matches(text, pattern):
        if not re.search(pattern, text):
            raise AssertionError(f"Pattern mismatch: {pattern}")
```

## 6. Robustness Design

### Reliability Assurance Measures

- **Input Validation**: Enforces validation of required test case fields

- **Error Isolation**: Individual test failures do not affect subsequent executions

- **Sub-process Protection**:

  ```Python
  subprocess.run(..., check=False, shell=True)
  ```
  
- **Path Security**: Automatically handles path separator differences

- **Exception Capture**: Three-level error handling (assertion error/execution error/system error)

- **Result Integrity**: Ensures all test results are recorded

## 7. Extensibility Design

### Extension Directions

1. **Format Extension**: Implement XMLRunner/TomlRunner
2. **Executor Extension**: Support Docker/SSH remote execution
3. **Assertion Extension**: Add performance metric assertions
4. **Report Format**: Support HTML/PDF report generation
5. **Distributed Execution**: Multi-process parallel testing

Extension Example (adding XML runner):

```Python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
```

## 8. Example Demonstration

### Input Example

```JSON
{
    "test_cases": [
        {
            "name": "Version Check Test",
            "command": "python",
            "args": ["--version"],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 1
Passed: 1 (100.0%)
Failed: 0 (0.0%)

Detailed Results:
✓ Version Check Test
```

## 9. Precautions

1. Path Handling:

   - Parameters starting with `--` will not be path-converted
   - Windows paths should use `/` or escaped `\\`
   
2. Command Limitations:

   - Only single command execution is supported
   - Complex pipelines need to be encapsulated into scripts
   
3. Security Specifications:

   - Do not run with root privileges
   - Prohibit execution of untrusted test cases
   
4. Performance Notes:

   - Single test timeout is unlimited by default
   - I/O intensive tests are recommended to control concurrency themselves
   
5. Environment Dependencies:

   - Tested programs need to be pre-installed
- Python path needs to be in the system PATH

================
File: requirements.txt
================
pytest
PyYAML
concurrent.futures

================
File: setup.py
================
from setuptools import setup, find_packages

setup(
    name="cli-test-framework",
    version="0.1.0",
    author="Your Name",
    author_email="your.email@example.com",
    description="A small command line testing framework in Python.",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        # List your project dependencies here
    ],
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
)

================
File: src/__init__.py
================
# File: /python-test-framework/python-test-framework/src/__init__.py

# This file is intentionally left blank.

================
File: src/core/__init__.py
================
from .base_runner import BaseRunner
from .parallel_runner import ParallelRunner
from .test_case import TestCase
from .assertions import Assertions

================
File: src/core/assertions.py
================
import re
from typing import Any, Pattern

class Assertions:
    @staticmethod
    def equals(actual: Any, expected: Any, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected: {expected}, but got: {actual}")
        return True

    @staticmethod
    def contains(container: str, item: str, message: str = "") -> bool:
        """
        Check if the item is contained within the container string.
        This method returns True if the item is found anywhere within the container,
        even if the container contains other information.
        """
        if item not in container:
            raise AssertionError(f"{message} Expected to contain: {item}")
        return True

    @staticmethod
    def matches(text: str, pattern: str, message: str = "") -> bool:
        if not re.search(pattern, text):
            raise AssertionError(f"{message} Text does not match pattern: {pattern}")
        return True

    @staticmethod
    def return_code_equals(actual: int, expected: int, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected return code: {expected}, got: {actual}")
        return True

================
File: src/core/base_runner.py
================
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from .test_case import TestCase
from .assertions import Assertions

class BaseRunner(ABC):
    def __init__(self, config_file: str, workspace: Optional[str] = None):
        if workspace:
            self.workspace = Path(workspace)
        else:
            self.workspace = Path(__file__).parent.parent.parent
        self.config_path = self.workspace / config_file
        self.test_cases: List[TestCase] = []
        self.results: Dict[str, Any] = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "details": []
        }
        self.assertions = Assertions()

    @abstractmethod
    def load_test_cases(self) -> None:
        """Load test cases from configuration file"""
        pass

    def run_tests(self) -> bool:
        """Run all test cases and return whether all tests passed"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting test execution... Total tests: {self.results['total']}")
        print("=" * 50)
        
        for i, case in enumerate(self.test_cases, 1):
            print(f"\nRunning test {i}/{self.results['total']}: {case.name}")
            result = self.run_single_test(case)
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"✓ Test passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"✗ Test failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
                
        print("\n" + "=" * 50)
        print(f"Test execution completed. Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0

    @abstractmethod
    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        """Run a single test case and return the result"""
        pass

================
File: src/core/parallel_runner.py
================
from abc import ABC
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Union
import time
import threading
from .base_runner import BaseRunner
from .test_case import TestCase
from .process_worker import run_test_in_process

class ParallelRunner(BaseRunner):
    """并行测试运行器基类，支持多线程和多进程执行"""
    
    def __init__(self, config_file: str, workspace: Optional[str] = None, 
                 max_workers: Optional[int] = None, 
                 execution_mode: str = "thread"):
        """
        初始化并行运行器
        
        Args:
            config_file: 配置文件路径
            workspace: 工作目录
            max_workers: 最大并发数，默认为CPU核心数
            execution_mode: 执行模式，'thread'(线程) 或 'process'(进程)
        """
        super().__init__(config_file, workspace)
        self.max_workers = max_workers
        self.execution_mode = execution_mode
        self.lock = threading.Lock()  # 用于线程安全的结果更新
        
    def run_tests(self) -> bool:
        """并行运行所有测试用例"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting parallel test execution... Total tests: {self.results['total']}")
        print(f"Execution mode: {self.execution_mode}, Max workers: {self.max_workers or 'auto'}")
        print("=" * 50)
        
        start_time = time.time()
        
        if self.execution_mode == "process":
            executor_class = ProcessPoolExecutor
        else:
            executor_class = ThreadPoolExecutor
            
        with executor_class(max_workers=self.max_workers) as executor:
            # 提交所有测试任务
            if self.execution_mode == "process":
                # 进程模式：使用独立的工作器函数
                future_to_case = {
                    executor.submit(
                        run_test_in_process, 
                        i, 
                        {
                            "name": case.name,
                            "command": case.command,
                            "args": case.args,
                            "expected": case.expected
                        },
                        str(self.workspace) if self.workspace else None
                    ): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            else:
                # 线程模式：使用实例方法
                future_to_case = {
                    executor.submit(self._run_test_with_index, i, case): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            
            # 收集结果
            for future in as_completed(future_to_case):
                test_index, case = future_to_case[future]
                try:
                    result = future.result()
                    self._update_results(result, test_index, case)
                except Exception as exc:
                    error_result = {
                        "name": case.name,
                        "status": "failed",
                        "message": f"Test execution failed: {str(exc)}",
                        "output": "",
                        "command": "",
                        "return_code": None
                    }
                    self._update_results(error_result, test_index, case)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print("\n" + "=" * 50)
        print(f"Parallel test execution completed in {execution_time:.2f} seconds")
        print(f"Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0
    
    def _run_test_with_index(self, test_index: int, case: TestCase) -> Dict[str, Any]:
        """运行单个测试并返回结果（包含索引信息）"""
        print(f"[Worker] Running test {test_index}: {case.name}")
        result = self.run_single_test(case)
        return result
    
    def _update_results(self, result: Dict[str, Any], test_index: int, case: TestCase) -> None:
        """线程安全地更新测试结果"""
        with self.lock:
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"✓ Test {test_index} passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"✗ Test {test_index} failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
    
    def run_tests_sequential(self) -> bool:
        """回退到顺序执行模式"""
        print("Falling back to sequential execution...")
        return super().run_tests()

================
File: src/core/process_worker.py
================
"""
进程工作器模块
用于多进程并行测试执行，避免序列化问题
"""

import subprocess
import sys
from typing import Dict, Any
from .test_case import TestCase
from .assertions import Assertions

def run_test_in_process(test_index: int, case_data: Dict[str, Any], workspace: str = None) -> Dict[str, Any]:
    """
    在独立进程中运行单个测试用例
    
    Args:
        test_index: 测试索引
        case_data: 测试用例数据字典
        workspace: 工作目录
    
    Returns:
        测试结果字典
    """
    # 重新创建TestCase对象（避免序列化问题）
    case = TestCase(
        name=case_data["name"],
        command=case_data["command"],
        args=case_data["args"],
        expected=case_data["expected"]
    )
    
    # 创建断言对象
    assertions = Assertions()
    
    result = {
        "name": case.name,
        "status": "failed",
        "message": "",
        "output": "",
        "command": "",
        "return_code": None
    }

    try:
        command = f"{case.command} {' '.join(case.args)}"
        result["command"] = command
        print(f"  [Process Worker {test_index}] Executing command: {command}")
        
        process = subprocess.run(
            command,
            cwd=workspace if workspace else None,
            capture_output=True,
            text=True,
            check=False,
            shell=True
        )

        output = process.stdout + process.stderr
        result["output"] = output
        result["return_code"] = process.returncode
        
        if output.strip():
            print(f"  [Process Worker {test_index}] Command output for {case.name}:")
            for line in output.splitlines():
                print(f"    {line}")

        # 检查返回码
        if "return_code" in case.expected:
            print(f"  [Process Worker {test_index}] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
            assertions.return_code_equals(
                process.returncode,
                case.expected["return_code"]
            )

        # 检查输出包含
        if "output_contains" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output contains for {case.name}...")
            for expected_text in case.expected["output_contains"]:
                assertions.contains(output, expected_text)

        # 检查正则匹配
        if "output_matches" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output matches regex for {case.name}...")
            assertions.matches(output, case.expected["output_matches"])

        result["status"] = "passed"
        
    except AssertionError as e:
        result["message"] = str(e)
    except Exception as e:
        result["message"] = f"Execution error: {str(e)}"

    return result

================
File: src/core/test_case.py
================
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class TestCase:
    name: str
    command: str
    args: List[str]
    expected: Dict[str, Any]
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert test case to dictionary format"""
        print("Convert test case to dictionary format")
        print(self.command)
        return {
            "name": self.name,
            "command": self.command,
            "args": self.args,
            "expected": self.expected
        }

================
File: src/runners/__init__.py
================
from .json_runner import JSONRunner
from .yaml_runner import YAMLRunner
from .parallel_json_runner import ParallelJSONRunner

================
File: src/runners/json_runner.py
================
from typing import Optional
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
from typing import Dict, Any

class JSONRunner(BaseRunner):
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self) -> None:
        """Load test cases from a JSON file."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
            # print(self.test_cases)
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",  # 添加命令输出字段
            "command": "",  # 添加执行的命令字段
            "return_code": None  # 添加返回码字段
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command  # 保存执行的命令
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output  # 保存命令的完整输出
            result["return_code"] = process.returncode  # 保存返回码
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/runners/parallel_json_runner.py
================
from typing import Optional, Dict, Any
from ..core.parallel_runner import ParallelRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
import threading

class ParallelJSONRunner(ParallelRunner):
    """并行JSON测试运行器"""
    
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None,
                 max_workers: Optional[int] = None, execution_mode: str = "thread"):
        """
        初始化并行JSON运行器
        
        Args:
            config_file: JSON配置文件路径
            workspace: 工作目录
            max_workers: 最大并发数
            execution_mode: 执行模式，'thread' 或 'process'
        """
        super().__init__(config_file, workspace, max_workers, execution_mode)
        self.path_resolver = PathResolver(self.workspace)
        self._print_lock = threading.Lock()  # 用于控制输出顺序

    def load_test_cases(self) -> None:
        """从JSON文件加载测试用例"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """运行单个测试用例（线程安全版本）"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            
            # 线程安全的输出
            with self._print_lock:
                print(f"  [Worker] Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            # 线程安全的输出
            if output.strip():
                with self._print_lock:
                    print(f"  [Worker] Command output for {case.name}:")
                    for line in output.splitlines():
                        print(f"    {line}")

            # 检查返回码
            if "return_code" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # 检查输出包含
            if "output_contains" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output contains for {case.name}...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # 检查正则匹配
            if "output_matches" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output matches regex for {case.name}...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/runners/yaml_runner.py
================
from typing import Optional, Dict, Any
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import subprocess
import sys

class YAMLRunner(BaseRunner):
    def __init__(self, config_file="test_cases.yaml", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self):
        """Load test cases from a YAML file."""
        try:
            import yaml
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")
                
                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))
                
            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """Run a single test case and return the result"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/utils/__init__.py
================
# File: /python-test-framework/python-test-framework/src/utils/__init__.py

# This file is intentionally left blank.

================
File: src/utils/path_resolver.py
================
from pathlib import Path
from typing import List

class PathResolver:
    def __init__(self, workspace: Path):
        self.workspace = workspace

    def resolve_paths(self, args: List[str]) -> List[str]:
        resolved_args = []
        for arg in args:
            if not arg.startswith("--"):
                # Only prepend workspace if the path is relative
                if not Path(arg).is_absolute():
                    resolved_args.append(str(self.workspace / arg))
                else:
                    resolved_args.append(arg)
            else:
                resolved_args.append(arg)
        return resolved_args

    def resolve_command(self, command: str) -> str:
        """
        解析命令路径
        - 系统命令（如echo, ping, dir等）保持原样
        - 相对路径的可执行文件转换为绝对路径
        """
        # 常见的系统命令列表
        system_commands = {
            'echo', 'ping', 'dir', 'ls', 'cat', 'grep', 'find', 'sort', 
            'head', 'tail', 'wc', 'curl', 'wget', 'git', 'python', 'node',
            'npm', 'pip', 'java', 'javac', 'gcc', 'make', 'cmake', 'docker',
            'kubectl', 'helm', 'terraform', 'ansible', 'ssh', 'scp', 'rsync'
        }
        
        # 如果是系统命令或绝对路径，保持原样
        if command in system_commands or Path(command).is_absolute():
            return command
        
        # 否则当作相对路径处理
        return str(self.workspace / command)

================
File: src/utils/report_generator.py
================
class ReportGenerator:
    def __init__(self, results: dict, file_path: str):
        self.results = results
        self.file_path = file_path

    def generate_report(self) -> str:
        report = "Test Results Summary:\n"
        report += f"Total Tests: {self.results['total']}\n"
        report += f"Passed: {self.results['passed']}\n"
        report += f"Failed: {self.results['failed']}\n\n"
        
        report += "Detailed Results:\n"
        for detail in self.results['details']:
            status_icon = "✓" if detail['status'] == 'passed' else "✗"
            report += f"{status_icon} {detail['name']}\n"
            if detail.get('message'):
                report += f"   -> {detail['message']}\n"
        
        # 添加失败案例的详细输出信息
        failed_tests = [detail for detail in self.results['details'] if detail['status'] == 'failed']
        if failed_tests:
            report += "\n" + "="*50 + "\n"
            report += "FAILED TEST CASES DETAILS:\n"
            report += "="*50 + "\n\n"
            
            for i, failed_test in enumerate(failed_tests, 1):
                report += f"{i}. Test: {failed_test['name']}\n"
                report += "-" * 40 + "\n"
                
                # 添加执行的命令
                if failed_test.get('command'):
                    report += f"Command: {failed_test['command']}\n"
                
                # 添加返回码
                if failed_test.get('return_code') is not None:
                    report += f"Return Code: {failed_test['return_code']}\n"
                
                # 添加失败原因
                if failed_test.get('message'):
                    report += f"Error Message: {failed_test['message']}\n"
                
                # 添加命令的完整输出（这是最重要的部分）
                if failed_test.get('output'):
                    report += f"\nCommand Output:\n"
                    report += "=" * 30 + "\n"
                    report += f"{failed_test['output']}\n"
                    report += "=" * 30 + "\n"
                
                # 添加错误堆栈信息（如果有的话）
                if failed_test.get('error_trace'):
                    report += f"Error Trace:\n{failed_test['error_trace']}\n"
                
                # 添加执行时间（如果有的话）
                if failed_test.get('duration'):
                    report += f"Duration: {failed_test['duration']}s\n"
                
                report += "\n"
        
        return report

    def save_report(self) -> None:
        report = self.generate_report()
        with open(self.file_path, 'w', encoding='utf-8') as f:
            f.write(report)

    def print_report(self) -> None:
        report = self.generate_report()
        print(report)

================
File: tests/__init__.py
================
# File: /python-test-framework/python-test-framework/tests/__init__.py

# This file is intentionally left blank.

================
File: tests/fixtures/test_cases.json
================
{
  "test_cases": [
    {
      "name": "测试Python版本",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    },
    {
      "name": "测试目录列表",
      "command": "dir",
      "args": ["."],
      "expected": {
        "return_code": 0,
        "output_contains": ["src"]
      }
    },
    {
      "name": "测试echo命令",
      "command": "echo",
      "args": ["Hello World"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Hello World"]
      }
    },
    {
      "name": "测试ping本地回环",
      "command": "ping",
      "args": ["-n", "1", "127.0.0.1"],
      "expected": {
        "return_code": 0,
        "output_contains": ["127.0.0.1"]
      }
    },
    {
      "name": "测试时间命令",
      "command": "echo",
      "args": ["%time%"],
      "expected": {
        "return_code": 0
      }
    },
    {
      "name": "测试文件存在性",
      "command": "dir",
      "args": ["src"],
      "expected": {
        "return_code": 0,
        "output_contains": ["core", "runners"]
      }
    }
  ]
}

================
File: tests/fixtures/test_cases.yaml
================
name: Sample Test Case
command: python script.py
args:
  - --input
  - input.txt
expected:
  return_code: 0
  output_contains:
    - "Success"
    - "Processed"

================
File: tests/fixtures/test_cases1.json
================
{
    "test_cases": [
        {
            "name": "text_identical_default",
            "description": "默认文本比较（相同文件）",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Files i identical"]
                
            }
        },
        {
            "name": "text_different_range",
            "description": "带行范围限制的文本比较",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf", 
                "./test/1_copy.bdf",
                "--start-line=93",
                "--end-line=96",
                "--start-column=1",
                "--end-column=30"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_exact_match",
            "description": "严格JSON比较（相同文件）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/1_copy.json",
                "--file-type=json",
                "--json-compare-mode=exact"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_key_based_match",
            "description": "JSON键值比较（结构不同但关键字段匹配）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--file-type=json",
                "--json-compare-mode=key-based",
                "--json-key-field=\"phone\""
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "binary_comparison",
            "description": "二进制文件比较（带相似度计算）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/2.bdf",
                "--file-type=binary",
                "--similarity",
                "--chunk-size=4096"
            ],
            "expected": {
                "output_contains": ["Similarity Index"]
                
            }
        },
        {
            "name": "multi_thread_comparison",
            "description": "多线程文本比较",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/1_copy.bdf",
                "--num-threads=8",
                "--verbose"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "different_output_format",
            "description": "JSON输出格式测试",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--output-format=json"
            ],
            "expected": {
                "output_contains": ["\"position\""]
                
            }
        },
        {
            "name": "auto_detect_filetype",
            "description": "自动文件类型检测",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Auto-detected file type: text"]
                
            }
        },
        {
            "name": "h5_comparison",
            "description": "HDF5文件比较特定表格",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODAL/DISPLACEMENT"],
            "expected": {
                "output_contains": ["Files are different"]
                
            }
        },
        {
            "name": "h5_comparison_all_tables",
            "description": "HDF5文件比较所有表格",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5"],
            "expected": {
                "output_contains": ["Files are different."]
                
            }
        },
        {
            "name": "h5_comparison_with_wrong_table",
            "description": "HDF5文件比较，错误表格路径",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODALl/DISPLACEMENT"],
            "expected": {
                "matches": ["WARNING - Table .* not found"]                
            }
        },
        {
            "name": "h5_comparison_with_strucutre_mode",
            "description": "HDF5文件比较，结构模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-structure-only"],
            "expected": {
                "output_contains": ["Files are identical."]                
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5文件比较，内容模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-show-content-diff"],
            "expected": {
                "output_contains": ["Difference at"]
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5文件比较，内容模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-rtol=1e-5", "--h5-atol=1e-8"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        },
        {
            "name": "h5_comparison_with_table_regex",
            "description": "HDF5文件比较，表格正则表达式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-table-regex=NASTRAN/RESULT/\\b\\w*al\\b/STRESS"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        }
    ]
}

================
File: tests/performance_test.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
并行测试性能验证脚本
快速验证并行测试功能和性能提升
"""

import sys
import time
import json
import tempfile
import os
from pathlib import Path

# 添加src目录到Python路径
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.runners.json_runner import JSONRunner
from src.runners.parallel_json_runner import ParallelJSONRunner

def create_test_config(num_tests=10):
    """创建测试配置文件"""
    test_cases = []
    
    for i in range(num_tests):
        test_cases.append({
            "name": f"测试用例 {i+1}",
            "command": "echo",
            "args": [f"test_{i+1}"],
            "expected": {
                "return_code": 0,
                "output_contains": [f"test_{i+1}"]
            }
        })
    
    return {"test_cases": test_cases}

def run_performance_test():
    """运行性能测试"""
    print("=" * 60)
    print("并行测试框架性能验证")
    print("=" * 60)
    
    # 创建临时测试配置
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "perf_test.json")
    
    # 创建测试用例（可以调整数量）
    num_tests = 8
    config = create_test_config(num_tests)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"创建了 {num_tests} 个测试用例")
    print(f"测试配置文件: {config_file}")
    
    results = {}
    
    # 1. 顺序执行测试
    print(f"\n1. 顺序执行测试...")
    start_time = time.time()
    sequential_runner = JSONRunner(config_file, temp_dir)
    seq_success = sequential_runner.run_tests()
    seq_time = time.time() - start_time
    results['sequential'] = {'time': seq_time, 'success': seq_success}
    
    # 2. 并行执行测试（线程模式）
    print(f"\n2. 并行执行测试（线程模式，4个工作线程）...")
    start_time = time.time()
    parallel_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=4, 
        execution_mode="thread"
    )
    par_success = parallel_runner.run_tests()
    par_time = time.time() - start_time
    results['parallel_thread'] = {'time': par_time, 'success': par_success}
    
    # 3. 并行执行测试（进程模式）
    print(f"\n3. 并行执行测试（进程模式，2个工作进程）...")
    start_time = time.time()
    process_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=2, 
        execution_mode="process"
    )
    proc_success = process_runner.run_tests()
    proc_time = time.time() - start_time
    results['parallel_process'] = {'time': proc_time, 'success': proc_success}
    
    # 性能分析
    print("\n" + "=" * 60)
    print("性能分析结果:")
    print("=" * 60)
    
    print(f"测试用例数量:      {num_tests}")
    print(f"顺序执行时间:      {seq_time:.2f} 秒")
    print(f"并行执行(线程):    {par_time:.2f} 秒 (加速比: {seq_time/par_time:.2f}x)")
    print(f"并行执行(进程):    {proc_time:.2f} 秒 (加速比: {seq_time/proc_time:.2f}x)")
    
    # 验证结果一致性
    print(f"\n结果验证:")
    print(f"顺序执行成功:      {seq_success}")
    print(f"并行执行(线程)成功: {par_success}")
    print(f"并行执行(进程)成功: {proc_success}")
    
    # 清理临时文件
    import shutil
    shutil.rmtree(temp_dir)
    
    # 总结
    if all(results[key]['success'] for key in results):
        print(f"\n✓ 所有测试模式都成功执行")
        if par_time < seq_time:
            print(f"✓ 并行执行确实提升了性能")
        else:
            print(f"⚠ 在当前测试规模下，并行优势不明显")
    else:
        print(f"\n✗ 部分测试模式执行失败")
        return False
    
    return True

if __name__ == "__main__":
    try:
        success = run_performance_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n测试被用户中断")
        sys.exit(1)
    except Exception as e:
        print(f"\n测试执行出错: {e}")
        sys.exit(1)

================
File: tests/test_parallel_runner.py
================
import unittest
import tempfile
import json
import os
import sys
import time
from pathlib import Path

# 添加src目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner

class TestParallelRunner(unittest.TestCase):
    """并行运行器测试类"""
    
    def setUp(self):
        """设置测试环境"""
        self.temp_dir = tempfile.mkdtemp()
        self.config_file = os.path.join(self.temp_dir, "test_config.json")
        
        # 创建测试配置
        test_config = {
            "test_cases": [
                {
                    "name": "测试1",
                    "command": "echo",
                    "args": ["test1"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test1"]
                    }
                },
                {
                    "name": "测试2",
                    "command": "echo",
                    "args": ["test2"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test2"]
                    }
                },
                {
                    "name": "测试3",
                    "command": "echo",
                    "args": ["test3"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test3"]
                    }
                },
                {
                    "name": "测试4",
                    "command": "echo",
                    "args": ["test4"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test4"]
                    }
                }
            ]
        }
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    def tearDown(self):
        """清理测试环境"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def test_parallel_vs_sequential_performance(self):
        """测试并行执行相比顺序执行的性能提升"""
        # 顺序执行
        sequential_runner = JSONRunner(self.config_file, self.temp_dir)
        start_time = time.time()
        seq_success = sequential_runner.run_tests()
        seq_time = time.time() - start_time
        
        # 并行执行
        parallel_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        start_time = time.time()
        par_success = parallel_runner.run_tests()
        par_time = time.time() - start_time
        
        # 验证结果
        self.assertTrue(seq_success, "顺序执行应该成功")
        self.assertTrue(par_success, "并行执行应该成功")
        self.assertEqual(
            sequential_runner.results["total"], 
            parallel_runner.results["total"],
            "测试总数应该相同"
        )
        self.assertEqual(
            sequential_runner.results["passed"], 
            parallel_runner.results["passed"],
            "通过的测试数应该相同"
        )
        
        print(f"\n性能比较:")
        print(f"顺序执行时间: {seq_time:.3f}秒")
        print(f"并行执行时间: {par_time:.3f}秒")
        if par_time > 0:
            print(f"加速比: {seq_time/par_time:.2f}x")
    
    def test_thread_vs_process_mode(self):
        """测试线程模式和进程模式"""
        # 线程模式
        thread_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        thread_success = thread_runner.run_tests()
        
        # 进程模式
        process_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="process"
        )
        process_success = process_runner.run_tests()
        
        # 验证结果
        self.assertTrue(thread_success, "线程模式应该成功")
        self.assertTrue(process_success, "进程模式应该成功")
        self.assertEqual(
            thread_runner.results["passed"], 
            process_runner.results["passed"],
            "两种模式的通过测试数应该相同"
        )
    
    def test_max_workers_configuration(self):
        """测试不同的最大工作线程数配置"""
        for max_workers in [1, 2, 4]:
            with self.subTest(max_workers=max_workers):
                runner = ParallelJSONRunner(
                    self.config_file, 
                    self.temp_dir, 
                    max_workers=max_workers, 
                    execution_mode="thread"
                )
                success = runner.run_tests()
                self.assertTrue(success, f"max_workers={max_workers}时应该成功")
                self.assertEqual(runner.results["passed"], 4, "应该通过4个测试")
    
    def test_fallback_to_sequential(self):
        """测试回退到顺序执行"""
        runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        
        # 测试回退功能
        success = runner.run_tests_sequential()
        self.assertTrue(success, "回退到顺序执行应该成功")
        self.assertEqual(runner.results["passed"], 4, "应该通过4个测试")

if __name__ == '__main__':
    unittest.main()

================
File: tests/test_report.txt
================
Test Results Summary:
Total Tests: 15
Passed: 14
Failed: 1

Detailed Results:
✗ text_identical_default
   ->  Expected to contain: Files i identical
✓ text_different_range
✓ json_exact_match
✓ json_key_based_match
✓ binary_comparison
✓ multi_thread_comparison
✓ different_output_format
✓ auto_detect_filetype
✓ h5_comparison
✓ h5_comparison_all_tables
✓ h5_comparison_with_wrong_table
✓ h5_comparison_with_strucutre_mode
✓ h5_comparison_with_content_mode
✓ h5_comparison_with_content_mode
✓ h5_comparison_with_table_regex

==================================================
FAILED TEST CASES DETAILS:
==================================================

1. Test: text_identical_default
----------------------------------------
Command: python ./compare_text.py D:\Document\xcode\Compare-File-Tool\test\1.bdf D:\Document\xcode\Compare-File-Tool\test\1_copy.bdf
Return Code: 0
Error Message:  Expected to contain: Files i identical

Command Output:
==============================
Files are identical.
2025-05-27 11:08:06,566 - file_comparator - INFO - Auto-detected file type: text
2025-05-27 11:08:06,668 - file_comparator.TextComparator - INFO - Comparing files: D:\Document\xcode\Compare-File-Tool\test\1.bdf and D:\Document\xcode\Compare-File-Tool\test\1_copy.bdf

==============================

================
File: tests/test_runners.py
================
import unittest
from src.runners.json_runner import JSONRunner
from src.runners.yaml_runner import YAMLRunner

class TestJSONRunner(unittest.TestCase):
    def setUp(self):
        self.runner = JSONRunner("tests/fixtures/test_cases.json")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

class TestYAMLRunner(unittest.TestCase):
    def setUp(self):
        self.runner = YAMLRunner("tests/fixtures/test_cases.yaml")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

if __name__ == "__main__":
    unittest.main()

================
File: tests/test1.py
================
import sys
import os
# 添加项目根目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import unittest
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator

def main():
    runner = JSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases1.json",
        workspace="D:/Document/xcode/Compare-File-Tool"
    )
    success = runner.run_tests()
    
    # 生成报告
    report_generator = ReportGenerator(
        runner.results, 
        "D:/Document/xcode/cli-test-framework/tests/test_report.txt"
    )
    report_generator.print_report()  # 打印到控制台
    report_generator.save_report()   # 保存到文件
    
    print(f"\n报告已保存到: D:/Document/xcode/cli-test-framework/tests/test_report.txt")
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
