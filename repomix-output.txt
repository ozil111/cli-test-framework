This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-06-07T02:51:10.351Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
CHANGELOG.md
docs/user_manual.md
Example_usage.py
MANIFEST.in
parallel_example.py
PARALLEL_TESTING_GUIDE.md
pyproject.toml
README_cn.md
README.md
requirements.txt
setup.py
SPACE_PATH_FIX.md
src/__init__.py
src/cli_test_framework.egg-info/dependency_links.txt
src/cli_test_framework.egg-info/entry_points.txt
src/cli_test_framework.egg-info/PKG-INFO
src/cli_test_framework.egg-info/requires.txt
src/cli_test_framework.egg-info/SOURCES.txt
src/cli_test_framework.egg-info/top_level.txt
src/cli_test_framework/__init__.py
src/cli_test_framework/cli.py
src/cli_test_framework/commands/__init__.py
src/cli_test_framework/commands/compare.py
src/cli_test_framework/core/__init__.py
src/cli_test_framework/core/assertions.py
src/cli_test_framework/core/base_runner.py
src/cli_test_framework/core/parallel_runner.py
src/cli_test_framework/core/process_worker.py
src/cli_test_framework/core/test_case.py
src/cli_test_framework/file_comparator/__init__.py
src/cli_test_framework/file_comparator/base_comparator.py
src/cli_test_framework/file_comparator/binary_comparator.py
src/cli_test_framework/file_comparator/csv_comparator.py
src/cli_test_framework/file_comparator/factory.py
src/cli_test_framework/file_comparator/h5_comparator.py
src/cli_test_framework/file_comparator/json_comparator.py
src/cli_test_framework/file_comparator/result.py
src/cli_test_framework/file_comparator/text_comparator.py
src/cli_test_framework/file_comparator/xml_comparator.py
src/cli_test_framework/runners/__init__.py
src/cli_test_framework/runners/json_runner.py
src/cli_test_framework/runners/parallel_json_runner.py
src/cli_test_framework/runners/yaml_runner.py
src/cli_test_framework/utils/__init__.py
src/cli_test_framework/utils/path_resolver.py
src/cli_test_framework/utils/report_generator.py
tests/__init__.py
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json
tests/performance_test.py
tests/test_comprehensive_space.py
tests/test_parallel_runner.py
tests/test_parallel_space.py
tests/test_report.txt
tests/test_runners.py
tests/test1.py
User_Manual.md

================================================================
Files
================================================================

================
File: CHANGELOG.md
================
# 变更日志

## [v2.0.0] - 2025-05-27

### 🚀 重大新功能

#### 并行测试执行
- **多线程并行执行**：支持 I/O 密集型测试的高效并行处理
- **多进程并行执行**：支持 CPU 密集型测试的完全隔离执行
- **可配置并发数**：灵活设置最大工作线程/进程数
- **性能提升**：典型场景下可获得 2-4 倍加速比

#### 智能命令解析
- **复杂命令支持**：智能处理 `"python ./script.py"` 等复杂命令格式
- **系统命令识别**：自动识别系统命令和相对路径可执行文件
- **路径解析增强**：更准确的路径转换和命令构建

### ✨ 功能增强

#### 线程安全设计
- **结果收集锁**：确保并发环境下测试结果的正确性
- **输出控制锁**：防止并发输出混乱
- **异常隔离**：单个测试失败不影响其他测试

#### 新增运行器
- **ParallelJSONRunner**：支持并行执行的 JSON 测试运行器
- **进程工作器**：独立的进程执行模块，解决序列化问题

### 🔧 问题修复

#### 命令解析修复
- **修复问题**：`'D:\Document\xcode\Compare-File-Tool\python' 不是内部或外部命令`
- **根本原因**：复杂命令字符串被整体当作命令名处理
- **解决方案**：智能分割命令字符串，分别处理命令和参数部分

#### 路径解析优化
- **系统命令白名单**：扩展系统命令识别列表
- **相对路径处理**：改进相对路径和绝对路径的判断逻辑

### 📚 文档更新

#### README 全面重写
- **英文版 README.md**：全新的结构和内容组织
- **中文版 README_cn.md**：同步更新，保持一致性
- **并行测试指南**：详细的 PARALLEL_TESTING_GUIDE.md

#### 新增示例
- **parallel_example.py**：性能比较演示
- **performance_test.py**：自动化性能测试
- **test_parallel_runner.py**：并行功能单元测试

### 🎯 性能基准

| 测试场景 | 顺序执行 | 并行执行(线程) | 并行执行(进程) | 加速比 |
|----------|----------|----------------|----------------|--------|
| 6个简单测试 | 0.12秒 | 0.03秒 | 0.16秒 | 3.84x |
| 10个I/O测试 | 5.2秒 | 1.4秒 | 2.1秒 | 3.7x |
| 20个CPU测试 | 12.8秒 | 8.9秒 | 6.2秒 | 2.1x |

### 🔄 向后兼容性

- **完全兼容**：现有的 JSONRunner 代码无需修改
- **渐进式升级**：可以逐步迁移到并行执行
- **配置兼容**：现有测试用例配置文件无需更改

### 📦 依赖更新

```txt
pytest
PyYAML  # 修正了之前的 pyyaml 拼写错误
concurrent.futures  # 标准库，无需额外安装
```

### 🚀 使用示例

#### 基本并行执行
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
success = runner.run_tests()
```

#### 性能比较
```bash
# 运行性能比较示例
python parallel_example.py

# 输出示例：
# 顺序执行时间:     0.12 秒
# 并行执行时间(线程): 0.03 秒 (加速比: 3.84x)
# 并行执行时间(进程): 0.16 秒 (加速比: 0.73x)
```

### 🔮 未来计划

- **分布式执行**：支持跨机器的测试执行
- **Web UI**：提供图形化的测试管理界面
- **更多格式支持**：XML、TOML 等配置格式
- **性能监控**：详细的执行时间和资源使用统计

---

## [v1.0.0] - 2025-03-05

### 初始版本
- 基础的 JSON/YAML 测试运行器
- 路径解析功能
- 断言机制
- 报告生成

================
File: docs/user_manual.md
================
# CLI Testing Framework User Manual

## Table of Contents
1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Basic Usage](#basic-usage)
4. [Test Case Definition](#test-case-definition)
5. [Parallel Testing](#parallel-testing)
6. [File Comparison](#file-comparison)
7. [Advanced Features](#advanced-features)
8. [Troubleshooting](#troubleshooting)
9. [API Reference](#api-reference)
10. [Examples](#examples)

## Introduction

The CLI Testing Framework is a powerful tool designed for testing command-line applications and scripts. It provides a structured way to define, execute, and verify test cases, with support for parallel execution and advanced file comparison capabilities.

### Key Features
- Parallel test execution with thread and process support
- JSON/YAML test case definition
- Advanced file comparison capabilities
- Comprehensive reporting
- Extensible architecture

## Installation

### Prerequisites
- Python 3.6 or higher
- pip package manager

### Basic Installation
```bash
pip install cli-test-framework
```

### Development Installation
```bash
git clone https://github.com/yourusername/cli-test-framework.git
cd cli-test-framework
pip install -e .
```

## Basic Usage

### Creating a Test Case

1. Create a JSON test case file (e.g., `test_cases.json`):
```json
{
    "test_cases": [
        {
            "name": "Basic Command Test",
            "command": "echo",
            "args": ["Hello, World!"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Hello, World!"]
            }
        }
    ]
}
```

2. Run the test:
```python
from cli_test_framework.runners import JSONRunner

runner = JSONRunner(
    config_file="test_cases.json",
    workspace="/path/to/workspace"
)
success = runner.run_tests()
```

### Using the Command Line

```bash
# Run tests from a JSON file
cli-test run test_cases.json

# Run tests in parallel
cli-test run test_cases.json --parallel --workers 4
```

## Test Case Definition

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "Test Case Name",
            "command": "command_to_execute",
            "args": ["arg1", "arg2"],
            "expected": {
                "return_code": 0,
                "output_contains": ["expected text"],
                "output_matches": [".*regex pattern.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Test Case Name
    command: command_to_execute
    args:
      - arg1
      - arg2
    expected:
      return_code: 0
      output_contains:
        - expected text
      output_matches:
        - ".*regex pattern.*"
```

## Parallel Testing

### Thread Mode
```python
from cli_test_framework.runners import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
success = runner.run_tests()
```

### Process Mode
```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=2,
    execution_mode="process"
)
success = runner.run_tests()
```

## File Comparison

### Basic File Comparison
```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare with specific options
compare-files file1.txt file2.txt --start-line 10 --end-line 20
```

### JSON File Comparison
```bash
# Exact comparison
compare-files data1.json data2.json

# Key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id
```

### HDF5 File Comparison
```bash
# Compare specific tables
compare-files data1.h5 data2.h5 --h5-table table1,table2

# Compare with numerical tolerance
compare-files data1.h5 data2.h5 --h5-rtol 1e-5 --h5-atol 1e-8
```

### Binary File Comparison
```bash
# Compare with similarity check
compare-files binary1.bin binary2.bin --similarity

# Compare with custom chunk size
compare-files binary1.bin binary2.bin --chunk-size 16384
```

## Advanced Features

### Custom Assertions
```python
from cli_test_framework.assertions import BaseAssertion

class CustomAssertion(BaseAssertion):
    def assert_custom_condition(self, actual, expected):
        if not self._check_custom_condition(actual, expected):
            raise AssertionError("Custom condition not met")
```

### Custom Runners
```python
from cli_test_framework.runners import BaseRunner

class CustomRunner(BaseRunner):
    def load_test_cases(self):
        # Custom test case loading logic
        pass

    def run_test(self, test_case):
        # Custom test execution logic
        pass
```

### Output Formats
```python
# JSON output
runner = JSONRunner(config_file="test_cases.json", output_format="json")

# HTML output
runner = JSONRunner(config_file="test_cases.json", output_format="html")
```

## Troubleshooting

### Common Issues

1. **Command Not Found**
   - Ensure the command is in the system PATH
   - Use absolute paths for scripts
   - Check command permissions

2. **Parallel Execution Issues**
   - Reduce number of workers
   - Check for resource conflicts
   - Use process mode for CPU-intensive tests

3. **File Comparison Issues**
   - Verify file permissions
   - Check file encoding
   - Ensure sufficient memory for large files

### Debug Mode
```python
runner = JSONRunner(
    config_file="test_cases.json",
    debug=True
)
```

## API Reference

### Core Classes

#### JSONRunner
```python
class JSONRunner:
    def __init__(self, config_file, workspace=None, debug=False):
        """
        Initialize JSONRunner
        :param config_file: Path to JSON test case file
        :param workspace: Working directory for test execution
        :param debug: Enable debug mode
        """
```

#### ParallelJSONRunner
```python
class ParallelJSONRunner:
    def __init__(self, config_file, max_workers=None, execution_mode="thread"):
        """
        Initialize ParallelJSONRunner
        :param config_file: Path to JSON test case file
        :param max_workers: Maximum number of parallel workers
        :param execution_mode: "thread" or "process"
        """
```

### File Comparison

#### ComparatorFactory
```python
class ComparatorFactory:
    @staticmethod
    def create_comparator(file_type, **kwargs):
        """
        Create a comparator instance
        :param file_type: Type of file to compare
        :param kwargs: Additional comparator options
        :return: Comparator instance
        """
```

## Examples

### Complete Test Suite
```python
from cli_test_framework.runners import JSONRunner
from cli_test_framework.assertions import Assertions

# Create test runner
runner = JSONRunner(
    config_file="test_suite.json",
    workspace="/project/root",
    debug=True
)

# Run tests
success = runner.run_tests()

# Process results
if success:
    print("All tests passed!")
else:
    print("Some tests failed:")
    for result in runner.results["details"]:
        if result["status"] == "failed":
            print(f"- {result['name']}: {result['message']}")
```

### Parallel Test Suite
```python
from cli_test_framework.runners import ParallelJSONRunner
import os

# Create parallel runner
runner = ParallelJSONRunner(
    config_file="test_suite.json",
    max_workers=os.cpu_count() * 2,
    execution_mode="thread"
)

# Run tests in parallel
success = runner.run_tests()

# Generate report
runner.generate_report("test_report.html")
```

### File Comparison Suite
```python
from cli_test_framework.file_comparator import ComparatorFactory

# Compare text files
text_comparator = ComparatorFactory.create_comparator(
    "text",
    encoding="utf-8",
    verbose=True
)
text_result = text_comparator.compare_files("file1.txt", "file2.txt")

# Compare JSON files
json_comparator = ComparatorFactory.create_comparator(
    "json",
    compare_mode="key-based",
    key_field="id"
)
json_result = json_comparator.compare_files("data1.json", "data2.json")
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

================
File: Example_usage.py
================
# Example usage
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator
import sys

def main():
    runner = JSONRunner(config_file="D:/Document/xcode/Compare-File-Tool/test_script/test_cases.json", workspace="D:/Document/xcode/Compare-File-Tool")
    success = runner.run_tests()
    
    # Generate and save the report
    report_generator = ReportGenerator(runner.results, "D:/Document/xcode/Compare-File-Tool/test_script/test_report.txt")
    report_generator.print_report()
    report_generator.save_report()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: MANIFEST.in
================
include README.md
include LICENSE
include CHANGELOG.md
include CONTRIBUTING.md
recursive-include docs *
recursive-include tests *

================
File: parallel_example.py
================
# 并行测试示例用法
from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner
from src.utils.report_generator import ReportGenerator
import sys
import time

def run_sequential_test():
    """运行顺序测试"""
    print("=" * 60)
    print("运行顺序测试...")
    print("=" * 60)
    
    start_time = time.time()
    runner = JSONRunner(config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", workspace=".")
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\n顺序测试完成，耗时: {end_time - start_time:.2f} 秒")
    return success, runner.results, end_time - start_time

def run_parallel_test(max_workers=None, execution_mode="thread"):
    """运行并行测试"""
    print("=" * 60)
    print(f"运行并行测试 (模式: {execution_mode}, 工作线程: {max_workers or 'auto'})...")
    print("=" * 60)
    
    start_time = time.time()
    runner = ParallelJSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", 
        workspace=".",
        max_workers=max_workers,
        execution_mode=execution_mode
    )
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\n并行测试完成，耗时: {end_time - start_time:.2f} 秒")
    return success, runner.results, end_time - start_time

def main():
    """主函数：比较顺序和并行测试的性能"""
    
    # 运行顺序测试
    seq_success, seq_results, seq_time = run_sequential_test()
    
    # 运行并行测试（线程模式）
    par_success, par_results, par_time = run_parallel_test(max_workers=4, execution_mode="thread")
    
    # 运行并行测试（进程模式）
    proc_success, proc_results, proc_time = run_parallel_test(max_workers=2, execution_mode="process")
    
    # 性能比较
    print("\n" + "=" * 60)
    print("性能比较结果:")
    print("=" * 60)
    print(f"顺序执行时间:     {seq_time:.2f} 秒")
    print(f"并行执行时间(线程): {par_time:.2f} 秒 (加速比: {seq_time/par_time:.2f}x)")
    print(f"并行执行时间(进程): {proc_time:.2f} 秒 (加速比: {seq_time/proc_time:.2f}x)")
    
    # 生成报告
    if par_success:
        report_generator = ReportGenerator(par_results, "parallel_test_report.txt")
        report_generator.print_report()
        report_generator.save_report()
        print(f"\n并行测试报告已保存到: parallel_test_report.txt")
    
    # 返回最终结果
    return seq_success and par_success and proc_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

================
File: PARALLEL_TESTING_GUIDE.md
================
# 并行测试功能使用指南

## 概述

你的测试框架现在支持并行测试执行，可以显著提升测试执行效率。框架提供了两种并行执行模式：**线程模式**和**进程模式**。

## 功能特性

### ✅ 已实现的功能

- **多线程并行执行**：适用于I/O密集型测试
- **多进程并行执行**：适用于CPU密集型测试，提供完全隔离的执行环境
- **可配置并发数**：支持自定义最大工作线程/进程数
- **线程安全设计**：确保测试结果的正确性和输出的清晰性
- **性能监控**：提供执行时间统计和加速比分析
- **向后兼容**：完全兼容现有的顺序执行代码

### 📊 性能提升

根据测试结果，并行执行可以带来显著的性能提升：

- **线程模式**：通常可获得 **2-4倍** 的加速比
- **进程模式**：适合需要完全隔离的场景，但启动开销较大

## 使用方法

### 基本用法

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# 创建并行运行器
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # 最大并发数
    execution_mode="thread"  # 执行模式：thread 或 process
)

# 运行测试
success = runner.run_tests()
```

### 配置选项

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `config_file` | str | "test_cases.json" | 测试配置文件路径 |
| `workspace` | str | None | 工作目录 |
| `max_workers` | int | None (自动) | 最大并发数 |
| `execution_mode` | str | "thread" | 执行模式：thread/process |

### 执行模式选择

#### 线程模式 (thread)
- **适用场景**：I/O密集型测试（如网络请求、文件操作）
- **优势**：启动快，内存共享，适合大多数测试场景
- **推荐并发数**：CPU核心数 × 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### 进程模式 (process)
- **适用场景**：需要完全隔离的测试，CPU密集型任务
- **优势**：完全隔离，避免GIL限制
- **推荐并发数**：CPU核心数

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json", 
    max_workers=2,
    execution_mode="process"
)
```

## 示例代码

### 性能比较示例

```python
# 运行性能比较
python parallel_example.py

# 输出示例：
# 顺序执行时间:     0.12 秒
# 并行执行时间(线程): 0.03 秒 (加速比: 3.58x)
# 并行执行时间(进程): 0.10 秒 (加速比: 1.15x)
```

### 快速验证

```python
# 快速验证并行功能
python test_parallel_simple.py

# 快速性能测试
python performance_test.py
```

## 最佳实践

### 1. 选择合适的并发数

```python
import os

# CPU密集型任务
max_workers = os.cpu_count()

# I/O密集型任务  
max_workers = os.cpu_count() * 2
```

### 2. 测试用例设计原则

- ✅ **确保测试独立性**：测试用例之间不应有依赖关系
- ✅ **避免共享资源冲突**：不同测试不应操作相同的文件或端口
- ✅ **使用相对路径**：框架会自动处理路径解析

### 3. 错误处理

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # 检查失败的测试
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"失败的测试: {detail['name']}")
                print(f"错误信息: {detail['message']}")
                
except Exception as e:
    print(f"执行出错: {e}")
    # 回退到顺序执行
    runner.run_tests_sequential()
```

## 技术实现

### 架构设计

```
ParallelRunner (基类)
├── 线程安全的结果收集
├── 可配置的执行模式
└── 异常处理机制

ParallelJSONRunner (实现类)
├── 继承 ParallelRunner
├── JSON配置解析
└── 路径解析功能

进程工作器 (process_worker.py)
├── 独立进程执行
├── 避免序列化问题
└── 完全隔离环境
```

### 线程安全机制

- **结果收集锁**：`threading.Lock()` 保护共享结果数据
- **输出控制锁**：避免并发输出混乱
- **异常隔离**：单个测试失败不影响其他测试

## 故障排除

### 常见问题

1. **进程模式序列化错误**
   - 原因：对象包含不可序列化的属性（如锁）
   - 解决：使用独立的进程工作器函数

2. **路径解析错误**
   - 原因：系统命令被当作相对路径处理
   - 解决：更新 `PathResolver` 的系统命令列表

3. **性能提升不明显**
   - 原因：测试用例执行时间太短，并行开销大于收益
   - 解决：增加测试用例数量或使用更复杂的测试

### 调试技巧

```python
# 启用详细输出
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=1,  # 设为1便于调试
    execution_mode="thread"
)

# 查看详细结果
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 版本兼容性

- **Python版本**：3.6+
- **依赖项**：无额外依赖，使用标准库
- **向后兼容**：完全兼容现有的 `JSONRunner` 代码

## 总结

并行测试功能为你的测试框架带来了显著的性能提升，特别适合：

- 🚀 **大规模测试套件**：数十个或数百个测试用例
- 🌐 **I/O密集型测试**：网络请求、文件操作等
- ⚡ **CI/CD流水线**：缩短构建时间
- 🔄 **回归测试**：快速验证代码变更

通过合理配置并发参数和选择适当的执行模式，你可以在保证测试可靠性的同时，大幅提升测试执行效率！

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

================
File: README_cn.md
================
# CLI 测试框架

## 1. 概述
这是一个轻量级、可扩展的自动化测试框架，支持通过JSON/YAML格式定义测试用例，提供完整的测试执行、结果验证和报告生成功能。框架专为命令行工具和脚本提供标准化测试管理，具备企业级并行执行支持。

## 2. 功能特点
- **🚀 并行测试执行**：支持多线程和多进程并行测试，显著提升执行效率
- **🏗️ 模块化架构**：核心组件解耦设计（运行器/断言/报告）
- **📄 多格式支持**：原生支持JSON/YAML测试用例格式
- **🧠 智能命令解析**：智能处理复杂命令如 `"python ./script.py"`
- **📁 智能路径解析**：自动处理相对路径与绝对路径转换
- **✅ 丰富断言机制**：返回值校验、输出内容匹配、正则表达式验证
- **🔌 可扩展接口**：通过继承BaseRunner可快速实现新测试格式支持
- **🔒 执行环境隔离**：独立子进程运行保证测试隔离性
- **📊 全面报告**：详细的通过率统计和失败诊断
- **🔧 线程安全设计**：稳健的并发执行和适当的同步机制

## 3. 使用说明

### 环境要求
```bash
pip install -r requirements.txt
Python >= 3.6
```

### 快速开始

1. 创建测试用例文件（示例见`tests/fixtures/`）
2. 编写执行脚本：

**顺序执行**：
```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

**并行执行（新功能！）**：
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# 多线程执行（推荐用于I/O密集型测试）
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # 最大并发数
    execution_mode="thread"  # 执行模式：thread 或 process
)
success = runner.run_tests()

# 性能比较示例
python parallel_example.py
```

**YAML支持**：
```python
from src.runners.yaml_runner import YAMLRunner

runner = YAMLRunner(
    config_file="path/to/test_cases.yaml",
    workspace="/project/root"
)
success = runner.run_tests()
```

## 4. 测试用例格式

### JSON格式

```json
{
  "test_cases": [
    {
      "name": "文件处理测试",
      "command": "python ./process_files.py",
      "args": ["file1.txt", "file2.txt", "--verbose"],
      "expected": {
        "return_code": 0,
        "output_contains": ["文件处理完成"],
        "output_matches": [".*比较完成.*"]
      }
    }
  ]
}
```

### YAML格式

```yaml
test_cases:
  - name: 目录扫描测试
    command: ls
    args: 
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

### 支持的命令格式

框架智能处理各种命令格式：

```json
{
    "command": "echo",                    // 简单命令
    "command": "python script.py",       // 命令+脚本
    "command": "node ./app.js --port",   // 复杂命令+参数
}
```

## 5. 并行测试功能

### 性能优势

并行执行带来显著的性能提升：

```bash
# 运行性能比较
python parallel_example.py

# 典型输出：
# 顺序执行时间:     12.45 秒
# 并行执行时间(线程): 3.21 秒 (3.88x 加速)
# 并行执行时间(进程): 4.12 秒 (3.02x 加速)
```

### 执行模式

#### 线程模式（推荐）
- **适用场景**：I/O密集型测试（网络请求、文件操作）
- **优势**：启动快，内存共享，适合大多数测试场景
- **推荐并发数**：CPU核心数 × 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### 进程模式
- **适用场景**：CPU密集型测试，需要完全隔离的场景
- **优势**：完全隔离，绕过GIL限制
- **推荐并发数**：CPU核心数

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=2,
    execution_mode="process"
)
```

### 线程安全特性

- **结果收集锁**：`threading.Lock()` 保护共享结果数据
- **输出控制锁**：防止并发输出混乱
- **异常隔离**：单个测试失败不影响其他测试

## 6. 系统架构

### 增强的架构流程

```mermaid
graph TD
    A[测试用例] --> B{执行模式}
    B -->|顺序| C[JSONRunner/YAMLRunner]
    B -->|并行| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[命令解析器]
    E --> F
    F --> G[路径解析器]
    G --> H[子进程执行]
    H --> I[断言引擎]
    I --> J[线程安全结果收集]
    J --> K[报告生成器]
```

### 核心组件

#### 1. 智能命令解析器
```python
# 处理复杂命令如 "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> 完整路径
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. 增强的路径解析器
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. 并行运行器基类
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. 高级用法

### 性能测试

```python
# 快速性能测试
python performance_test.py

# 并行功能单元测试
python -m pytest tests/test_parallel_runner.py -v
```

### 错误处理和回退

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # 检查失败的测试
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"失败的测试: {detail['name']}")
                print(f"错误信息: {detail['message']}")
                
except Exception as e:
    print(f"执行出错: {e}")
    # 回退到顺序执行
    runner.run_tests_sequential()
```

### 最佳实践

1. **选择合适的并发数**：
   ```python
   import os
   
   # CPU密集型任务
   max_workers = os.cpu_count()
   
   # I/O密集型任务
   max_workers = os.cpu_count() * 2
   ```

2. **测试用例设计**：
   - ✅ 确保测试独立性（测试间无依赖关系）
   - ✅ 避免共享资源冲突（不同文件/端口）
   - ✅ 使用相对路径（框架自动处理解析）

3. **调试技巧**：
   ```python
   # 启用详细输出便于调试
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # 设为1便于调试
       execution_mode="thread"
   )
   ```

## 8. 系统流程

### 架构模块

```mermaid
graph TD
    A[测试用例] --> B[Runner]
    B --> C[路径解析]
    B --> D[子进程执行]
    D --> E[断言验证]
    E --> F[结果收集]
    F --> G[报告生成]
```

### 核心模块说明

1. **Test Runner**

   - 加载测试配置
   - 管理测试生命周期
   - 协调各组件协作

2. **PathResolver**

   ```python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   智能处理路径参数，自动将相对路径转换为基于workspace的绝对路径

3. **Assertion Engine**

   - 返回值校验（return_code）
   - 输出内容匹配（contains/matches）
   - 异常捕获机制

4. **Report Generator**

   - 实时统计测试进度
   - 生成带错误定位的详细报告
   - 支持控制台输出和文件保存



## 9. 示例演示

### 输入样例

```json
{
    "test_cases": [
        {
            "name": "Python版本检查",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "文件处理测试",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["处理完成"]
            }
        }
    ]
}
```

### 输出报告

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
✓ Python版本检查
✓ 文件处理测试
✓ JSON比较测试
...
```

## 10. 故障排除

### 常见问题

1. **进程模式序列化错误**
   - **原因**：对象包含不可序列化的属性（如锁）
   - **解决**：使用独立的进程工作器函数

2. **路径解析错误**
   - **原因**：系统命令被当作相对路径处理
   - **解决**：更新 `PathResolver` 的系统命令列表

3. **性能提升不明显**
   - **原因**：测试用例执行时间太短，并行开销大于收益
   - **解决**：增加测试用例数量或使用更复杂的测试

4. **命令未找到错误**
   - **原因**：复杂命令如 `"python ./script.py"` 解析不正确
   - **解决**：框架现已自动处理此问题（最新版本已修复）

### 调试技巧

```python
# 启用详细日志
import logging
logging.basicConfig(level=logging.DEBUG)

# 查看详细结果
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 11. 扩展和自定义

### 添加新的运行器

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # 解析XML结构并转换为TestCase对象
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # 在测试执行前添加自定义逻辑
        pass
```

### 自定义断言

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"执行太慢: {execution_time}s > {max_time}s")
```

## 12. 版本兼容性

- **Python版本**：3.6+
- **依赖项**：仅使用标准库（核心功能无外部依赖）
- **向后兼容**：完全兼容现有的 `JSONRunner` 代码
- **平台支持**：Windows、macOS、Linux

## 13. 性能基准测试

| 测试场景 | 顺序执行 | 并行执行(线程) | 并行执行(进程) | 加速比 |
|----------|----------|----------------|----------------|--------|
| 10个I/O测试 | 5.2秒 | 1.4秒 | 2.1秒 | 3.7x |
| 20个CPU测试 | 12.8秒 | 8.9秒 | 6.2秒 | 2.1x |
| 混合测试 | 8.5秒 | 2.3秒 | 3.1秒 | 3.7x |

## 14. 贡献指南

1. Fork 仓库
2. 创建功能分支
3. 为新功能添加测试
4. 确保所有测试通过：`python -m pytest tests/ -v`
5. 提交 Pull Request

## 15. 许可证

本项目采用 MIT 许可证 - 详见 LICENSE 文件。

---

**🚀 准备好用并行执行来加速你的测试工作流程吧！**

详细的并行测试指南请参见：[PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: README.md
================
# CLI Testing Framework

## 1. Overview

This is a lightweight and extensible automated testing framework that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The framework is designed to provide standardized test management for command-line tools and scripts, with enterprise-grade parallel execution support and advanced file comparison features.

## 2. Features

- **🚀 Parallel Test Execution**: Support for multi-threading and multi-processing parallel testing with significant performance improvements
- **🏗️ Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **📄 Multi-Format Support**: Native support for JSON/YAML test case formats
- **🧠 Intelligent Command Parsing**: Smart handling of complex commands like `"python ./script.py"`
- **📁 Smart Path Resolution**: Automatic handling of relative and absolute path conversions
- **✅ Rich Assertion Mechanism**: Return code validation, output content matching, regex verification
- **🔌 Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **🔒 Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **📊 Comprehensive Reports**: Detailed pass rate statistics and failure diagnostics
- **🔧 Thread-Safe Design**: Robust concurrent execution with proper synchronization
- **📝 Advanced File Comparison**: Support for comparing various file types (text, binary, JSON, HDF5) with detailed diff output

## 3. Quick Start

### Environment Requirements

```bash
pip install cli-test-framework
Python >= 3.6
```

### Sequential Execution

```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Parallel Execution

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# Multi-threaded execution (recommended for I/O-intensive tests)
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # Maximum concurrent workers
    execution_mode="thread"  # "thread" or "process"
)
success = runner.run_tests()
```

### File Comparison

```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare JSON files with key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id

# Compare HDF5 files with specific options
compare-files data1.h5 data2.h5 --h5-table table1,table2 --h5-rtol 1e-6

# Compare binary files with similarity check
compare-files binary1.bin binary2.bin --similarity
```

## 4. Test Case Format

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "compare-files",
            "args": ["file1.txt", "file2.txt", "--verbose"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Files are identical"],
                "output_matches": [".*comparison completed.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Directory Scan Test
    command: ls
    args:
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

## 5. File Comparison Features

### Supported File Types

- **Text Files**: Plain text, source code, markdown, etc.
- **JSON Files**: With exact or key-based comparison
- **HDF5 Files**: Structure and content comparison with numerical tolerance
- **Binary Files**: With optional similarity index calculation

### Comparison Options

#### Text Comparison
```bash
compare-files file1.txt file2.txt \
    --start-line 10 \
    --end-line 20 \
    --encoding utf-8
```

#### JSON Comparison
```bash
compare-files data1.json data2.json \
    --json-compare-mode key-based \
    --json-key-field id,name
```

#### HDF5 Comparison
```bash
compare-files data1.h5 data2.h5 \
    --h5-table table1,table2 \
    --h5-structure-only \
    --h5-rtol 1e-5 \
    --h5-atol 1e-8
```

#### Binary Comparison
```bash
compare-files binary1.bin binary2.bin \
    --similarity \
    --chunk-size 16384
```

### Output Formats

- **Text**: Human-readable diff output
- **JSON**: Structured comparison results
- **HTML**: Visual diff with syntax highlighting

## 6. System Architecture

### Enhanced Architecture Flow

```mermaid
graph TD
    A[Test Cases] --> B{Execution Mode}
    B -->|Sequential| C[JSONRunner/YAMLRunner]
    B -->|Parallel| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[Command Parser]
    E --> F
    F --> G[Path Resolver]
    G --> H[Sub-process Execution]
    H --> I[Assertion Engine]
    I --> J[Thread-Safe Result Collection]
    J --> K[Report Generator]
    L[File Comparator] --> M[Text Comparator]
    L --> N[JSON Comparator]
    L --> O[HDF5 Comparator]
    L --> P[Binary Comparator]
```

### Core Components

#### 1. Intelligent Command Parser
```python
# Handles complex commands like "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> full path
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. Enhanced Path Resolver
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. Parallel Runner Base Class
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. Advanced Usage

### Performance Testing

```python
# Quick performance test
python performance_test.py

# Unit tests for parallel functionality
python -m pytest tests/test_parallel_runner.py -v
```

### Error Handling and Fallback

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # Check failed tests
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"Failed test: {detail['name']}")
                print(f"Error: {detail['message']}")
                
except Exception as e:
    print(f"Execution error: {e}")
    # Fallback to sequential execution
    runner.run_tests_sequential()
```

### Best Practices

1. **Choose Appropriate Concurrency**:
   ```python
   import os
   
   # For CPU-intensive tasks
   max_workers = os.cpu_count()
   
   # For I/O-intensive tasks
   max_workers = os.cpu_count() * 2
   ```

2. **Test Case Design**:
   - ✅ Ensure test independence (no dependencies between tests)
   - ✅ Avoid shared resource conflicts (different files/ports)
   - ✅ Use relative paths (framework handles resolution automatically)

3. **Debugging**:
   ```python
   # Enable verbose output for debugging
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # Set to 1 for easier debugging
       execution_mode="thread"
   )
   ```

## 8. Example Demonstrations

### Input Example

```json
{
    "test_cases": [
        {
            "name": "Python Version Check",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "File Processing Test",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Processing completed"]
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
✓ Python Version Check
✓ File Processing Test
✓ JSON Comparison Test
...
```

## 9. Troubleshooting

### Common Issues

1. **Process Mode Serialization Error**
   - **Cause**: Objects contain non-serializable attributes (like locks)
   - **Solution**: Use independent process worker functions

2. **Path Resolution Error**
   - **Cause**: System commands treated as relative paths
   - **Solution**: Update `PathResolver` system command list

3. **Performance Not Improved**
   - **Cause**: Test cases too short, parallel overhead exceeds benefits
   - **Solution**: Increase test case count or use more complex tests

4. **Command Not Found Error**
   - **Cause**: Complex commands like `"python ./script.py"` not parsed correctly
   - **Solution**: Framework now automatically handles this (fixed in latest version)

### Debug Tips

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check detailed results
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 10. Extension and Customization

### Adding New Runners

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # Add custom logic before test execution
        pass
```

### Custom Assertions

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"Execution too slow: {execution_time}s > {max_time}s")
```

## 11. Version Compatibility

- **Python Version**: 3.6+
- **Dependencies**: Standard library only (no external dependencies for core functionality)
- **Backward Compatibility**: Fully compatible with existing `JSONRunner` code
- **Platform Support**: Windows, macOS, Linux

## 12. Performance Benchmarks

| Test Scenario | Sequential | Parallel (Thread) | Parallel (Process) | Speedup |
|---------------|------------|-------------------|-------------------|---------|
| 10 I/O tests  | 5.2s       | 1.4s              | 2.1s              | 3.7x    |
| 20 CPU tests  | 12.8s      | 8.9s              | 6.2s              | 2.1x    |
| Mixed tests   | 8.5s       | 2.3s              | 3.1s              | 3.7x    |

## 13. Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass: `python -m pytest tests/ -v`
5. Submit a pull request

## 14. License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**🚀 Ready to supercharge your testing workflow with parallel execution and advanced file comparison!**

For detailed parallel testing guide, see: [PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: requirements.txt
================
pytest
PyYAML
concurrent.futures

================
File: setup.py
================
from setuptools import setup, find_packages
import os

# Read the contents of the README file
this_directory = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

setup(
    name="cli-test-framework",
    version="0.2.4",
    author="Xiaotong Wang",
    author_email="xiaotongwang98@gmail.com",
    description="A small command line testing framework in Python with file comparison capabilities.",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/cli-test-framework",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "dukpy==0.5.0",
        "h5py>=3.8.0",
        "numpy>=2.0.1",
        "setuptools>=75.8.0",
        "wheel>=0.45.1"
    ],
    entry_points={
        'console_scripts': [
            'cli-test=cli_test_framework.cli:main',
            'compare-files=cli_test_framework.commands.compare:main',
        ],
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Topic :: Software Development :: Testing",
        "Topic :: Software Development :: Libraries :: Python Modules",
    ],
    python_requires='>=3.6',
    project_urls={
        'Documentation': 'https://github.com/yourusername/cli-test-framework/docs/user_manual.md',
        'Source': 'https://github.com/yourusername/cli-test-framework',
        'Tracker': 'https://github.com/yourusername/cli-test-framework/issues',
    },
)

================
File: SPACE_PATH_FIX.md
================
# 包含空格的路径解析修复

## 问题描述

之前的命令解析器在处理包含空格的路径时存在问题，例如：
- `"C:\Program Files (x86)\python.exe script.py"` 会被错误解析
- `C:\Program Files (x86)\python.exe script.py` 会被分割成多个部分

## 修复方案

### 1. 智能命令解析函数

在 `PathResolver` 类中添加了 `parse_command_string()` 方法，能够：

- **处理带引号的路径**：使用 `shlex.split(posix=True)` 正确解析引号
- **处理不带引号的绝对路径**：识别Windows绝对路径模式（如 `C:\...`）
- **智能参数分类**：区分命令、文件路径和选项参数

### 2. 核心改进

#### 引号处理
```python
# 修复前：引号被保留
'"C:\Program Files (x86)\python.exe"' → '"C:\Program Files (x86)\python.exe"'

# 修复后：引号被正确去除
'"C:\Program Files (x86)\python.exe"' → 'C:\Program Files (x86)\python.exe'
```

#### 绝对路径识别
```python
# 修复前：空格导致路径被错误分割
'C:\Program Files (x86)\python.exe script.py' → ['C:\Program', 'Files', '(x86)\python.exe', 'script.py']

# 修复后：正确识别完整路径
'C:\Program Files (x86)\python.exe script.py' → 'C:\Program Files (x86)\python.exe script.py'
```

### 3. 更新的运行器

所有运行器都已更新使用新的解析方法：
- `JSONRunner`
- `ParallelJSONRunner` 
- `YAMLRunner`

## 测试验证

### 测试用例

1. **简单命令**：`echo hello` ✓
2. **相对路径脚本**：`python script.py` ✓
3. **带引号的Windows路径**：`"C:\Program Files (x86)\Python\python.exe" script.py` ✓
4. **不带引号的Windows路径**：`C:\Program Files (x86)\Python\python.exe script.py` ✓
5. **复杂命令**：`node app.js --port 3000` ✓
6. **Unix风格路径**：`"/usr/local/bin/my app" script.py` ✓

### 运行测试

```bash
# 基本测试
python test_simple_space.py

# 全面测试
python test_comprehensive_space.py

# 并行运行器测试
python test_parallel_space.py
```

## 技术细节

### 关键函数

```python
def parse_command_string(self, command_string: str) -> str:
    """智能解析命令字符串，正确处理包含空格的路径"""
    
    # 1. 处理带引号的命令
    if '"' in command_string or "'" in command_string:
        parts = shlex.split(command_string, posix=True)
        # 解析命令和参数...
    
    # 2. 处理绝对路径开头的命令
    elif self._starts_with_absolute_path(command_string):
        return self._parse_absolute_path_command(command_string)
    
    # 3. 普通命令处理
    else:
        parts = command_string.split()
        # 标准解析...
```

### 路径识别逻辑

```python
def _starts_with_absolute_path(self, command_string: str) -> bool:
    """检查是否以绝对路径开头"""
    if os.name == 'nt':  # Windows
        return (len(command_string) >= 3 and 
                command_string[1:3] == ':\\') or command_string.startswith('\\\\')
    else:  # Unix/Linux
        return command_string.startswith('/')
```

## 兼容性

- ✅ Windows 路径（`C:\Program Files\...`）
- ✅ Unix/Linux 路径（`/usr/local/bin/...`）
- ✅ 相对路径（`./script.py`）
- ✅ 系统命令（`echo`, `python`, `node`等）
- ✅ 复杂参数（`--port 3000`, `--env development`）

## 性能影响

- 解析性能提升：使用更智能的分割逻辑
- 内存使用优化：避免不必要的路径转换
- 并行执行兼容：所有并行功能正常工作

## 总结

这次修复彻底解决了包含空格的路径解析问题，使测试框架能够正确处理各种复杂的命令格式，特别是Windows环境下的路径。所有现有功能保持兼容，并行测试功能也完全正常。

================
File: src/__init__.py
================
# File: /python-test-framework/python-test-framework/src/__init__.py

# This file is intentionally left blank.

================
File: src/cli_test_framework.egg-info/dependency_links.txt
================


================
File: src/cli_test_framework.egg-info/entry_points.txt
================
[console_scripts]
cli-test = cli_test_framework.cli:main
compare-files = cli_test_framework.commands.compare:main

================
File: src/cli_test_framework.egg-info/PKG-INFO
================
Metadata-Version: 2.4
Name: cli-test-framework
Version: 0.2.4
Summary: A small command line testing framework in Python with file comparison capabilities.
Home-page: https://github.com/yourusername/cli-test-framework
Author: Xiaotong Wang
Author-email: xiaotongwang98@gmail.com
Project-URL: Documentation, https://github.com/yourusername/cli-test-framework/docs/user_manual.md
Project-URL: Source, https://github.com/yourusername/cli-test-framework
Project-URL: Tracker, https://github.com/yourusername/cli-test-framework/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Testing
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: dukpy==0.5.0
Requires-Dist: h5py>=3.8.0
Requires-Dist: numpy>=2.0.1
Requires-Dist: setuptools>=75.8.0
Requires-Dist: wheel>=0.45.1
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# CLI Testing Framework

## 1. Overview

This is a lightweight and extensible automated testing framework that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The framework is designed to provide standardized test management for command-line tools and scripts, with enterprise-grade parallel execution support and advanced file comparison features.

## 2. Features

- **🚀 Parallel Test Execution**: Support for multi-threading and multi-processing parallel testing with significant performance improvements
- **🏗️ Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **📄 Multi-Format Support**: Native support for JSON/YAML test case formats
- **🧠 Intelligent Command Parsing**: Smart handling of complex commands like `"python ./script.py"`
- **📁 Smart Path Resolution**: Automatic handling of relative and absolute path conversions
- **✅ Rich Assertion Mechanism**: Return code validation, output content matching, regex verification
- **🔌 Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **🔒 Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **📊 Comprehensive Reports**: Detailed pass rate statistics and failure diagnostics
- **🔧 Thread-Safe Design**: Robust concurrent execution with proper synchronization
- **📝 Advanced File Comparison**: Support for comparing various file types (text, binary, JSON, HDF5) with detailed diff output

## 3. Quick Start

### Environment Requirements

```bash
pip install cli-test-framework
Python >= 3.6
```

### Sequential Execution

```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Parallel Execution

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# Multi-threaded execution (recommended for I/O-intensive tests)
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # Maximum concurrent workers
    execution_mode="thread"  # "thread" or "process"
)
success = runner.run_tests()
```

### File Comparison

```bash
# Compare two text files
compare-files file1.txt file2.txt

# Compare JSON files with key-based comparison
compare-files data1.json data2.json --json-compare-mode key-based --json-key-field id

# Compare HDF5 files with specific options
compare-files data1.h5 data2.h5 --h5-table table1,table2 --h5-rtol 1e-6

# Compare binary files with similarity check
compare-files binary1.bin binary2.bin --similarity
```

## 4. Test Case Format

### JSON Format

```json
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "compare-files",
            "args": ["file1.txt", "file2.txt", "--verbose"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Files are identical"],
                "output_matches": [".*comparison completed.*"]
            }
        }
    ]
}
```

### YAML Format

```yaml
test_cases:
  - name: Directory Scan Test
    command: ls
    args:
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\\.md$"
```

## 5. File Comparison Features

### Supported File Types

- **Text Files**: Plain text, source code, markdown, etc.
- **JSON Files**: With exact or key-based comparison
- **HDF5 Files**: Structure and content comparison with numerical tolerance
- **Binary Files**: With optional similarity index calculation

### Comparison Options

#### Text Comparison
```bash
compare-files file1.txt file2.txt \
    --start-line 10 \
    --end-line 20 \
    --encoding utf-8
```

#### JSON Comparison
```bash
compare-files data1.json data2.json \
    --json-compare-mode key-based \
    --json-key-field id,name
```

#### HDF5 Comparison
```bash
compare-files data1.h5 data2.h5 \
    --h5-table table1,table2 \
    --h5-structure-only \
    --h5-rtol 1e-5 \
    --h5-atol 1e-8
```

#### Binary Comparison
```bash
compare-files binary1.bin binary2.bin \
    --similarity \
    --chunk-size 16384
```

### Output Formats

- **Text**: Human-readable diff output
- **JSON**: Structured comparison results
- **HTML**: Visual diff with syntax highlighting

## 6. System Architecture

### Enhanced Architecture Flow

```mermaid
graph TD
    A[Test Cases] --> B{Execution Mode}
    B -->|Sequential| C[JSONRunner/YAMLRunner]
    B -->|Parallel| D[ParallelRunner]
    D --> E[ThreadPoolExecutor/ProcessPoolExecutor]
    C --> F[Command Parser]
    E --> F
    F --> G[Path Resolver]
    G --> H[Sub-process Execution]
    H --> I[Assertion Engine]
    I --> J[Thread-Safe Result Collection]
    J --> K[Report Generator]
    L[File Comparator] --> M[Text Comparator]
    L --> N[JSON Comparator]
    L --> O[HDF5 Comparator]
    L --> P[Binary Comparator]
```

### Core Components

#### 1. Intelligent Command Parser
```python
# Handles complex commands like "python ./script.py"
command_parts = case["command"].split()
if len(command_parts) > 1:
    actual_command = resolve_command(command_parts[0])  # "python"
    script_parts = resolve_paths(command_parts[1:])     # "./script.py" -> full path
    final_command = f"{actual_command} {' '.join(script_parts)}"
```

#### 2. Enhanced Path Resolver
```python
def resolve_command(self, command: str) -> str:
    system_commands = {
        'echo', 'ping', 'python', 'node', 'java', 'docker', ...
    }
    if command in system_commands or Path(command).is_absolute():
        return command
    return str(self.workspace / command)
```

#### 3. Parallel Runner Base Class
```python
class ParallelRunner(BaseRunner):
    def __init__(self, max_workers=None, execution_mode="thread"):
        self.max_workers = max_workers or os.cpu_count()
        self.execution_mode = execution_mode
        self._results_lock = threading.Lock()
        self._print_lock = threading.Lock()
```

## 7. Advanced Usage

### Performance Testing

```python
# Quick performance test
python performance_test.py

# Unit tests for parallel functionality
python -m pytest tests/test_parallel_runner.py -v
```

### Error Handling and Fallback

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # Check failed tests
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"Failed test: {detail['name']}")
                print(f"Error: {detail['message']}")
                
except Exception as e:
    print(f"Execution error: {e}")
    # Fallback to sequential execution
    runner.run_tests_sequential()
```

### Best Practices

1. **Choose Appropriate Concurrency**:
   ```python
   import os
   
   # For CPU-intensive tasks
   max_workers = os.cpu_count()
   
   # For I/O-intensive tasks
   max_workers = os.cpu_count() * 2
   ```

2. **Test Case Design**:
   - ✅ Ensure test independence (no dependencies between tests)
   - ✅ Avoid shared resource conflicts (different files/ports)
   - ✅ Use relative paths (framework handles resolution automatically)

3. **Debugging**:
   ```python
   # Enable verbose output for debugging
   runner = ParallelJSONRunner(
       config_file="test_cases.json",
       max_workers=1,  # Set to 1 for easier debugging
       execution_mode="thread"
   )
   ```

## 8. Example Demonstrations

### Input Example

```json
{
    "test_cases": [
        {
            "name": "Python Version Check",
            "command": "python --version",
            "args": [],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        },
        {
            "name": "File Processing Test",
            "command": "python ./process_file.py",
            "args": ["input.txt", "--output", "result.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["Processing completed"]
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Performance Statistics:
Sequential execution time: 12.45 seconds
Parallel execution time:   3.21 seconds
Speedup ratio:            3.88x

Detailed Results:
✓ Python Version Check
✓ File Processing Test
✓ JSON Comparison Test
...
```

## 9. Troubleshooting

### Common Issues

1. **Process Mode Serialization Error**
   - **Cause**: Objects contain non-serializable attributes (like locks)
   - **Solution**: Use independent process worker functions

2. **Path Resolution Error**
   - **Cause**: System commands treated as relative paths
   - **Solution**: Update `PathResolver` system command list

3. **Performance Not Improved**
   - **Cause**: Test cases too short, parallel overhead exceeds benefits
   - **Solution**: Increase test case count or use more complex tests

4. **Command Not Found Error**
   - **Cause**: Complex commands like `"python ./script.py"` not parsed correctly
   - **Solution**: Framework now automatically handles this (fixed in latest version)

### Debug Tips

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check detailed results
import json
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## 10. Extension and Customization

### Adding New Runners

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
        
class CustomParallelRunner(ParallelRunner):
    def custom_preprocessing(self):
        # Add custom logic before test execution
        pass
```

### Custom Assertions

```python
class CustomAssertions(Assertions):
    @staticmethod
    def performance_threshold(execution_time, max_time):
        if execution_time > max_time:
            raise AssertionError(f"Execution too slow: {execution_time}s > {max_time}s")
```

## 11. Version Compatibility

- **Python Version**: 3.6+
- **Dependencies**: Standard library only (no external dependencies for core functionality)
- **Backward Compatibility**: Fully compatible with existing `JSONRunner` code
- **Platform Support**: Windows, macOS, Linux

## 12. Performance Benchmarks

| Test Scenario | Sequential | Parallel (Thread) | Parallel (Process) | Speedup |
|---------------|------------|-------------------|-------------------|---------|
| 10 I/O tests  | 5.2s       | 1.4s              | 2.1s              | 3.7x    |
| 20 CPU tests  | 12.8s      | 8.9s              | 6.2s              | 2.1x    |
| Mixed tests   | 8.5s       | 2.3s              | 3.1s              | 3.7x    |

## 13. Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass: `python -m pytest tests/ -v`
5. Submit a pull request

## 14. License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**🚀 Ready to supercharge your testing workflow with parallel execution and advanced file comparison!**

For detailed parallel testing guide, see: [PARALLEL_TESTING_GUIDE.md](PARALLEL_TESTING_GUIDE.md)

================
File: src/cli_test_framework.egg-info/requires.txt
================
dukpy==0.5.0
h5py>=3.8.0
numpy>=2.0.1
setuptools>=75.8.0
wheel>=0.45.1

================
File: src/cli_test_framework.egg-info/SOURCES.txt
================
CHANGELOG.md
MANIFEST.in
README.md
pyproject.toml
setup.py
docs/user_manual.md
src/cli_test_framework/__init__.py
src/cli_test_framework/cli.py
src/cli_test_framework.egg-info/PKG-INFO
src/cli_test_framework.egg-info/SOURCES.txt
src/cli_test_framework.egg-info/dependency_links.txt
src/cli_test_framework.egg-info/entry_points.txt
src/cli_test_framework.egg-info/requires.txt
src/cli_test_framework.egg-info/top_level.txt
src/cli_test_framework/commands/__init__.py
src/cli_test_framework/commands/compare.py
src/cli_test_framework/core/__init__.py
src/cli_test_framework/core/assertions.py
src/cli_test_framework/core/base_runner.py
src/cli_test_framework/core/parallel_runner.py
src/cli_test_framework/core/process_worker.py
src/cli_test_framework/core/test_case.py
src/cli_test_framework/file_comparator/__init__.py
src/cli_test_framework/file_comparator/base_comparator.py
src/cli_test_framework/file_comparator/binary_comparator.py
src/cli_test_framework/file_comparator/csv_comparator.py
src/cli_test_framework/file_comparator/factory.py
src/cli_test_framework/file_comparator/h5_comparator.py
src/cli_test_framework/file_comparator/json_comparator.py
src/cli_test_framework/file_comparator/result.py
src/cli_test_framework/file_comparator/text_comparator.py
src/cli_test_framework/file_comparator/xml_comparator.py
src/cli_test_framework/runners/__init__.py
src/cli_test_framework/runners/json_runner.py
src/cli_test_framework/runners/parallel_json_runner.py
src/cli_test_framework/runners/yaml_runner.py
src/cli_test_framework/utils/__init__.py
src/cli_test_framework/utils/path_resolver.py
src/cli_test_framework/utils/report_generator.py
tests/__init__.py
tests/performance_test.py
tests/test1.py
tests/test_comprehensive_space.py
tests/test_parallel_runner.py
tests/test_parallel_space.py
tests/test_report.txt
tests/test_runners.py
tests/__pycache__/__init__.cpython-312.pyc
tests/__pycache__/test_parallel_runner.cpython-312-pytest-7.4.4.pyc
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json

================
File: src/cli_test_framework.egg-info/top_level.txt
================
cli_test_framework

================
File: src/cli_test_framework/__init__.py
================
"""
CLI Test Framework - A powerful command-line testing framework

This package provides tools for testing command-line applications and scripts
with support for parallel execution and advanced file comparison capabilities.
"""

__version__ = "0.2.4"
__author__ = "Xiaotong Wang"
__email__ = "xiaotongwang98@gmail.com"

# Import main classes for convenient access
from .runners.json_runner import JSONRunner
from .runners.parallel_json_runner import ParallelJSONRunner
from .runners.yaml_runner import YAMLRunner

__all__ = [
    'JSONRunner',
    'ParallelJSONRunner', 
    'YAMLRunner',
]

================
File: src/cli_test_framework/cli.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CLI Test Framework - Command Line Interface

This module provides the main command-line interface for the CLI Testing Framework.
"""

import argparse
import sys
import os
from pathlib import Path

from .runners import JSONRunner, ParallelJSONRunner, YAMLRunner


def create_parser():
    """Create and configure the argument parser"""
    parser = argparse.ArgumentParser(
        description="CLI Testing Framework - A powerful tool for testing command-line applications",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  cli-test run test_cases.json
  cli-test run test_cases.json --parallel --workers 4
  cli-test run test_cases.yaml --workspace /path/to/project
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Run command
    run_parser = subparsers.add_parser('run', help='Run test cases from a configuration file')
    run_parser.add_argument('config_file', help='Path to the test configuration file (JSON or YAML)')
    run_parser.add_argument('--workspace', '-w', help='Working directory for test execution')
    run_parser.add_argument('--parallel', '-p', action='store_true', help='Run tests in parallel')
    run_parser.add_argument('--workers', type=int, help='Number of parallel workers (default: CPU count)')
    run_parser.add_argument('--execution-mode', choices=['thread', 'process'], default='thread',
                           help='Parallel execution mode (default: thread)')
    run_parser.add_argument('--output-format', choices=['text', 'json', 'html'], default='text',
                           help='Output format for test results')
    run_parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')
    run_parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    
    return parser


def run_tests(args):
    """Run tests based on command line arguments"""
    config_file = Path(args.config_file)
    
    if not config_file.exists():
        print(f"Error: Configuration file not found: {config_file}")
        return False
    
    # Determine file type
    file_ext = config_file.suffix.lower()
    
    try:
        if args.parallel:
            # Use parallel runner
            runner = ParallelJSONRunner(
                config_file=str(config_file),
                workspace=args.workspace,
                max_workers=args.workers,
                execution_mode=args.execution_mode
            )
        else:
            # Use appropriate single-threaded runner
            if file_ext in ['.json']:
                runner = JSONRunner(
                    config_file=str(config_file),
                    workspace=args.workspace
                )
            elif file_ext in ['.yaml', '.yml']:
                runner = YAMLRunner(
                    config_file=str(config_file),
                    workspace=args.workspace
                )
            else:
                print(f"Error: Unsupported configuration file format: {file_ext}")
                return False
        
        # Run tests
        print(f"Running tests from: {config_file}")
        if args.parallel:
            print(f"Parallel mode: {args.execution_mode}, workers: {args.workers or 'auto'}")
        
        success = runner.run_tests()
        
        # Output results
        if hasattr(runner, 'results'):
            results = runner.results
            print(f"\nTest Results:")
            print(f"Total tests: {results.get('total_tests', 0)}")
            print(f"Passed: {results.get('passed', 0)}")
            print(f"Failed: {results.get('failed', 0)}")
            
            if args.verbose and 'details' in results:
                print("\nDetailed Results:")
                for result in results['details']:
                    status_symbol = "✓" if result['status'] == 'passed' else "✗"
                    print(f"  {status_symbol} {result['name']}: {result['status']}")
                    if result['status'] == 'failed' and result.get('message'):
                        print(f"    Error: {result['message']}")
        
        return success
        
    except Exception as e:
        print(f"Error running tests: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return False


def main():
    """Main entry point for the CLI"""
    parser = create_parser()
    args = parser.parse_args()
    
    if args.command == 'run':
        success = run_tests(args)
        sys.exit(0 if success else 1)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()

================
File: src/cli_test_framework/commands/__init__.py
================
"""
Command-line commands for the CLI Testing Framework
"""

from . import compare

__all__ = [
    'compare'
]

================
File: src/cli_test_framework/commands/compare.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file compare.py
@brief Command for comparing files in cli-test-framework
@author Xiaotong Wang
@date 2024
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from ..file_comparator.factory import ComparatorFactory
from ..file_comparator.result import ComparisonResult

def configure_logging():
    """Configure logging settings for the application"""
    logger = logging.getLogger("cli_test_framework.file_comparator")
    logger.setLevel(logging.INFO)
    
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    
    return logger

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Compare two files.")
    parser.add_argument("file1", help="Path to the first file")
    parser.add_argument("file2", help="Path to the second file")
    parser.add_argument("--start-line", type=int, default=1, help="Starting line number (1-based)")
    parser.add_argument("--end-line", type=int, help="Ending line number (1-based)")
    parser.add_argument("--start-column", type=int, default=1, help="Starting column number (1-based)")
    parser.add_argument("--end-column", type=int, help="Ending column number (1-based)")
    parser.add_argument("--file-type", help="Type of the files to compare", default="auto")
    parser.add_argument("--encoding", default="utf-8", help="File encoding for text files")
    parser.add_argument("--chunk-size", type=int, default=8192, help="Chunk size for binary comparison")
    parser.add_argument("--output-format", choices=["text", "json", "html"], default="text",
                        help="Output format for the comparison result")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with detailed logging")
    parser.add_argument("--similarity", action="store_true",
                        help="When comparing binary files, compute and show similarity index")
    parser.add_argument("--num-threads", type=int, default=4, help="Number of threads for parallel processing")
    
    # JSON comparison options
    json_group = parser.add_argument_group('JSON comparison options')
    json_group.add_argument("--json-compare-mode", choices=["exact", "key-based"], default="exact",
                      help="JSON comparison mode: exact (default) or key-based")
    json_group.add_argument("--json-key-field", help="Key field(s) to use for key-based JSON comparison")
    
    # H5 comparison options
    h5_group = parser.add_argument_group('HDF5 comparison options')
    h5_group.add_argument("--h5-table", help="Comma-separated list of table names to compare in HDF5 files")
    h5_group.add_argument("--h5-table-regex", help="Regular expression pattern to match table names in HDF5 files")
    h5_group.add_argument("--h5-structure-only", action="store_true", 
                         help="Only compare HDF5 file structure without comparing content")
    h5_group.add_argument("--h5-show-content-diff", action="store_true",
                         help="Show detailed content differences when content differs")
    h5_group.add_argument("--h5-rtol", type=float, default=1e-5,
                         help="Relative tolerance for numerical comparison in HDF5 files")
    h5_group.add_argument("--h5-atol", type=float, default=1e-8,
                         help="Absolute tolerance for numerical comparison in HDF5 files")
    
    return parser.parse_args()

def detect_file_type(file_path):
    """Detect the type of file based on its extension"""
    ext = file_path.suffix.lower()
    if ext in ['.txt', '.py', '.md', '.json', '.xml', '.html', '.css', '.js']:
        return 'text'
    elif ext == '.json':
        return 'json'
    elif ext in ['.h5', '.hdf5']:
        return 'h5'
    else:
        return 'binary'

def format_result(result, output_format):
    """Format the comparison result according to the specified output format"""
    if output_format == "json":
        return result.to_json()
    elif output_format == "html":
        return result.to_html()
    else:
        return str(result)

def main():
    """Main entry point for the compare-files command"""
    logger = configure_logging()

    try:
        args = parse_arguments()
        
        if args.debug:
            logger.setLevel(logging.DEBUG)
            logger.debug("Debug mode enabled")

        # Adjust for 0-based indexing
        start_line = max(0, args.start_line - 1)
        end_line = None if args.end_line is None else max(0, args.end_line - 1)
        start_column = max(0, args.start_column - 1)
        end_column = None if args.end_column is None else max(0, args.end_column - 1)

        # Resolve file paths
        file1_path = Path(args.file1).resolve()
        file2_path = Path(args.file2).resolve()

        if not file1_path.exists():
            raise ValueError(f"File not found: {file1_path}")
        if not file2_path.exists():
            raise ValueError(f"File not found: {file2_path}")

        # Determine file type
        file_type = args.file_type
        if file_type == "auto":
            file_type = detect_file_type(file1_path)
            logger.info(f"Auto-detected file type: {file_type}")

        # Prepare comparator kwargs
        comparator_kwargs = {
            "encoding": args.encoding,
            "chunk_size": args.chunk_size,
            "verbose": args.verbose or args.debug,
            "num_threads": args.num_threads
        }
        
        # Add file type specific arguments
        if file_type == "json":
            comparator_kwargs["compare_mode"] = args.json_compare_mode
            if args.json_key_field:
                key_fields = [field.strip() for field in args.json_key_field.split(',')]
                comparator_kwargs["key_field"] = key_fields[0] if len(key_fields) == 1 else key_fields
        
        if file_type == "h5":
            if args.h5_table:
                tables = [table.strip() for table in args.h5_table.split(',')]
                comparator_kwargs["tables"] = tables
            if args.h5_table_regex:
                comparator_kwargs["table_regex"] = args.h5_table_regex
            comparator_kwargs["structure_only"] = args.h5_structure_only
            comparator_kwargs["show_content_diff"] = args.h5_show_content_diff
            comparator_kwargs["rtol"] = args.h5_rtol
            comparator_kwargs["atol"] = args.h5_atol
        
        if file_type == "binary":
            comparator_kwargs["similarity"] = args.similarity

        # Create comparator and perform comparison
        comparator = ComparatorFactory.create_comparator(file_type, **comparator_kwargs)
        result = comparator.compare_files(
            file1_path,
            file2_path,
            start_line,
            end_line,
            start_column,
            end_column
        )

        # Output result
        output = format_result(result, args.output_format)
        print(output)

        sys.exit(0 if result.identical else 1)

    except ValueError as ve:
        logger.error(f"ValueError: {ve}")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"An unexpected error occurred")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: src/cli_test_framework/core/__init__.py
================
"""
Core components for the CLI Testing Framework
"""

from .base_runner import BaseRunner
from .parallel_runner import ParallelRunner
from .test_case import TestCase
from .assertions import Assertions

__all__ = [
    'BaseRunner',
    'ParallelRunner', 
    'TestCase',
    'Assertions'
]

================
File: src/cli_test_framework/core/assertions.py
================
import re
from typing import Any, Pattern

class Assertions:
    @staticmethod
    def equals(actual: Any, expected: Any, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected: {expected}, but got: {actual}")
        return True

    @staticmethod
    def contains(container: str, item: str, message: str = "") -> bool:
        """
        Check if the item is contained within the container string.
        This method returns True if the item is found anywhere within the container,
        even if the container contains other information.
        """
        if item not in container:
            raise AssertionError(f"{message} Expected to contain: {item}")
        return True

    @staticmethod
    def matches(text: str, pattern: str, message: str = "") -> bool:
        if not re.search(pattern, text):
            raise AssertionError(f"{message} Text does not match pattern: {pattern}")
        return True

    @staticmethod
    def return_code_equals(actual: int, expected: int, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected return code: {expected}, got: {actual}")
        return True

================
File: src/cli_test_framework/core/base_runner.py
================
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from .test_case import TestCase
from .assertions import Assertions

class BaseRunner(ABC):
    def __init__(self, config_file: str, workspace: Optional[str] = None):
        if workspace:
            self.workspace = Path(workspace)
        else:
            self.workspace = Path(__file__).parent.parent.parent
        self.config_path = self.workspace / config_file
        self.test_cases: List[TestCase] = []
        self.results: Dict[str, Any] = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "details": []
        }
        self.assertions = Assertions()

    @abstractmethod
    def load_test_cases(self) -> None:
        """Load test cases from configuration file"""
        pass

    def run_tests(self) -> bool:
        """Run all test cases and return whether all tests passed"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting test execution... Total tests: {self.results['total']}")
        print("=" * 50)
        
        for i, case in enumerate(self.test_cases, 1):
            print(f"\nRunning test {i}/{self.results['total']}: {case.name}")
            result = self.run_single_test(case)
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"✓ Test passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"✗ Test failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
                
        print("\n" + "=" * 50)
        print(f"Test execution completed. Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0

    @abstractmethod
    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        """Run a single test case and return the result"""
        pass

================
File: src/cli_test_framework/core/parallel_runner.py
================
from abc import ABC
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Union
import time
import threading
from .base_runner import BaseRunner
from .test_case import TestCase
from .process_worker import run_test_in_process

class ParallelRunner(BaseRunner):
    """并行测试运行器基类，支持多线程和多进程执行"""
    
    def __init__(self, config_file: str, workspace: Optional[str] = None, 
                 max_workers: Optional[int] = None, 
                 execution_mode: str = "thread"):
        """
        初始化并行运行器
        
        Args:
            config_file: 配置文件路径
            workspace: 工作目录
            max_workers: 最大并发数，默认为CPU核心数
            execution_mode: 执行模式，'thread'(线程) 或 'process'(进程)
        """
        super().__init__(config_file, workspace)
        self.max_workers = max_workers
        self.execution_mode = execution_mode
        self.lock = threading.Lock()  # 用于线程安全的结果更新
        
    def run_tests(self) -> bool:
        """并行运行所有测试用例"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting parallel test execution... Total tests: {self.results['total']}")
        print(f"Execution mode: {self.execution_mode}, Max workers: {self.max_workers or 'auto'}")
        print("=" * 50)
        
        start_time = time.time()
        
        if self.execution_mode == "process":
            executor_class = ProcessPoolExecutor
        else:
            executor_class = ThreadPoolExecutor
            
        with executor_class(max_workers=self.max_workers) as executor:
            # 提交所有测试任务
            if self.execution_mode == "process":
                # 进程模式：使用独立的工作器函数
                future_to_case = {
                    executor.submit(
                        run_test_in_process, 
                        i, 
                        {
                            "name": case.name,
                            "command": case.command,
                            "args": case.args,
                            "expected": case.expected
                        },
                        str(self.workspace) if self.workspace else None
                    ): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            else:
                # 线程模式：使用实例方法
                future_to_case = {
                    executor.submit(self._run_test_with_index, i, case): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            
            # 收集结果
            for future in as_completed(future_to_case):
                test_index, case = future_to_case[future]
                try:
                    result = future.result()
                    self._update_results(result, test_index, case)
                except Exception as exc:
                    error_result = {
                        "name": case.name,
                        "status": "failed",
                        "message": f"Test execution failed: {str(exc)}",
                        "output": "",
                        "command": "",
                        "return_code": None
                    }
                    self._update_results(error_result, test_index, case)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print("\n" + "=" * 50)
        print(f"Parallel test execution completed in {execution_time:.2f} seconds")
        print(f"Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0
    
    def _run_test_with_index(self, test_index: int, case: TestCase) -> Dict[str, Any]:
        """运行单个测试并返回结果（包含索引信息）"""
        print(f"[Worker] Running test {test_index}: {case.name}")
        result = self.run_single_test(case)
        return result
    
    def _update_results(self, result: Dict[str, Any], test_index: int, case: TestCase) -> None:
        """线程安全地更新测试结果"""
        with self.lock:
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"✓ Test {test_index} passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"✗ Test {test_index} failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
    
    def run_tests_sequential(self) -> bool:
        """回退到顺序执行模式"""
        print("Falling back to sequential execution...")
        return super().run_tests()

================
File: src/cli_test_framework/core/process_worker.py
================
"""
进程工作器模块
用于多进程并行测试执行，避免序列化问题
"""

import subprocess
import sys
from typing import Dict, Any
from .test_case import TestCase
from .assertions import Assertions

def run_test_in_process(test_index: int, case_data: Dict[str, Any], workspace: str = None) -> Dict[str, Any]:
    """
    在独立进程中运行单个测试用例
    
    Args:
        test_index: 测试索引
        case_data: 测试用例数据字典
        workspace: 工作目录
    
    Returns:
        测试结果字典
    """
    # 重新创建TestCase对象（避免序列化问题）
    case = TestCase(
        name=case_data["name"],
        command=case_data["command"],
        args=case_data["args"],
        expected=case_data["expected"]
    )
    
    # 创建断言对象
    assertions = Assertions()
    
    result = {
        "name": case.name,
        "status": "failed",
        "message": "",
        "output": "",
        "command": "",
        "return_code": None
    }

    try:
        command = f"{case.command} {' '.join(case.args)}"
        result["command"] = command
        print(f"  [Process Worker {test_index}] Executing command: {command}")
        
        process = subprocess.run(
            command,
            cwd=workspace if workspace else None,
            capture_output=True,
            text=True,
            check=False,
            shell=True
        )

        output = process.stdout + process.stderr
        result["output"] = output
        result["return_code"] = process.returncode
        
        if output.strip():
            print(f"  [Process Worker {test_index}] Command output for {case.name}:")
            for line in output.splitlines():
                print(f"    {line}")

        # 检查返回码
        if "return_code" in case.expected:
            print(f"  [Process Worker {test_index}] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
            assertions.return_code_equals(
                process.returncode,
                case.expected["return_code"]
            )

        # 检查输出包含
        if "output_contains" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output contains for {case.name}...")
            for expected_text in case.expected["output_contains"]:
                assertions.contains(output, expected_text)

        # 检查正则匹配
        if "output_matches" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output matches regex for {case.name}...")
            assertions.matches(output, case.expected["output_matches"])

        result["status"] = "passed"
        
    except AssertionError as e:
        result["message"] = str(e)
    except Exception as e:
        result["message"] = f"Execution error: {str(e)}"

    return result

================
File: src/cli_test_framework/core/test_case.py
================
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class TestCase:
    name: str
    command: str
    args: List[str]
    expected: Dict[str, Any]
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert test case to dictionary format"""
        print("Convert test case to dictionary format")
        print(self.command)
        return {
            "name": self.name,
            "command": self.command,
            "args": self.args,
            "expected": self.expected
        }

================
File: src/cli_test_framework/file_comparator/__init__.py
================
"""
File comparison module for cli-test-framework.
This module provides functionality for comparing different types of files.
"""

from .factory import ComparatorFactory
from .result import ComparisonResult

__all__ = ['ComparatorFactory', 'ComparisonResult']

================
File: src/cli_test_framework/file_comparator/base_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file base_comparator.py
@brief Base abstract class for file comparison operations
@author Xiaotong Wang
@date 2025
"""

from abc import ABC, abstractmethod
import logging
from pathlib import Path
from .result import ComparisonResult, Difference

class BaseComparator(ABC):
    """
    @brief Base abstract class for all file comparators
    @details This class defines the interface and common functionality for all file comparators.
             It provides basic file comparison operations and logging capabilities.
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False):
        """
        @brief Initialize the base comparator
        @param encoding str: File encoding to use (default: "utf-8")
        @param chunk_size int: Size of chunks for reading large files (default: 8192)
        @param verbose bool: Enable verbose logging (default: False)
        """
        self.encoding = encoding
        self.chunk_size = chunk_size
        self.logger = logging.getLogger(f"file_comparator.{self.__class__.__name__}")
        if verbose:
            self.logger.setLevel(logging.DEBUG)
    
    @abstractmethod
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read file content with specified range
        @param file_path Path: Path to the file to read
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return object: File content in a format suitable for comparison
        """
        pass

    @abstractmethod
    def compare_content(self, content1, content2):
        """
        @brief Compare two content objects and return comparison details
        @param content1 object: First content object to compare
        @param content2 object: Second content object to compare
        @return tuple: (bool, list) - (identical, differences)
        """
        pass

    def compare_files(self, file1, file2, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Compare two files with the specified parameters
        @param file1 Path: Path to the first file
        @param file2 Path: Path to the second file
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return ComparisonResult: Result object containing comparison details
        """
        result = ComparisonResult(
            file1=str(file1),
            file2=str(file2),
            start_line=start_line,
            end_line=end_line,
            start_column=start_column,
            end_column=end_column
        )
        
        try:
            self.logger.info(f"Comparing files: {file1} and {file2}")
            
            # Record file metadata
            file1_path = Path(file1)
            file2_path = Path(file2)
            result.file1_size = file1_path.stat().st_size
            result.file2_size = file2_path.stat().st_size
            
            # Read content with specified ranges
            self.logger.debug(f"Reading content from files")
            content1 = self.read_content(file1, start_line, end_line, start_column, end_column)
            content2 = self.read_content(file2, start_line, end_line, start_column, end_column)
            
            # Compare content
            self.logger.debug(f"Comparing content")
            identical, differences = self.compare_content(content1, content2)
            
            # Update result
            result.identical = identical
            result.differences = differences
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error during comparison: {str(e)}")
            result.error = str(e)
            result.identical = False
            return result

================
File: src/cli_test_framework/file_comparator/binary_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file binary_comparator.py
@brief Binary file comparator implementation with efficient byte-level comparison
@author Xiaotong Wang
@date 2025
"""

import hashlib
from .base_comparator import BaseComparator
from .result import Difference
from concurrent.futures import ThreadPoolExecutor

class BinaryComparator(BaseComparator):
    """
    @brief Comparator for binary files with efficient byte-level comparison
    @details This class implements binary file comparison with support for:
             - Byte-level difference detection
             - Similarity index calculation using LCS
             - Parallel processing for large files
             - File hash calculation
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False, similarity=False, num_threads=4):
        """
        @brief Initialize the binary comparator
        @param encoding str: File encoding (not used for binary files)
        @param chunk_size int: Size of chunks for reading large files
        @param verbose bool: Enable verbose logging
        @param similarity bool: Enable similarity index calculation
        @param num_threads int: Number of threads for parallel processing
        """
        super().__init__(encoding, chunk_size, verbose)
        self.similarity = similarity
        self.num_threads = num_threads

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read binary content with specified range
        @param file_path Path: Path to the binary file to read
        @param start_line int: Starting byte offset (interpreted as bytes for binary files)
        @param end_line int: Ending byte offset (interpreted as bytes for binary files)
        @param start_column int: Ignored for binary files
        @param end_column int: Ignored for binary files
        @return bytes: Binary content within the specified range
        @throws ValueError: If byte offsets are invalid
        @throws FileNotFoundError: If file doesn't exist
        @throws IOError: If there are other file reading errors
        """
        try:
            self.logger.debug(f"Reading binary file: {file_path}")
            
            # For binary files, interpret start_line as byte offset
            start_offset = start_line
            end_offset = end_line
            
            with open(file_path, 'rb') as f:
                if start_offset > 0:
                    f.seek(start_offset)
                
                if end_offset is not None:
                    if end_offset <= start_offset:
                        raise ValueError("End offset must be greater than start offset")
                    bytes_to_read = end_offset - start_offset
                    content = f.read(bytes_to_read)
                else:
                    content = f.read()
                    
            return content
                
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")
        except IOError as e:
            raise ValueError(f"Error reading file {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare binary content efficiently
        @param content1 bytes: First binary content to compare
        @param content2 bytes: Second binary content to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Performs efficient byte-level comparison of binary content.
                 Reports differences with hex context and limits the number
                 of differences to avoid overwhelming output.
        """
        self.logger.debug(f"Comparing binary content")
        
        if len(content1) != len(content2):
            differences = [Difference(
                position="file size",
                expected=f"{len(content1)} bytes",
                actual=f"{len(content2)} bytes",
                diff_type="size"
            )]
            identical = False
        elif content1 == content2:
            differences = []
            identical = True
        else:
            identical = False
            differences = []
            offset = 0
            max_differences = 10  # Limit number of differences reported
        
            for i in range(0, len(content1), self.chunk_size):
                chunk1 = content1[i:i+self.chunk_size]
                chunk2 = content2[i:i+self.chunk_size]
                
                if chunk1 != chunk2:
                    # Find the exact byte position where the difference starts
                    for j in range(len(chunk1)):
                        if j >= len(chunk2) or chunk1[j] != chunk2[j]:
                            diff_pos = i + j
                            # Show a few bytes before and after the difference for context
                            context_size = 8
                            start_ctx = max(0, diff_pos - context_size)
                            end_ctx = min(len(content1), diff_pos + context_size)
                            
                            # Create hex representations of the differing sections
                            expected_bytes = content1[start_ctx:end_ctx]
                            actual_bytes = content2[start_ctx:min(len(content2), end_ctx)]
                            
                            expected_hex = ' '.join(f"{b:02x}" for b in expected_bytes)
                            actual_hex = ' '.join(f"{b:02x}" for b in actual_bytes)
                            
                            differences.append(Difference(
                                position=f"byte {diff_pos}",
                                expected=expected_hex,
                                actual=actual_hex,
                                diff_type="content"
                            ))
                            break
                            
                    if len(differences) >= max_differences:
                        differences.append(Difference(
                            position=None,
                            expected=None,
                            actual=None,
                            diff_type=f"more differences not shown"
                        ))
                        break
        
        return identical, differences

    def compute_lcs_length(self, a: bytes, b: bytes) -> int:
        """
        @brief Compute the length of the longest common subsequence
        @param a bytes: First binary sequence
        @param b bytes: Second binary sequence
        @return int: Length of the longest common subsequence
        @details Uses dynamic programming with memory optimization to compute LCS.
                 Supports parallel processing for large sequences.
        """
        if not a or not b:
            return 0

        def lcs_worker(start, end):
            previous = [0] * (len(b) + 1)
            for i in range(start, end):
                current = [0] * (len(b) + 1)
                for j in range(1, len(b) + 1):
                    if a[i - 1] == b[j - 1]:
                        current[j] = previous[j - 1] + 1
                    else:
                        current[j] = max(previous[j], current[j - 1])
                previous = current
            return previous[len(b)]

        chunk_size = len(a) // self.num_threads
        futures = []

        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            for i in range(self.num_threads):
                start = i * chunk_size
                end = (i + 1) * chunk_size if i != self.num_threads - 1 else len(a)
                futures.append(executor.submit(lcs_worker, start, end))

        lcs_length = sum(f.result() for f in futures)
        return lcs_length

    def compare_files(self, file1, file2, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Compare two binary files with optional similarity calculation
        @param file1 Path: Path to the first binary file
        @param file2 Path: Path to the second binary file
        @param start_line int: Starting byte offset
        @param end_line int: Ending byte offset
        @param start_column int: Ignored for binary files
        @param end_column int: Ignored for binary files
        @return ComparisonResult: Result object containing comparison details
        """
        from pathlib import Path
        from .result import ComparisonResult
        result = ComparisonResult(
            file1=str(file1),
            file2=str(file2),
            start_line=start_line,
            end_line=end_line,
            start_column=start_column,
            end_column=end_column
        )
        try:
            self.logger.info(f"Comparing files: {file1} and {file2}")
            file1_path = Path(file1)
            file2_path = Path(file2)
            result.file1_size = file1_path.stat().st_size
            result.file2_size = file2_path.stat().st_size
            self.logger.debug("Reading content from files")
            content1 = self.read_content(file1, start_line, end_line, start_column, end_column)
            content2 = self.read_content(file2, start_line, end_line, start_column, end_column)
            self.logger.debug("Comparing content")
            identical, differences = self.compare_content(content1, content2)
            result.identical = identical
            result.differences = differences
            if self.similarity:
                if (len(content1) + len(content2)) > 0:
                    lcs_len = self.compute_lcs_length(content1, content2)
                    similarity = 2 * lcs_len / (len(content1) + len(content2))
                else:
                    similarity = 1
                result.similarity = similarity
            return result
        except Exception as e:
            self.logger.error(f"Error during comparison: {str(e)}")
            result.error = str(e)
            result.identical = False
            return result

    def get_file_hash(self, file_path, chunk_size=8192):
        """
        @brief Calculate SHA-256 hash of a file efficiently
        @param file_path Path: Path to the file to hash
        @param chunk_size int: Size of chunks for reading large files
        @return str: Hexadecimal representation of the file's SHA-256 hash
        """
        h = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(chunk_size), b''):
                h.update(chunk)
        return h.hexdigest()

================
File: src/cli_test_framework/file_comparator/csv_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file csv_comparator.py
@brief CSV file comparator implementation with row and column comparison
@author Xiaotong Wang
@date 2025
"""

import csv
import io
from .text_comparator import TextComparator
from .result import Difference

class CsvComparator(TextComparator):
    """
    @brief Comparator for CSV files with row and column comparison
    @details This class extends TextComparator to provide specialized CSV comparison
             capabilities, including:
             - Row count comparison
             - Column count comparison
             - Cell value comparison
             - Configurable delimiter and quote character
    """
    
    def __init__(self, encoding="utf-8", delimiter=",", quotechar='"', chunk_size=8192, verbose=False):
        """
        @brief Initialize CSV comparator with configuration
        @param encoding str: File encoding (default: utf-8)
        @param delimiter str: CSV field delimiter (default: comma)
        @param quotechar str: Character used for quoting fields (default: double quote)
        @param chunk_size int: Size of chunks for reading large files
        @param verbose bool: Enable verbose output
        """
        super().__init__(encoding, chunk_size, verbose)
        self.delimiter = delimiter
        self.quotechar = quotechar
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse CSV content from file
        @param file_path Path: Path to the CSV file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return list: List of rows, where each row is a list of cell values
        @details Reads CSV content and parses it into a structured format,
                 supporting line and column range selection
        """
        # First read the file as text
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Join the lines to create a CSV string
        csv_text = ''.join(text_content)
        
        # Parse the CSV
        csv_data = []
        csv_reader = csv.reader(
            io.StringIO(csv_text), 
            delimiter=self.delimiter,
            quotechar=self.quotechar
        )
        
        for row in csv_reader:
            # Apply column range if specified
            if start_column > 0 or end_column is not None:
                col_start = start_column
                col_end = end_column if end_column is not None else len(row)
                row = row[col_start:col_end+1]
            csv_data.append(row)
            
        return csv_data
    
    def compare_content(self, content1, content2):
        """
        @brief Compare CSV content structurally
        @param content1 list: First CSV data to compare (list of rows)
        @param content2 list: Second CSV data to compare (list of rows)
        @return tuple: (bool, list) - (identical, differences)
        @details Performs structural comparison of CSV data, including:
                 - Row count comparison
                 - Column count comparison per row
                 - Cell value comparison
                 - Limits the number of reported differences
        """
        if content1 == content2:
            return True, []
            
        differences = []
        
        # Check row count
        if len(content1) != len(content2):
            differences.append(Difference(
                position="row count",
                expected=f"{len(content1)} rows",
                actual=f"{len(content2)} rows",
                diff_type="row_count_mismatch"
            ))
        
        # Compare rows
        max_diffs = 10
        for i, (row1, row2) in enumerate(zip(content1, content2)):
            # Check column count in this row
            if len(row1) != len(row2):
                differences.append(Difference(
                    position=f"row {i+1}",
                    expected=f"{len(row1)} columns",
                    actual=f"{len(row2)} columns",
                    diff_type="column_count_mismatch"
                ))
                if len(differences) >= max_diffs:
                    break
            
            # Compare column values
            for j, (cell1, cell2) in enumerate(zip(row1, row2)):
                if cell1 != cell2:
                    differences.append(Difference(
                        position=f"row {i+1}, column {j+1}",
                        expected=cell1,
                        actual=cell2,
                        diff_type="cell_mismatch"
                    ))
                    if len(differences) >= max_diffs:
                        break
            
            if len(differences) >= max_diffs:
                break
        
        # Add a summary if there are more differences
        if len(differences) >= max_diffs:
            differences.append(Difference(
                position=None,
                expected=None,
                actual=None,
                diff_type=f"more differences not shown"
            ))
        
        return False, differences

================
File: src/cli_test_framework/file_comparator/factory.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file factory.py
@brief Factory class for creating file comparators based on file type
@author Xiaotong Wang
@date 2025
"""

import os
import importlib
import pkgutil
from pathlib import Path

class ComparatorFactory:
    """
    @brief Factory class for creating file comparators
    @details This class manages the creation and registration of different types of file comparators.
             It provides a centralized way to create appropriate comparators based on file type
             and handles parameter filtering for different comparator types.
    """
    _comparators = {}
    _initialized = False

    @staticmethod
    def register_comparator(file_type, comparator_class):
        """
        @brief Register a new comparator class for a specific file type
        @param file_type str: Type of file the comparator handles
        @param comparator_class class: Comparator class to register
        """
        ComparatorFactory._comparators[file_type.lower()] = comparator_class

    @staticmethod
    def create_comparator(file_type, **kwargs):
        """
        @brief Create a comparator instance for the specified file type
        @param file_type str: Type of file to compare
        @param **kwargs: Additional arguments to pass to the comparator
        @return BaseComparator: An instance of the appropriate comparator class
        @details Creates and returns a comparator instance based on the file type.
                 If no specific comparator is found, falls back to TextComparator
                 for text files or BinaryComparator for other types.
        """
        if not ComparatorFactory._initialized:
            ComparatorFactory._load_comparators()

        comparator_class = ComparatorFactory._comparators.get(file_type.lower())
        if not comparator_class:
            if file_type.lower() in ['auto', 'text']:
                from .text_comparator import TextComparator
                # Only pass TextComparator supported parameters
                text_kwargs = {k: v for k, v in kwargs.items() 
                              if k in ['encoding', 'chunk_size', 'verbose']}
                return TextComparator(**text_kwargs)
            else:
                from .binary_comparator import BinaryComparator
                # Pass all BinaryComparator supported parameters
                binary_kwargs = {k: v for k, v in kwargs.items()
                               if k in ['chunk_size', 'verbose', 'similarity', 'num_threads']}
                return BinaryComparator(**binary_kwargs)

        # Filter parameters based on comparator type
        if file_type.lower() == 'h5':
            # H5 comparator accepts specific parameters
            h5_kwargs = {k: v for k, v in kwargs.items()
                        if k in ['tables', 'table_regex', 'encoding', 'chunk_size', 'verbose', 'structure_only', 'show_content_diff', 'debug', 'rtol', 'atol']}
            return comparator_class(**h5_kwargs)
        elif file_type.lower() == 'binary':
            # Binary comparator accepts all parameters, including num_threads
            binary_kwargs = {k: v for k, v in kwargs.items()
                           if k in ['chunk_size', 'verbose', 'similarity', 'num_threads']}
            return comparator_class(**binary_kwargs)
        elif file_type.lower() == 'json':
            # JSON comparator accepts specific parameters
            json_kwargs = {k: v for k, v in kwargs.items()
                         if k in ['encoding', 'chunk_size', 'verbose', 'compare_mode', 'key_field']}
            return comparator_class(**json_kwargs)
        else:
            # Other comparators only accept basic parameters
            basic_kwargs = {k: v for k, v in kwargs.items()
                          if k in ['encoding', 'chunk_size', 'verbose']}
            return comparator_class(**basic_kwargs)

    @staticmethod
    def _load_comparators():
        """
        @brief Load and register all available comparators
        @details Automatically discovers and registers comparator classes from the package.
                 This includes both built-in comparators and any additional comparators
                 that follow the naming convention '*_comparator.py'.
        """
        from .text_comparator import TextComparator
        from .binary_comparator import BinaryComparator

        ComparatorFactory.register_comparator('text', TextComparator)
        ComparatorFactory.register_comparator('binary', BinaryComparator)

        package_dir = Path(__file__).parent
        for module_info in pkgutil.iter_modules([str(package_dir)]):
            if module_info.name.endswith('_comparator') and module_info.name not in [
                'base_comparator', 'text_comparator', 'binary_comparator'
            ]:
                try:
                    module = importlib.import_module(f".{module_info.name}", package=__package__)

                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if (isinstance(attr, type) and
                            attr.__module__ == module.__name__ and
                            attr_name.endswith('Comparator')):
                            type_name = attr_name.lower().replace('comparator', '')
                            ComparatorFactory.register_comparator(type_name, attr)
                except ImportError as e:
                    print(f"Failed to import comparator module {module_info.name}: {e}")

        ComparatorFactory._initialized = True

    @staticmethod
    def get_available_comparators():
        """
        @brief Get a list of all registered comparator types
        @return list: List of available comparator type names
        """
        if not ComparatorFactory._initialized:
            ComparatorFactory._load_comparators()
        return sorted(ComparatorFactory._comparators.keys())

# Register built-in comparators
from .json_comparator import JsonComparator
from .xml_comparator import XmlComparator
from .csv_comparator import CsvComparator
from .text_comparator import TextComparator
from .binary_comparator import BinaryComparator

ComparatorFactory.register_comparator('json', JsonComparator)
ComparatorFactory.register_comparator('xml', XmlComparator)
ComparatorFactory.register_comparator('csv', CsvComparator)
ComparatorFactory.register_comparator('text', TextComparator)
ComparatorFactory.register_comparator('binary', BinaryComparator)

================
File: src/cli_test_framework/file_comparator/h5_comparator.py
================
from .base_comparator import BaseComparator
import h5py
import numpy as np
import logging
import re

class H5Comparator(BaseComparator):
    def __init__(self, tables=None, table_regex=None, structure_only=False, show_content_diff=False, debug=False, rtol=1e-5, atol=1e-8, **kwargs):
        """
        Initialize H5 comparator
        :param tables: List of table names to compare. If None, compare all tables
        :param table_regex: Regular expression pattern to match table names
        :param structure_only: If True, only compare file structure without comparing content
        :param show_content_diff: If True, show detailed content differences
        :param debug: If True, enable debug mode
        :param rtol: Relative tolerance for numerical comparison
        :param atol: Absolute tolerance for numerical comparison
        """
        super().__init__(**kwargs)
        self.tables = tables
        self.table_regex = table_regex
        self.structure_only = structure_only
        self.show_content_diff = show_content_diff
        self.rtol = rtol
        self.atol = atol
        
        # Set debug level if verbose is enabled
        if kwargs.get('verbose', False) or debug:
            self.logger.setLevel(logging.DEBUG)
            
        self.logger.debug(f"Initialized H5Comparator with structure_only={structure_only}, show_content_diff={show_content_diff}, rtol={rtol}, atol={atol}")
        if table_regex:
            self.logger.debug(f"Using table regex pattern: {table_regex}")

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """Read H5 file content"""
        content = {}
        
        # Log whether we're in structure-only mode
        self.logger.debug(f"Reading file {file_path} in structure-only mode: {self.structure_only}")
        self.logger.debug(f"Tables parameter: {self.tables}")
        self.logger.debug(f"Table regex parameter: {self.table_regex}")
        
        with h5py.File(file_path, 'r') as f:
            # Function to collect structure information
            def collect_structure(name, obj):
                if isinstance(obj, h5py.Dataset):
                    content[name] = {
                        'type': 'dataset',
                        'shape': obj.shape,
                        'dtype': str(obj.dtype),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for dataset: {name}")
                elif isinstance(obj, h5py.Group) and name:  # Skip root group
                    content[name] = {
                        'type': 'group',
                        'keys': list(obj.keys()),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for group: {name}")
            
            # Function to collect structure and data
            def collect_structure_and_data(name, obj):
                if isinstance(obj, h5py.Dataset):
                    dataset_info = {
                        'type': 'dataset',
                        'shape': obj.shape,
                        'dtype': str(obj.dtype),
                        'attrs': dict(obj.attrs)
                    }
                    
                    # Read data with range constraints
                    try:
                        data = obj[:]
                        if isinstance(data, np.ndarray):
                            if end_line is None:
                                end_line_actual = data.shape[0]
                            else:
                                end_line_actual = min(end_line, data.shape[0])
                                
                            if len(data.shape) == 1:
                                data = data[start_line:end_line_actual]
                            elif len(data.shape) > 1:
                                if end_column is None:
                                    end_column_actual = data.shape[1]
                                else:
                                    end_column_actual = min(end_column, data.shape[1])
                                data = data[start_line:end_line_actual, start_column:end_column_actual]
                        
                        dataset_info['data'] = data
                        self.logger.debug(f"Collected data for dataset: {name}")
                    except Exception as e:
                        self.logger.error(f"Error reading data from {name}: {str(e)}")
                    
                    content[name] = dataset_info
                    
                elif isinstance(obj, h5py.Group) and name:  # Skip root group
                    content[name] = {
                        'type': 'group',
                        'keys': list(obj.keys()),
                        'attrs': dict(obj.attrs)
                    }
                    self.logger.debug(f"Collected structure for group: {name}")
            
            if self.tables or self.table_regex:
                # If specific tables or regex pattern is specified
                if self.table_regex:
                    # If the regex looks like a simple path (no regex metacharacters except . and /),
                    # escape it to treat it as a literal string
                    regex_str = self.table_regex
                    self.logger.debug(f"Original table_regex: {regex_str}")
                    # Check if it contains regex metacharacters other than . and /
                    import string
                    regex_metacharacters = set('[]{}()*+?^$|\\')
                    if not any(char in regex_str for char in regex_metacharacters):
                        # Escape dots and other special characters for literal matching
                        regex_str = re.escape(regex_str)
                        self.logger.debug(f"Treating table_regex as literal path, escaped: {regex_str}")
                    else:
                        self.logger.debug(f"Using table_regex as regular expression: {regex_str}")
                    regex_pattern = re.compile(regex_str)
                else:
                    regex_pattern = None
                
                def should_process(name):
                    if self.tables and name in self.tables:
                        self.logger.debug(f"Matched by tables list: {name}")
                        return True
                    if regex_pattern and regex_pattern.fullmatch(name):
                        self.logger.debug(f"Matched by regex: {name}")
                        return True
                    return False
                
                def process_item(name, item):
                    try:
                        if self.structure_only:
                            collect_structure(name, item)
                        else:
                            collect_structure_and_data(name, item)
                    except Exception as e:
                        self.logger.error(f"Error processing {name}: {str(e)}")
                
                # First try direct path access for table names
                if self.tables:
                    for table_path in self.tables:
                        try:
                            if table_path in f:
                                process_item(table_path, f[table_path])
                            else:
                                self.logger.warning(f"Table {table_path} not found in {file_path}")
                        except Exception as e:
                            self.logger.error(f"Error processing {table_path}: {str(e)}")
                
                # Then process regex pattern if specified
                if regex_pattern:
                    def visit_with_regex(name, obj):
                        self.logger.debug(f"Checking path: {name}")
                        if should_process(name):
                            self.logger.debug(f"Processing matched path: {name}")
                            process_item(name, obj)
                        else:
                            self.logger.debug(f"Skipping path: {name}")
                    f.visititems(visit_with_regex)
            else:
                # If no tables specified, read all datasets
                if self.structure_only:
                    f.visititems(collect_structure)
                else:
                    f.visititems(collect_structure_and_data)
        
        self.logger.debug(f"Read {len(content)} items from {file_path}")
        self.logger.debug(f"Items read: {list(content.keys())}")
        return content

    def compare_content(self, content1, content2):
        """Compare two H5 file contents"""
        identical = True
        differences = []

        # Get all unique table names
        all_tables = set(content1.keys()) | set(content2.keys())
        
        # Debug log
        self.logger.debug(f"Structure-only mode: {self.structure_only}")
        self.logger.debug(f"Number of tables to compare: {len(all_tables)}")
        
        for table_name in all_tables:
            # Debug log
            self.logger.debug(f"Comparing table: {table_name}")
            if table_name in content1 and table_name in content2:
                self.logger.debug(f"Table1 keys: {content1[table_name].keys()}")
                self.logger.debug(f"Table2 keys: {content2[table_name].keys()}")
            
            # Check if table exists in both files
            if table_name not in content1:
                differences.append(self._create_difference(
                    position=table_name,
                    expected="Table exists",
                    actual="Table missing",
                    diff_type="structure"
                ))
                identical = False
                continue
                
            if table_name not in content2:
                differences.append(self._create_difference(
                    position=table_name,
                    expected="Table exists",
                    actual="Table missing",
                    diff_type="structure"
                ))
                identical = False
                continue
            
            table1 = content1[table_name]
            table2 = content2[table_name]
            
            # Compare table type
            if table1.get('type') != table2.get('type'):
                differences.append(self._create_difference(
                    position=f"{table_name}/type",
                    expected=table1.get('type'),
                    actual=table2.get('type'),
                    diff_type="structure"
                ))
                identical = False
                continue
            
            # For datasets, compare shape and dtype
            if table1.get('type') == 'dataset':
                if table1['shape'] != table2['shape']:
                    differences.append(self._create_difference(
                        position=f"{table_name}/shape",
                        expected=str(table1['shape']),
                        actual=str(table2['shape']),
                        diff_type="structure"
                    ))
                    identical = False
                
                if table1['dtype'] != table2['dtype']:
                    differences.append(self._create_difference(
                        position=f"{table_name}/dtype",
                        expected=str(table1['dtype']),
                        actual=str(table2['dtype']),
                        diff_type="structure"
                    ))
                    identical = False
            
            # For groups, compare keys
            elif table1.get('type') == 'group':
                keys1 = set(table1['keys'])
                keys2 = set(table2['keys'])
                if keys1 != keys2:
                    missing_keys = keys1 - keys2
                    extra_keys = keys2 - keys1
                    if missing_keys:
                        differences.append(self._create_difference(
                            position=f"{table_name}/keys",
                            expected=str(sorted(missing_keys)),
                            actual="Keys missing",
                            diff_type="structure"
                        ))
                    if extra_keys:
                        differences.append(self._create_difference(
                            position=f"{table_name}/keys",
                            expected="No extra keys",
                            actual=str(sorted(extra_keys)),
                            diff_type="structure"
                        ))
                    identical = False
            
            # Only compare attributes and data if not in structure-only mode
            if not self.structure_only:
                self.logger.debug(f"Comparing attributes and data for {table_name}")
                
                # Compare attributes
                attr_diff = self._compare_attributes(table1['attrs'], table2['attrs'], table_name)
                if attr_diff:
                    differences.extend(attr_diff)
                    identical = False
                
                # Compare data content
                if 'data' in table1 and 'data' in table2:
                    data1 = table1['data']
                    data2 = table2['data']
                    
                    if isinstance(data1, np.ndarray) and isinstance(data2, np.ndarray):
                        try:
                            # 对于数值类型数据使用 isclose
                            if np.issubdtype(data1.dtype, np.number) and np.issubdtype(data2.dtype, np.number):
                                equal_mask = np.isclose(data1, data2, equal_nan=True, rtol=self.rtol, atol=self.atol)
                                if not np.all(equal_mask):
                                    diff_indices = np.where(~equal_mask)
                                    if self.show_content_diff:
                                        # Report up to 10 differences
                                        for idx in zip(*diff_indices)[:10]:
                                            position = f"{table_name}[{','.join(map(str, idx))}]"
                                            differences.append(self._create_difference(
                                                position=position,
                                                expected=str(data1[idx]),
                                                actual=str(data2[idx]),
                                                diff_type="content"
                                            ))
                                    else:
                                        # Just report that content differs
                                        differences.append(self._create_difference(
                                            position=table_name,
                                            expected="Same content",
                                            actual="Content differs",
                                            diff_type="content"
                                        ))
                                    identical = False
                            # 对于字符串或其他类型直接比较
                            else:
                                if not np.array_equal(data1, data2):
                                    if self.show_content_diff:
                                        # For non-numeric arrays, find the first difference
                                        diff_indices = np.where(data1 != data2)
                                        for idx in zip(*diff_indices)[:10]:
                                            position = f"{table_name}[{','.join(map(str, idx))}]"
                                            differences.append(self._create_difference(
                                                position=position,
                                                expected=str(data1[idx]),
                                                actual=str(data2[idx]),
                                                diff_type="content"
                                            ))
                                    else:
                                        differences.append(self._create_difference(
                                            position=table_name,
                                            expected="Same content",
                                            actual="Content differs",
                                            diff_type="content"
                                        ))
                                    identical = False
                        except Exception as e:
                            self.logger.error(f"Error comparing data in table {table_name}: {str(e)}")
                            differences.append(self._create_difference(
                                position=table_name,
                                expected=f"Data type: {table1.get('dtype', 'unknown')}",
                                actual=f"Data type: {table2.get('dtype', 'unknown')}",
                                diff_type="error"
                            ))
                            identical = False
        
        return identical, differences

    def _compare_attributes(self, attrs1, attrs2, table_name):
        """Compare HDF5 attributes"""
        differences = []
        
        # Compare attribute keys
        keys1 = set(attrs1.keys())
        keys2 = set(attrs2.keys())
        
        # Check for missing attributes
        for key in keys1 - keys2:
            differences.append(self._create_difference(
                position=f"{table_name}/attrs/{key}",
                expected=str(attrs1[key]),
                actual="Attribute missing",
                diff_type="missing_attribute"
            ))
            
        for key in keys2 - keys1:
            differences.append(self._create_difference(
                position=f"{table_name}/attrs/{key}",
                expected="Attribute missing",
                actual=str(attrs2[key]),
                diff_type="extra_attribute"
            ))
            
        # Compare common attributes
        for key in keys1 & keys2:
            val1 = attrs1[key]
            val2 = attrs2[key]
            
            # Handle numpy arrays in attributes
            if isinstance(val1, np.ndarray) and isinstance(val2, np.ndarray):
                try:
                    if not np.array_equal(val1, val2):
                        differences.append(self._create_difference(
                            position=f"{table_name}/attrs/{key}",
                            expected=str(val1),
                            actual=str(val2),
                            diff_type="attribute"
                        ))
                except Exception as e:
                    self.logger.error(f"Error comparing array attribute {key}: {str(e)}")
                    differences.append(self._create_difference(
                        position=f"{table_name}/attrs/{key}",
                        expected=str(val1),
                        actual=str(val2),
                        diff_type="attribute"
                    ))
            elif isinstance(val1, np.ndarray) or isinstance(val2, np.ndarray):
                # One is array, one is not - they're different
                differences.append(self._create_difference(
                    position=f"{table_name}/attrs/{key}",
                    expected=str(val1),
                    actual=str(val2),
                    diff_type="attribute"
                ))
            else:
                # Regular comparison for non-array values
                if val1 != val2:
                    differences.append(self._create_difference(
                        position=f"{table_name}/attrs/{key}",
                        expected=str(val1),
                        actual=str(val2),
                        diff_type="attribute"
                    ))
                
        return differences

    def _create_difference(self, position, expected, actual, diff_type):
        """Create a Difference object"""
        from .result import Difference
        return Difference(position=position, expected=expected, actual=actual, diff_type=diff_type)

# Register the new comparator
from .factory import ComparatorFactory
ComparatorFactory.register_comparator('h5', H5Comparator)

================
File: src/cli_test_framework/file_comparator/json_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file json_comparator.py
@brief JSON file comparator implementation with support for exact and key-based comparison
@author Xiaotong Wang
@date 2025
"""

import json
from .text_comparator import TextComparator
from .result import Difference

class JsonComparator(TextComparator):
    """
    @brief Comparator for JSON files with support for exact and key-based comparison
    @details This class extends TextComparator to provide specialized JSON comparison
             capabilities, including:
             - Exact comparison of JSON structures
             - Key-based comparison for lists of objects
             - Detailed difference reporting with path information
    """
    
    def __init__(self, encoding="utf-8", chunk_size=8192, verbose=False, key_field=None, compare_mode="exact"):
        """
        @brief Initialize the JSON comparator
        @param encoding str: File encoding
        @param chunk_size int: Chunk size for reading files
        @param verbose bool: Enable verbose logging
        @param key_field str or list: Field name(s) to use as key for comparing JSON objects in lists
        @param compare_mode str: Comparison mode: 'exact' (default) or 'key-based'
        """
        super().__init__(encoding, chunk_size, verbose)
        self.key_field = key_field
        self.compare_mode = compare_mode

    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse JSON content from file
        @param file_path Path: Path to the JSON file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return dict or list: Parsed JSON content
        @throws ValueError: If JSON is invalid or key fields are missing
        """
        # Read the text content using the parent class method
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Convert to a single string
        json_text = ''.join(text_content)
        try:
            json_data = json.loads(json_text)
            if self.key_field:
                # Only keep the specified key field(s)
                key_fields = self.key_field if isinstance(self.key_field, list) else [self.key_field]
                filtered_data = {key: json_data[key] for key in key_fields if key in json_data}
                return filtered_data
            return json_data
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in {file_path}: {str(e)}")

    def compare_content(self, content1, content2):
        """
        @brief Compare JSON content using the specified comparison mode
        @param content1 dict or list: First JSON content to compare
        @param content2 dict or list: Second JSON content to compare
        @return tuple: (bool, list) - (identical, differences)
        """
        # Quick check for exact equality
        if content1 == content2:
            return True, []
        
        # Different comparison modes
        differences = []
        if self.compare_mode == "key-based" and self.key_field:
            self._compare_json_key_based(content1, content2, "", differences)
        else:
            self._compare_json_exact(content1, content2, "", differences)
            
        return False, differences

    def _compare_json_exact(self, obj1, obj2, path, differences, max_diffs=10):
        """
        @brief Perform exact JSON comparison
        @param obj1: First JSON object to compare
        @param obj2: Second JSON object to compare
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Compares JSON objects recursively, checking for:
                 - Type mismatches
                 - Missing or extra keys in dictionaries
                 - Length mismatches in lists
                 - Value mismatches
        """
        if len(differences) >= max_diffs:
            return

        # Type check
        if type(obj1) != type(obj2):
            differences.append(Difference(
                position=path or "root",
                expected=f"{type(obj1).__name__}: {obj1}",
                actual=f"{type(obj2).__name__}: {obj2}",
                diff_type="type_mismatch"
            ))
            return

        # Dictionary comparison
        if isinstance(obj1, dict):
            keys1 = set(obj1.keys())
            keys2 = set(obj2.keys())

            # Check for missing keys
            for key in keys1 - keys2:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=obj1[key],
                    actual=None,
                    diff_type="missing_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Check for extra keys
            for key in keys2 - keys1:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=None,
                    actual=obj2[key],
                    diff_type="extra_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Compare common keys recursively
            for key in keys1 & keys2:
                new_path = f"{path}.{key}" if path else key
                self._compare_json_exact(obj1[key], obj2[key], new_path, differences, max_diffs)

        # List comparison
        elif isinstance(obj1, list):
            if len(obj1) != len(obj2):
                differences.append(Difference(
                    position=path or "root",
                    expected=f"list with {len(obj1)} items",
                    actual=f"list with {len(obj2)} items",
                    diff_type="length_mismatch"
                ))

            # Compare items by position
            for i in range(min(len(obj1), len(obj2))):
                new_path = f"{path}[{i}]"
                self._compare_json_exact(obj1[i], obj2[i], new_path, differences, max_diffs)

        # Value comparison
        elif obj1 != obj2:
            differences.append(Difference(
                position=path or "root",
                expected=obj1,
                actual=obj2,
                diff_type="value_mismatch"
            ))

    def _compare_json_key_based(self, obj1, obj2, path, differences, max_diffs=10):
        """
        @brief Perform key-based JSON comparison for lists of objects
        @param obj1: First JSON object to compare
        @param obj2: Second JSON object to compare
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Similar to exact comparison but with special handling for lists
                 of objects, using key fields to match items instead of position
        """
        if len(differences) >= max_diffs:
            return

        # Type check
        if type(obj1) != type(obj2):
            differences.append(Difference(
                position=path or "root",
                expected=f"{type(obj1).__name__}: {obj1}",
                actual=f"{type(obj2).__name__}: {obj2}",
                diff_type="type_mismatch"
            ))
            return

        # Dictionary comparison (same as exact comparison)
        if isinstance(obj1, dict):
            keys1 = set(obj1.keys())
            keys2 = set(obj2.keys())

            # Check for missing keys
            for key in keys1 - keys2:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=obj1[key],
                    actual=None,
                    diff_type="missing_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Check for extra keys
            for key in keys2 - keys1:
                differences.append(Difference(
                    position=f"{path}.{key}" if path else key,
                    expected=None,
                    actual=obj2[key],
                    diff_type="extra_key"
                ))
                if len(differences) >= max_diffs:
                    return

            # Compare common keys recursively
            for key in keys1 & keys2:
                new_path = f"{path}.{key}" if path else key
                self._compare_json_key_based(obj1[key], obj2[key], new_path, differences, max_diffs)

        # List comparison - the key difference for key-based comparison
        elif isinstance(obj1, list) and isinstance(obj2, list):
            # Check if we can do key-based comparison
            if self.key_field and all(isinstance(item, dict) for item in obj1 + obj2):
                self._compare_lists_by_key(obj1, obj2, path, differences, max_diffs)
            else:
                # Fall back to position-based comparison if key-based is not possible
                if len(obj1) != len(obj2):
                    differences.append(Difference(
                        position=path or "root",
                        expected=f"list with {len(obj1)} items",
                        actual=f"list with {len(obj2)} items", 
                        diff_type="length_mismatch"
                    ))

                # Compare items by position
                for i in range(min(len(obj1), len(obj2))):
                    new_path = f"{path}[{i}]"
                    self._compare_json_key_based(obj1[i], obj2[i], new_path, differences, max_diffs)

        # Value comparison
        elif obj1 != obj2:
            differences.append(Difference(
                position=path or "root",
                expected=obj1,
                actual=obj2,
                diff_type="value_mismatch"
            ))

    def _compare_lists_by_key(self, list1, list2, path, differences, max_diffs=10):
        """
        @brief Compare two lists of dictionaries using key field(s) to match items
        @param list1 list: First list of dictionaries
        @param list2 list: Second list of dictionaries
        @param path str: Current path in the JSON structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Matches items in lists using specified key fields instead of position,
                 allowing for reordered lists with the same content
        """
        # Convert key_field to list if it's a string
        key_fields = self.key_field if isinstance(self.key_field, list) else [self.key_field]
        
        # Create dictionaries indexed by the key fields
        dict1 = {}
        dict2 = {}
        
        # Function to create compound key from an item
        def get_key(item):
            if not all(k in item for k in key_fields):
                return None  # Skip items without all key fields
            return tuple(str(item.get(k)) for k in key_fields)
        
        # Build dictionaries from lists
        for i, item in enumerate(list1):
            key = get_key(item)
            if key:
                dict1[key] = (i, item)
        
        for i, item in enumerate(list2):
            key = get_key(item)
            if key:
                dict2[key] = (i, item)
        
        # Find keys in the first list that are missing from the second
        for key in set(dict1.keys()) - set(dict2.keys()):
            idx, item = dict1[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            differences.append(Difference(
                position=f"{path}[{idx}] (key: {key_str})",
                expected=item,
                actual=None,
                diff_type="missing_item"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Find keys in the second list that are missing from the first
        for key in set(dict2.keys()) - set(dict1.keys()):
            idx, item = dict2[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            differences.append(Difference(
                position=f"{path}[{idx}] (key: {key_str})",
                expected=None,
                actual=item,
                diff_type="extra_item"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Compare matching items
        for key in set(dict1.keys()) & set(dict2.keys()):
            idx1, item1 = dict1[key]
            idx2, item2 = dict2[key]
            key_str = ".".join(f"{k}={v}" for k, v in zip(key_fields, key))
            new_path = f"{path}[key:{key_str}]"
            
            # Skip identical items
            if item1 == item2:
                continue
                
            # Recursive comparison of matched items
            self._compare_json_key_based(item1, item2, new_path, differences, max_diffs)

================
File: src/cli_test_framework/file_comparator/result.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file result.py
@brief Classes for representing file comparison results and differences
@author Xiaotong Wang
@date 2025
"""

class Difference:
    """
    @brief Represents a single difference between files
    @details This class encapsulates information about a single difference found
             during file comparison, including position, expected and actual content,
             and the type of difference.
    """
    
    def __init__(self, position=None, expected=None, actual=None, diff_type="content"):
        """
        @brief Initialize a Difference object
        @param position: Position of the difference (can be line number, byte position, etc.)
        @param expected: Expected content at the position
        @param actual: Actual content found at the position
        @param diff_type: Type of difference ("content", "missing", "extra", etc.)
        """
        self.position = position  # Can be line number, byte position, etc.
        self.expected = expected  # Expected content
        self.actual = actual      # Actual content
        self.diff_type = diff_type  # Type of difference: "content", "missing", "extra", etc.
    
    def __str__(self):
        """
        @brief Convert the difference to a string representation
        @return str: Human-readable description of the difference
        """
        if self.diff_type == "content":
            return f"At {self.position}: expected '{self.expected}', got '{self.actual}'"
        elif self.diff_type == "missing":
            return f"Missing content at {self.position}: '{self.expected}'"
        elif self.diff_type == "extra":
            return f"Extra content at {self.position}: '{self.actual}'"
        else:
            return f"Difference at {self.position}"
    
    def to_dict(self):
        """
        @brief Convert the difference to a dictionary representation
        @return dict: Dictionary containing the difference details
        """
        return {
            "position": self.position,
            "expected": self.expected,
            "actual": self.actual,
            "diff_type": self.diff_type
        }

class ComparisonResult:
    """
    @brief Represents the result of a file comparison
    @details This class encapsulates all information about a file comparison,
             including file paths, comparison range, differences found,
             and additional metadata like file sizes and similarity index.
    """
    
    def __init__(self, file1=None, file2=None, start_line=0, end_line=None, 
                 start_column=0, end_column=None):
        """
        @brief Initialize a ComparisonResult object
        @param file1: Path to the first file
        @param file2: Path to the second file
        @param start_line: Starting line number for comparison (0-based)
        @param end_line: Ending line number for comparison (0-based, None for end of file)
        @param start_column: Starting column number for comparison (0-based)
        @param end_column: Ending column number for comparison (0-based, None for end of line)
        """
        self.file1 = file1
        self.file2 = file2
        self.file1_size = None
        self.file2_size = None
        self.start_line = start_line
        self.end_line = end_line
        self.start_column = start_column
        self.end_column = end_column
        self.identical = None
        self.differences = []
        self.error = None
        self.similarity = None  # Similarity index for binary comparisons
    
    def __str__(self):
        """
        @brief Convert the comparison result to a string representation
        @return str: Human-readable description of the comparison result
        """
        if self.error:
            return f"Error during comparison: {self.error}"
        lines = []
        if self.identical:
            range_str = self._get_range_str()
            lines.append(f"Files are identical{range_str}.")
        else:
            lines.append(f"Files are different. Found {len(self.differences)} differences:")
            for i, diff in enumerate(self.differences, 1):
                lines.append(f"{i}. {diff}")
            if self.similarity is not None:
                lines.append(f"Similarity Index: {self.similarity:.2f}")
        return "\n".join(lines)
    
    def _get_range_str(self):
        """
        @brief Get a string representation of the comparison range
        @return str: Description of the line and column ranges being compared
        """
        parts = []
        if self.start_line > 0 or self.end_line is not None:
            line_range = f"lines {self.start_line+1}"
            if self.end_line is not None:
                line_range += f"-{self.end_line+1}"
            parts.append(line_range)
            
        if self.start_column > 0 or self.end_column is not None:
            col_range = f"columns {self.start_column+1}"
            if self.end_column is not None:
                col_range += f"-{self.end_column+1}"
            parts.append(col_range)
            
        if parts:
            return " in " + ", ".join(parts)
        return ""
    
    def to_dict(self):
        """
        @brief Convert the comparison result to a dictionary representation
        @return dict: Dictionary containing all comparison details
        """
        return {
            "file1": self.file1,
            "file2": self.file2,
            "file1_size": self.file1_size,
            "file2_size": self.file2_size,
            "range": {
                "start_line": self.start_line,
                "end_line": self.end_line,
                "start_column": self.start_column,
                "end_column": self.end_column
            },
            "identical": self.identical,
            "differences": [diff.to_dict() for diff in self.differences],
            "similarity": self.similarity,
            "error": self.error
        }
    
    def to_html(self):
        """
        @brief Convert the comparison result to HTML format
        @details Generates a complete HTML document with styling for displaying
                 the comparison results in a web browser
        @return str: HTML representation of the comparison result
        """
        if self.error:
            return f"<div class='error'>Error during comparison: {self.error}</div>"
            
        html = ["<html><head><style>",
                "body { font-family: Arial, sans-serif; }",
                ".identical { color: green; }",
                ".different { color: red; }",
                ".diff-item { margin: 10px 0; padding: 5px; border: 1px solid #ccc; }",
                "</style></head><body>"]
        
        if self.identical:
            range_str = self._get_range_str()
            html.append(f"<h2 class='identical'>Files are identical{range_str}.</h2>")
        else:
            html.append(f"<h2 class='different'>Files are different. Found {len(self.differences)} differences:</h2>")
            if self.similarity is not None:
                html.append(f"<p>Similarity Index: {self.similarity:.2f}</p>")
            html.append("<div class='diff-list'>")
            
            for i, diff in enumerate(self.differences, 1):
                html.append(f"<div class='diff-item'>")
                html.append(f"<h3>Difference {i}</h3>")
                html.append(f"<p>Position: {diff.position}</p>")
                html.append(f"<p>Expected: <pre>{diff.expected}</pre></p>")
                html.append(f"<p>Actual: <pre>{diff.actual}</pre></p>")
                html.append(f"<p>Type: {diff.diff_type}</p>")
                html.append("</div>")
                
            html.append("</div>")
            
        html.append("</body></html>")
        return "\n".join(html)

================
File: src/cli_test_framework/file_comparator/text_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file text_comparator.py
@brief Text file comparator implementation with line-by-line comparison
@author Xiaotong Wang
@date 2025
"""

import difflib
from .base_comparator import BaseComparator
from .result import Difference

class TextComparator(BaseComparator):
    """
    @brief Comparator for text files with line-by-line comparison
    @details This class implements text file comparison using Python's difflib
             for detailed difference detection. It supports line and column-based
             range selection for comparison.
    """
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read text content with specified range
        @param file_path Path: Path to the text file to read
        @param start_line int: Starting line number (0-based)
        @param end_line int: Ending line number (0-based, None for end of file)
        @param start_column int: Starting column number (0-based)
        @param end_column int: Ending column number (0-based, None for end of line)
        @return list: List of text lines within the specified range
        @throws ValueError: If line or column ranges are invalid
        @throws UnicodeDecodeError: If file encoding is incorrect
        @throws FileNotFoundError: If file doesn't exist
        @throws IOError: If there are other file reading errors
        """
        try:
            self.logger.debug(f"Reading text file: {file_path}")
            with open(file_path, 'r', encoding=self.encoding) as f:
                lines = f.readlines()
                
            if start_line < 0:
                raise ValueError("Start line cannot be negative")
                
            if end_line is not None:
                if end_line < start_line:
                    raise ValueError("End line cannot be before start line")
                if end_line >= len(lines):
                    self.logger.warning(f"End line {end_line} exceeds file length {len(lines)}, capping at {len(lines)-1}")
                    end_line = len(lines) - 1
            else:
                end_line = len(lines) - 1
                
            if start_line >= len(lines):
                raise ValueError(f"Start line {start_line} is beyond file length {len(lines)}")
                
            selected_lines = lines[start_line:end_line+1]
            
            if start_column < 0:
                raise ValueError("Start column cannot be negative")
                
            if start_column > 0 or end_column is not None:
                self.logger.debug(f"Applying column range: {start_column} to {end_column}")
                processed_lines = []
                for line in selected_lines:
                    if end_column is not None and end_column < start_column:
                        raise ValueError("End column cannot be before start column")
                    # Make sure we don't exceed line length
                    effective_end = end_column
                    if effective_end is not None and effective_end >= len(line):
                        effective_end = len(line) - 1
                    # Apply column range, handle if start_column is beyond line length
                    if start_column >= len(line):
                        processed_lines.append("")
                    else:
                        processed_lines.append(line[start_column:None if effective_end is None else effective_end+1])
                return processed_lines
            
            return selected_lines
            
        except UnicodeDecodeError as e:
            raise ValueError(f"File encoding error for {file_path}. Try specifying a different encoding. Error: {str(e)}")
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")
        except IOError as e:
            raise ValueError(f"Error reading file {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare text content and return detailed differences
        @param content1 list: First list of text lines to compare
        @param content2 list: Second list of text lines to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Uses difflib to generate a detailed comparison of the text content.
                 Returns a tuple containing a boolean indicating if the content is identical
                 and a list of Difference objects describing any differences found.
                 Limits the number of differences reported to 10 to avoid overwhelming output.
        """
        self.logger.debug(f"Comparing text content")
        
        if content1 == content2:
            return True, []
            
        differences = []
        
        # Use difflib for more detailed comparison
        diff = list(difflib.unified_diff(content1, content2, n=0))
        
        # Process the diff output to create structured differences
        line_diffs = []
        for line in diff[2:]:  # Skip the first two header lines
            if line.startswith('@@'):
                continue
            elif line.startswith('-'):
                line_diffs.append(('remove', line[1:]))
            elif line.startswith('+'):
                line_diffs.append(('add', line[1:]))
            else:
                line_diffs.append(('context', line[1:]))
        
        # Convert diff to our difference format
        line_num1 = 0
        line_num2 = 0
        for i, (action, line) in enumerate(line_diffs):
            if action == 'remove':
                # Look ahead for a corresponding 'add'
                add_match = None
                for j in range(i+1, min(i+5, len(line_diffs))):
                    if line_diffs[j][0] == 'add':
                        add_match = line_diffs[j][1]
                        del line_diffs[j]
                        break
                
                if add_match is not None:
                    # Content difference
                    differences.append(Difference(
                        position=f"line {line_num1+1}",
                        expected=line,
                        actual=add_match,
                        diff_type="content"
                    ))
                else:
                    # Missing line
                    differences.append(Difference(
                        position=f"line {line_num1+1}",
                        expected=line,
                        actual=None,
                        diff_type="missing"
                    ))
                line_num1 += 1
                
            elif action == 'add':
                # Extra line
                differences.append(Difference(
                    position=f"line {line_num2+1}",
                    expected=None,
                    actual=line,
                    diff_type="extra"
                ))
                line_num2 += 1
                
            elif action == 'context':
                line_num1 += 1
                line_num2 += 1
        
        # Limit the number of differences reported
        max_diffs = 10
        if len(differences) > max_diffs:
            differences = differences[:max_diffs]
            differences.append(Difference(
                position=None,
                expected=None,
                actual=None,
                diff_type=f"more differences not shown (total: {len(differences)})"
            ))
            
        return False, differences

================
File: src/cli_test_framework/file_comparator/xml_comparator.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@file xml_comparator.py
@brief XML file comparator implementation with structural comparison
@author Xiaotong Wang
@date 2025
"""

import xml.etree.ElementTree as ET
from .text_comparator import TextComparator
from .result import Difference

class XmlComparator(TextComparator):
    """
    @brief Comparator for XML files with structural comparison
    @details This class extends TextComparator to provide specialized XML comparison
             capabilities, including:
             - Tag comparison
             - Attribute comparison
             - Text content comparison
             - Child element comparison
    """
    
    def read_content(self, file_path, start_line=0, end_line=None, start_column=0, end_column=None):
        """
        @brief Read and parse XML content from file
        @param file_path Path: Path to the XML file
        @param start_line int: Starting line number
        @param end_line int: Ending line number
        @param start_column int: Starting column number
        @param end_column int: Ending column number
        @return ET.Element: Parsed XML element tree
        @throws ValueError: If XML is invalid
        """
        # First read the file as text
        text_content = super().read_content(file_path, start_line, end_line, start_column, end_column)
        
        # Join the lines
        xml_text = ''.join(text_content)
        try:
            return ET.fromstring(xml_text)
        except ET.ParseError as e:
            raise ValueError(f"Invalid XML in {file_path}: {str(e)}")
    
    def compare_content(self, content1, content2):
        """
        @brief Compare XML content structurally
        @param content1 ET.Element: First XML element to compare
        @param content2 ET.Element: Second XML element to compare
        @return tuple: (bool, list) - (identical, differences)
        @details Performs structural comparison of XML elements, including tags,
                 attributes, text content, and child elements
        """
        # Convert back to strings for comparison
        xml_str1 = ET.tostring(content1, encoding='unicode')
        xml_str2 = ET.tostring(content2, encoding='unicode')
        
        if xml_str1 == xml_str2:
            return True, []
            
        # Use a recursive function to find differences in XML structures
        differences = []
        self._compare_elements(content1, content2, "", differences)
        
        return False, differences
    
    def _compare_elements(self, elem1, elem2, path, differences, max_diffs=10):
        """
        @brief Recursively compare XML elements and collect differences
        @param elem1 ET.Element: First XML element to compare
        @param elem2 ET.Element: Second XML element to compare
        @param path str: Current path in the XML structure
        @param differences list: List to store found differences
        @param max_diffs int: Maximum number of differences to report
        @details Compares XML elements recursively, checking for:
                 - Tag mismatches
                 - Missing or extra attributes
                 - Text content differences
                 - Child element count mismatches
                 - Child element differences
        """
        if len(differences) >= max_diffs:
            return
            
        # Compare tags
        if elem1.tag != elem2.tag:
            differences.append(Difference(
                position=path or "/",
                expected=elem1.tag,
                actual=elem2.tag,
                diff_type="tag_mismatch"
            ))
            return  # If tags don't match, don't compare further
            
        # Compare attributes
        attrib1 = set(elem1.attrib.items())
        attrib2 = set(elem2.attrib.items())
        
        for attr, value in attrib1 - attrib2:
            differences.append(Difference(
                position=f"{path}/@{attr}" if path else f"/@{attr}",
                expected=value,
                actual="missing attribute",
                diff_type="missing_attribute"
            ))
            if len(differences) >= max_diffs:
                return
                
        for attr, value in attrib2 - attrib1:
            differences.append(Difference(
                position=f"{path}/@{attr}" if path else f"/@{attr}",
                expected="missing attribute",
                actual=value,
                diff_type="extra_attribute"
            ))
            if len(differences) >= max_diffs:
                return
        
        # Compare text content if leaf nodes
        if len(elem1) == 0 and len(elem2) == 0:
            text1 = elem1.text.strip() if elem1.text else ""
            text2 = elem2.text.strip() if elem2.text else ""
            
            if text1 != text2:
                differences.append(Difference(
                    position=path or "/",
                    expected=text1,
                    actual=text2,
                    diff_type="text_mismatch"
                ))
                return
                
        # Compare children elements
        children1 = list(elem1)
        children2 = list(elem2)
        
        if len(children1) != len(children2):
            differences.append(Difference(
                position=path or "/",
                expected=f"{len(children1)} child elements",
                actual=f"{len(children2)} child elements",
                diff_type="children_count_mismatch"
            ))
            
        # Compare matching children
        for i, (child1, child2) in enumerate(zip(children1, children2)):
            new_path = f"{path}/{child1.tag}[{i}]" if path else f"/{child1.tag}[{i}]"
            self._compare_elements(child1, child2, new_path, differences, max_diffs)

================
File: src/cli_test_framework/runners/__init__.py
================
"""
Test runners for the CLI Testing Framework
"""

from .json_runner import JSONRunner
from .parallel_json_runner import ParallelJSONRunner
from .yaml_runner import YAMLRunner

__all__ = [
    'JSONRunner',
    'ParallelJSONRunner',
    'YAMLRunner'
]

================
File: src/cli_test_framework/runners/json_runner.py
================
from typing import Optional
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
from typing import Dict, Any

class JSONRunner(BaseRunner):
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self) -> None:
        """Load test cases from a JSON file."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                # 使用智能命令解析，正确处理包含空格的路径
                case["command"] = self.path_resolver.parse_command_string(case["command"])
                
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
            # print(self.test_cases)
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",  # 添加命令输出字段
            "command": "",  # 添加执行的命令字段
            "return_code": None  # 添加返回码字段
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command  # 保存执行的命令
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output  # 保存命令的完整输出
            result["return_code"] = process.returncode  # 保存返回码
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/runners/parallel_json_runner.py
================
from typing import Optional, Dict, Any
from ..core.parallel_runner import ParallelRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
import threading

class ParallelJSONRunner(ParallelRunner):
    """并行JSON测试运行器"""
    
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None,
                 max_workers: Optional[int] = None, execution_mode: str = "thread"):
        """
        初始化并行JSON运行器
        
        Args:
            config_file: JSON配置文件路径
            workspace: 工作目录
            max_workers: 最大并发数
            execution_mode: 执行模式，'thread' 或 'process'
        """
        super().__init__(config_file, workspace, max_workers, execution_mode)
        self.path_resolver = PathResolver(self.workspace)
        self._print_lock = threading.Lock()  # 用于控制输出顺序

    def load_test_cases(self) -> None:
        """从JSON文件加载测试用例"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.parse_command_string(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """运行单个测试用例（线程安全版本）"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            
            # 线程安全的输出
            with self._print_lock:
                print(f"  [Worker] Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            # 线程安全的输出
            if output.strip():
                with self._print_lock:
                    print(f"  [Worker] Command output for {case.name}:")
                    for line in output.splitlines():
                        print(f"    {line}")

            # 检查返回码
            if "return_code" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # 检查输出包含
            if "output_contains" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output contains for {case.name}...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # 检查正则匹配
            if "output_matches" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output matches regex for {case.name}...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/runners/yaml_runner.py
================
from typing import Optional, Dict, Any
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import subprocess
import sys

class YAMLRunner(BaseRunner):
    def __init__(self, config_file="test_cases.yaml", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self):
        """Load test cases from a YAML file."""
        try:
            import yaml
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")
                
                case["command"] = self.path_resolver.parse_command_string(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))
                
            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """Run a single test case and return the result"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/cli_test_framework/utils/__init__.py
================
# File: /python-test-framework/python-test-framework/src/utils/__init__.py

"""
Utility functions for the CLI Testing Framework
"""

from .path_resolver import PathResolver

__all__ = [
    'PathResolver'
]

================
File: src/cli_test_framework/utils/path_resolver.py
================
from pathlib import Path
from typing import List
import shlex
import os
import shutil

class PathResolver:
    def __init__(self, workspace: Path):
        self.workspace = workspace

    def resolve_paths(self, args: List[str]) -> List[str]:
        resolved_args = []
        for arg in args:
            if not arg.startswith("--"):
                # Only prepend workspace if the path is relative
                if not Path(arg).is_absolute():
                    resolved_args.append(str(self.workspace / arg))
                else:
                    resolved_args.append(arg)
            else:
                resolved_args.append(arg)
        return resolved_args

    def resolve_command(self, command: str) -> str:
        """
        解析命令路径
        - 系统命令（如echo, ping, dir等）保持原样
        - 已安装的命令（在PATH中可找到）保持原样
        - 相对路径的可执行文件转换为绝对路径
        """
        # 如果是绝对路径，保持原样
        if Path(command).is_absolute():
            return command
        
        # 检查命令是否在系统PATH中
        if shutil.which(command) is not None:
            return command
        
        # 常见的系统命令列表（作为备用，以防shutil.which在某些情况下失效）
        system_commands = {
            'echo', 'ping', 'dir', 'ls', 'cat', 'grep', 'find', 'sort', 
            'head', 'tail', 'wc', 'curl', 'wget', 'git', 'python', 'node',
            'npm', 'pip', 'java', 'javac', 'gcc', 'make', 'cmake', 'docker',
            'kubectl', 'helm', 'terraform', 'ansible', 'ssh', 'scp', 'rsync'
        }
        
        # 如果在系统命令列表中，保持原样
        if command in system_commands:
            return command
        
        # 否则当作相对路径处理
        return str(self.workspace / command)

    def parse_command_string(self, command_string: str) -> str:
        """
        智能解析命令字符串，正确处理包含空格的路径
        
        Args:
            command_string: 原始命令字符串，如 "python ./script.py" 或 r"C:\\Program Files (x86)\\python.exe script.py"
            
        Returns:
            解析后的完整命令字符串
        """
        # 特殊处理：如果命令字符串包含引号，使用shlex解析
        if '"' in command_string or "'" in command_string:
            try:
                # 对于所有系统都使用posix=True来正确处理引号
                # 这样可以去掉引号，但保留路径内容
                parts = shlex.split(command_string, posix=True)
                
                if not parts:
                    return command_string
                
                # 第一部分是命令，其余是参数
                command_part = parts[0]
                remaining_parts = parts[1:]
                
                # 解析命令部分（如果是绝对路径，保持原样；否则解析）
                if Path(command_part).is_absolute():
                    resolved_command = command_part
                else:
                    resolved_command = self.resolve_command(command_part)
                
                # 解析参数部分
                resolved_parts = []
                for part in remaining_parts:
                    if part.startswith('-'):
                        # 选项参数，保持原样
                        resolved_parts.append(part)
                    elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                        # 看起来像文件路径
                        if not Path(part).is_absolute():
                            resolved_parts.append(str(self.workspace / part))
                        else:
                            resolved_parts.append(part)
                    else:
                        # 其他参数，保持原样
                        resolved_parts.append(part)
                
                return f"{resolved_command} {' '.join(resolved_parts)}"
                
            except ValueError:
                # shlex解析失败，回退到简单处理
                pass
        
        # 简单情况：没有引号的命令字符串
        # 先尝试识别是否以绝对路径开头
        if self._starts_with_absolute_path(command_string):
            # 处理以绝对路径开头的命令
            return self._parse_absolute_path_command(command_string)
        else:
            # 普通命令处理
            parts = command_string.split()
            if not parts:
                return command_string
            
            if len(parts) == 1:
                return self.resolve_command(parts[0])
            else:
                command_part = parts[0]
                remaining_parts = parts[1:]
                
                resolved_command = self.resolve_command(command_part)
                resolved_parts = []
                
                for part in remaining_parts:
                    if part.startswith('-'):
                        resolved_parts.append(part)
                    elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                        if not Path(part).is_absolute():
                            resolved_parts.append(str(self.workspace / part))
                        else:
                            resolved_parts.append(part)
                    else:
                        resolved_parts.append(part)
                
                return f"{resolved_command} {' '.join(resolved_parts)}"
    
    def _starts_with_absolute_path(self, command_string: str) -> bool:
        """检查命令字符串是否以绝对路径开头"""
        if os.name == 'nt':  # Windows
            # Windows绝对路径模式：C:\... 或 \\server\...
            return (len(command_string) >= 3 and 
                    command_string[1:3] == ':\\') or command_string.startswith('\\\\')
        else:  # Unix/Linux
            return command_string.startswith('/')
    
    def _parse_absolute_path_command(self, command_string: str) -> str:
        """解析以绝对路径开头的命令字符串"""
        # 对于Windows路径，需要特殊处理空格
        if os.name == 'nt':
            # 尝试找到第一个.exe或.bat等可执行文件扩展名
            exe_extensions = ['.exe', '.bat', '.cmd', '.com']
            
            for ext in exe_extensions:
                if ext in command_string:
                    # 找到可执行文件的结束位置
                    ext_pos = command_string.find(ext)
                    if ext_pos != -1:
                        command_end = ext_pos + len(ext)
                        command_part = command_string[:command_end]
                        remaining = command_string[command_end:].strip()
                        
                        if remaining:
                            # 解析剩余参数
                            remaining_parts = remaining.split()
                            resolved_parts = []
                            
                            for part in remaining_parts:
                                if part.startswith('-'):
                                    resolved_parts.append(part)
                                elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                                    if not Path(part).is_absolute():
                                        resolved_parts.append(str(self.workspace / part))
                                    else:
                                        resolved_parts.append(part)
                                else:
                                    resolved_parts.append(part)
                            
                            return f"{command_part} {' '.join(resolved_parts)}"
                        else:
                            return command_part
        
        # 如果没有找到可执行文件扩展名，回退到简单分割
        parts = command_string.split()
        if not parts:
            return command_string
        
        # 假设第一个部分是命令
        command_part = parts[0]
        remaining_parts = parts[1:]
        
        resolved_parts = []
        for part in remaining_parts:
            if part.startswith('-'):
                resolved_parts.append(part)
            elif ('.' in part or '/' in part or '\\' in part) and not part.isdigit():
                if not Path(part).is_absolute():
                    resolved_parts.append(str(self.workspace / part))
                else:
                    resolved_parts.append(part)
            else:
                resolved_parts.append(part)
        
        return f"{command_part} {' '.join(resolved_parts)}"

================
File: src/cli_test_framework/utils/report_generator.py
================
class ReportGenerator:
    def __init__(self, results: dict, file_path: str):
        self.results = results
        self.file_path = file_path

    def generate_report(self) -> str:
        report = "Test Results Summary:\n"
        report += f"Total Tests: {self.results['total']}\n"
        report += f"Passed: {self.results['passed']}\n"
        report += f"Failed: {self.results['failed']}\n\n"
        
        report += "Detailed Results:\n"
        for detail in self.results['details']:
            status_icon = "✓" if detail['status'] == 'passed' else "✗"
            report += f"{status_icon} {detail['name']}\n"
            if detail.get('message'):
                report += f"   -> {detail['message']}\n"
        
        # 添加失败案例的详细输出信息
        failed_tests = [detail for detail in self.results['details'] if detail['status'] == 'failed']
        if failed_tests:
            report += "\n" + "="*50 + "\n"
            report += "FAILED TEST CASES DETAILS:\n"
            report += "="*50 + "\n\n"
            
            for i, failed_test in enumerate(failed_tests, 1):
                report += f"{i}. Test: {failed_test['name']}\n"
                report += "-" * 40 + "\n"
                
                # 添加执行的命令
                if failed_test.get('command'):
                    report += f"Command: {failed_test['command']}\n"
                
                # 添加返回码
                if failed_test.get('return_code') is not None:
                    report += f"Return Code: {failed_test['return_code']}\n"
                
                # 添加失败原因
                if failed_test.get('message'):
                    report += f"Error Message: {failed_test['message']}\n"
                
                # 添加命令的完整输出（这是最重要的部分）
                if failed_test.get('output'):
                    report += f"\nCommand Output:\n"
                    report += "=" * 30 + "\n"
                    report += f"{failed_test['output']}\n"
                    report += "=" * 30 + "\n"
                
                # 添加错误堆栈信息（如果有的话）
                if failed_test.get('error_trace'):
                    report += f"Error Trace:\n{failed_test['error_trace']}\n"
                
                # 添加执行时间（如果有的话）
                if failed_test.get('duration'):
                    report += f"Duration: {failed_test['duration']}s\n"
                
                report += "\n"
        
        return report

    def save_report(self) -> None:
        report = self.generate_report()
        with open(self.file_path, 'w', encoding='utf-8') as f:
            f.write(report)

    def print_report(self) -> None:
        report = self.generate_report()
        print(report)

================
File: tests/__init__.py
================
# File: /python-test-framework/python-test-framework/tests/__init__.py

# This file is intentionally left blank.

================
File: tests/fixtures/test_cases.json
================
{
  "test_cases": [
    {
      "name": "测试Python版本",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    },
    {
      "name": "测试目录列表",
      "command": "dir",
      "args": ["."],
      "expected": {
        "return_code": 0,
        "output_contains": ["src"]
      }
    },
    {
      "name": "测试echo命令",
      "command": "echo",
      "args": ["Hello World"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Hello World"]
      }
    },
    {
      "name": "测试ping本地回环",
      "command": "ping",
      "args": ["-n", "1", "127.0.0.1"],
      "expected": {
        "return_code": 0,
        "output_contains": ["127.0.0.1"]
      }
    },
    {
      "name": "测试时间命令",
      "command": "echo",
      "args": ["%time%"],
      "expected": {
        "return_code": 0
      }
    },
    {
      "name": "测试文件存在性",
      "command": "dir",
      "args": ["src"],
      "expected": {
        "return_code": 0,
        "output_contains": ["core", "runners"]
      }
    }
  ]
}

================
File: tests/fixtures/test_cases.yaml
================
name: Sample Test Case
command: python script.py
args:
  - --input
  - input.txt
expected:
  return_code: 0
  output_contains:
    - "Success"
    - "Processed"

================
File: tests/fixtures/test_cases1.json
================
{
    "test_cases": [
        {
            "name": "text_identical_default",
            "description": "默认文本比较（相同文件）",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "text_different_range",
            "description": "带行范围限制的文本比较",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf", 
                "./test/1_copy.bdf",
                "--start-line=93",
                "--end-line=96",
                "--start-column=1",
                "--end-column=30"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_exact_match",
            "description": "严格JSON比较（相同文件）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/1_copy.json",
                "--file-type=json",
                "--json-compare-mode=exact"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_key_based_match",
            "description": "JSON键值比较（结构不同但关键字段匹配）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--file-type=json",
                "--json-compare-mode=key-based",
                "--json-key-field=\"phone\""
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "binary_comparison",
            "description": "二进制文件比较（带相似度计算）",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/2.bdf",
                "--file-type=binary",
                "--similarity",
                "--chunk-size=4096"
            ],
            "expected": {
                "output_contains": ["Similarity Index"]
                
            }
        },
        {
            "name": "multi_thread_comparison",
            "description": "多线程文本比较",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/1_copy.bdf",
                "--num-threads=8",
                "--verbose"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "different_output_format",
            "description": "JSON输出格式测试",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--output-format=json"
            ],
            "expected": {
                "output_contains": ["\"position\""]
                
            }
        },
        {
            "name": "auto_detect_filetype",
            "description": "自动文件类型检测",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Auto-detected file type: text"]
                
            }
        },
        {
            "name": "h5_comparison",
            "description": "HDF5文件比较特定表格",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODAL/DISPLACEMENT"],
            "expected": {
                "output_contains": ["Files are different"]
                
            }
        },
        {
            "name": "h5_comparison_all_tables",
            "description": "HDF5文件比较所有表格",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5"],
            "expected": {
                "output_contains": ["Files are different."]
                
            }
        },
        {
            "name": "h5_comparison_with_wrong_table",
            "description": "HDF5文件比较，错误表格路径",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODALl/DISPLACEMENT"],
            "expected": {
                "matches": ["WARNING - Table .* not found"]                
            }
        },
        {
            "name": "h5_comparison_with_strucutre_mode",
            "description": "HDF5文件比较，结构模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-structure-only"],
            "expected": {
                "output_contains": ["Files are identical."]                
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5文件比较，内容模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-show-content-diff"],
            "expected": {
                "output_contains": ["Difference at"]
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5文件比较，内容模式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-rtol=1e-5", "--h5-atol=1e-8"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        },
        {
            "name": "h5_comparison_with_table_regex",
            "description": "HDF5文件比较，表格正则表达式",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-table-regex=NASTRAN/RESULT/\\b\\w*al\\b/STRESS"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        }
    ]
}

================
File: tests/performance_test.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
并行测试性能验证脚本
快速验证并行测试功能和性能提升
"""

import sys
import time
import json
import tempfile
import os
from pathlib import Path

# 添加src目录到Python路径
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.runners.json_runner import JSONRunner
from src.runners.parallel_json_runner import ParallelJSONRunner

def create_test_config(num_tests=10):
    """创建测试配置文件"""
    test_cases = []
    
    for i in range(num_tests):
        test_cases.append({
            "name": f"测试用例 {i+1}",
            "command": "echo",
            "args": [f"test_{i+1}"],
            "expected": {
                "return_code": 0,
                "output_contains": [f"test_{i+1}"]
            }
        })
    
    return {"test_cases": test_cases}

def run_performance_test():
    """运行性能测试"""
    print("=" * 60)
    print("并行测试框架性能验证")
    print("=" * 60)
    
    # 创建临时测试配置
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "perf_test.json")
    
    # 创建测试用例（可以调整数量）
    num_tests = 8
    config = create_test_config(num_tests)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"创建了 {num_tests} 个测试用例")
    print(f"测试配置文件: {config_file}")
    
    results = {}
    
    # 1. 顺序执行测试
    print(f"\n1. 顺序执行测试...")
    start_time = time.time()
    sequential_runner = JSONRunner(config_file, temp_dir)
    seq_success = sequential_runner.run_tests()
    seq_time = time.time() - start_time
    results['sequential'] = {'time': seq_time, 'success': seq_success}
    
    # 2. 并行执行测试（线程模式）
    print(f"\n2. 并行执行测试（线程模式，4个工作线程）...")
    start_time = time.time()
    parallel_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=4, 
        execution_mode="thread"
    )
    par_success = parallel_runner.run_tests()
    par_time = time.time() - start_time
    results['parallel_thread'] = {'time': par_time, 'success': par_success}
    
    # 3. 并行执行测试（进程模式）
    print(f"\n3. 并行执行测试（进程模式，2个工作进程）...")
    start_time = time.time()
    process_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=2, 
        execution_mode="process"
    )
    proc_success = process_runner.run_tests()
    proc_time = time.time() - start_time
    results['parallel_process'] = {'time': proc_time, 'success': proc_success}
    
    # 性能分析
    print("\n" + "=" * 60)
    print("性能分析结果:")
    print("=" * 60)
    
    print(f"测试用例数量:      {num_tests}")
    print(f"顺序执行时间:      {seq_time:.2f} 秒")
    print(f"并行执行(线程):    {par_time:.2f} 秒 (加速比: {seq_time/par_time:.2f}x)")
    print(f"并行执行(进程):    {proc_time:.2f} 秒 (加速比: {seq_time/proc_time:.2f}x)")
    
    # 验证结果一致性
    print(f"\n结果验证:")
    print(f"顺序执行成功:      {seq_success}")
    print(f"并行执行(线程)成功: {par_success}")
    print(f"并行执行(进程)成功: {proc_success}")
    
    # 清理临时文件
    import shutil
    shutil.rmtree(temp_dir)
    
    # 总结
    if all(results[key]['success'] for key in results):
        print(f"\n✓ 所有测试模式都成功执行")
        if par_time < seq_time:
            print(f"✓ 并行执行确实提升了性能")
        else:
            print(f"⚠ 在当前测试规模下，并行优势不明显")
    else:
        print(f"\n✗ 部分测试模式执行失败")
        return False
    
    return True

if __name__ == "__main__":
    try:
        success = run_performance_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n测试被用户中断")
        sys.exit(1)
    except Exception as e:
        print(f"\n测试执行出错: {e}")
        sys.exit(1)

================
File: tests/test_comprehensive_space.py
================
#!/usr/bin/env python3
"""
全面测试包含空格的路径解析功能
"""

import os
import sys
import tempfile
import json
from pathlib import Path

# 添加src目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.runners.json_runner import JSONRunner

def test_comprehensive_space_handling():
    """全面测试包含空格的路径处理"""
    print("=" * 60)
    print("全面测试包含空格的路径解析功能")
    print("=" * 60)
    
    # 创建临时目录和测试文件
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "comprehensive_test.json")
    
    # 创建测试配置，包含各种可能的空格路径场景
    test_config = {
        "test_cases": [
            {
                "name": "简单命令",
                "command": "echo hello",
                "args": [],
                "expected": {
                    "return_code": 0,
                    "output_contains": ["hello"]
                }
            },
            {
                "name": "带引号的Windows路径",
                "command": '"C:\\Program Files (x86)\\Python\\python.exe" --version',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "不带引号的Windows路径",
                "command": "C:\\Program Files (x86)\\Python\\python.exe --version",
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "相对路径脚本",
                "command": "python script.py",
                "args": ["--verbose"],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "复杂命令带参数",
                "command": "node app.js",
                "args": ["--port", "3000", "--env", "development"],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "带空格的Unix路径",
                "command": '"/usr/local/bin/my app" --help',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            }
        ]
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    print(f"测试配置文件: {config_file}")
    print(f"临时目录: {temp_dir}")
    print()
    
    # 测试加载配置
    try:
        runner = JSONRunner(config_file, temp_dir)
        runner.load_test_cases()
        
        print(f"成功加载 {len(runner.test_cases)} 个测试用例:")
        print()
        
        for i, case in enumerate(runner.test_cases, 1):
            print(f"{i}. {case.name}")
            print(f"   原始命令: {test_config['test_cases'][i-1]['command']}")
            print(f"   解析后命令: {case.command}")
            print(f"   参数: {case.args}")
            print()
        
        print("=" * 60)
        print("命令解析测试完成！")
        print("所有包含空格的路径都已正确解析。")
        
    except Exception as e:
        print(f"测试失败: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # 清理
    import shutil
    shutil.rmtree(temp_dir)

if __name__ == "__main__":
    test_comprehensive_space_handling()

================
File: tests/test_parallel_runner.py
================
import unittest
import tempfile
import json
import os
import sys
import time
from pathlib import Path

# 添加src目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner

class TestParallelRunner(unittest.TestCase):
    """并行运行器测试类"""
    
    def setUp(self):
        """设置测试环境"""
        self.temp_dir = tempfile.mkdtemp()
        self.config_file = os.path.join(self.temp_dir, "test_config.json")
        
        # 创建测试配置
        test_config = {
            "test_cases": [
                {
                    "name": "测试1",
                    "command": "echo",
                    "args": ["test1"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test1"]
                    }
                },
                {
                    "name": "测试2",
                    "command": "echo",
                    "args": ["test2"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test2"]
                    }
                },
                {
                    "name": "测试3",
                    "command": "echo",
                    "args": ["test3"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test3"]
                    }
                },
                {
                    "name": "测试4",
                    "command": "echo",
                    "args": ["test4"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test4"]
                    }
                }
            ]
        }
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    def tearDown(self):
        """清理测试环境"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def test_parallel_vs_sequential_performance(self):
        """测试并行执行相比顺序执行的性能提升"""
        # 顺序执行
        sequential_runner = JSONRunner(self.config_file, self.temp_dir)
        start_time = time.time()
        seq_success = sequential_runner.run_tests()
        seq_time = time.time() - start_time
        
        # 并行执行
        parallel_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        start_time = time.time()
        par_success = parallel_runner.run_tests()
        par_time = time.time() - start_time
        
        # 验证结果
        self.assertTrue(seq_success, "顺序执行应该成功")
        self.assertTrue(par_success, "并行执行应该成功")
        self.assertEqual(
            sequential_runner.results["total"], 
            parallel_runner.results["total"],
            "测试总数应该相同"
        )
        self.assertEqual(
            sequential_runner.results["passed"], 
            parallel_runner.results["passed"],
            "通过的测试数应该相同"
        )
        
        print(f"\n性能比较:")
        print(f"顺序执行时间: {seq_time:.3f}秒")
        print(f"并行执行时间: {par_time:.3f}秒")
        if par_time > 0:
            print(f"加速比: {seq_time/par_time:.2f}x")
    
    def test_thread_vs_process_mode(self):
        """测试线程模式和进程模式"""
        # 线程模式
        thread_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        thread_success = thread_runner.run_tests()
        
        # 进程模式
        process_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="process"
        )
        process_success = process_runner.run_tests()
        
        # 验证结果
        self.assertTrue(thread_success, "线程模式应该成功")
        self.assertTrue(process_success, "进程模式应该成功")
        self.assertEqual(
            thread_runner.results["passed"], 
            process_runner.results["passed"],
            "两种模式的通过测试数应该相同"
        )
    
    def test_max_workers_configuration(self):
        """测试不同的最大工作线程数配置"""
        for max_workers in [1, 2, 4]:
            with self.subTest(max_workers=max_workers):
                runner = ParallelJSONRunner(
                    self.config_file, 
                    self.temp_dir, 
                    max_workers=max_workers, 
                    execution_mode="thread"
                )
                success = runner.run_tests()
                self.assertTrue(success, f"max_workers={max_workers}时应该成功")
                self.assertEqual(runner.results["passed"], 4, "应该通过4个测试")
    
    def test_fallback_to_sequential(self):
        """测试回退到顺序执行"""
        runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        
        # 测试回退功能
        success = runner.run_tests_sequential()
        self.assertTrue(success, "回退到顺序执行应该成功")
        self.assertEqual(runner.results["passed"], 4, "应该通过4个测试")

if __name__ == '__main__':
    unittest.main()

================
File: tests/test_parallel_space.py
================
#!/usr/bin/env python3
"""
测试并行运行器的空格路径处理
"""

import os
import sys
import tempfile
import json

# 添加src目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.runners.parallel_json_runner import ParallelJSONRunner

def test_parallel_space_handling():
    """测试并行运行器的空格路径处理"""
    print("=" * 60)
    print("测试并行运行器的空格路径处理")
    print("=" * 60)
    
    # 创建临时目录和测试文件
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "parallel_space_test.json")
    
    # 创建测试配置
    test_config = {
        "test_cases": [
            {
                "name": "简单命令",
                "command": "echo hello",
                "args": [],
                "expected": {
                    "return_code": 0,
                    "output_contains": ["hello"]
                }
            },
            {
                "name": "带引号的路径",
                "command": '"C:\\Program Files (x86)\\Python\\python.exe" --version',
                "args": [],
                "expected": {
                    "return_code": 0
                }
            },
            {
                "name": "不带引号的路径",
                "command": "C:\\Program Files (x86)\\Python\\python.exe --version",
                "args": [],
                "expected": {
                    "return_code": 0
                }
            }
        ]
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    print(f"测试配置文件: {config_file}")
    print(f"临时目录: {temp_dir}")
    print()
    
    # 测试并行运行器
    try:
        runner = ParallelJSONRunner(
            config_file, 
            temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        runner.load_test_cases()
        
        print(f"成功加载 {len(runner.test_cases)} 个测试用例:")
        for i, case in enumerate(runner.test_cases, 1):
            print(f"{i}. {case.name}")
            print(f"   解析后命令: {case.command}")
            print(f"   参数: {case.args}")
        
        print("\n" + "=" * 60)
        print("并行运行器空格路径解析测试完成！")
        
    except Exception as e:
        print(f"测试失败: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # 清理
    import shutil
    shutil.rmtree(temp_dir)

if __name__ == "__main__":
    test_parallel_space_handling()

================
File: tests/test_report.txt
================
Test Results Summary:
Total Tests: 15
Passed: 15
Failed: 0

Detailed Results:
✓ text_identical_default
✓ text_different_range
✓ json_exact_match
✓ json_key_based_match
✓ binary_comparison
✓ multi_thread_comparison
✓ different_output_format
✓ auto_detect_filetype
✓ h5_comparison
✓ h5_comparison_all_tables
✓ h5_comparison_with_wrong_table
✓ h5_comparison_with_strucutre_mode
✓ h5_comparison_with_content_mode
✓ h5_comparison_with_content_mode
✓ h5_comparison_with_table_regex

================
File: tests/test_runners.py
================
import unittest
from src.runners.json_runner import JSONRunner
from src.runners.yaml_runner import YAMLRunner

class TestJSONRunner(unittest.TestCase):
    def setUp(self):
        self.runner = JSONRunner("tests/fixtures/test_cases.json")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

class TestYAMLRunner(unittest.TestCase):
    def setUp(self):
        self.runner = YAMLRunner("tests/fixtures/test_cases.yaml")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

if __name__ == "__main__":
    unittest.main()

================
File: tests/test1.py
================
import sys
import os
# 添加项目根目录到Python路径
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import unittest
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator

def main():
    runner = JSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases1.json",
        workspace="D:/Document/xcode/Compare-File-Tool"
    )
    success = runner.run_tests()
    
    # 生成报告
    report_generator = ReportGenerator(
        runner.results, 
        "D:/Document/xcode/cli-test-framework/tests/test_report.txt"
    )
    report_generator.print_report()  # 打印到控制台
    report_generator.save_report()   # 保存到文件
    
    print(f"\n报告已保存到: D:/Document/xcode/cli-test-framework/tests/test_report.txt")
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: User_Manual.md
================
# 📘 CLI-Test-Framework 使用手册

本框架是一个用于命令行工具的自动化测试框架，支持 **JSON/YAML 测试配置文件**、**顺序执行**与**并行执行（线程/进程）**，可自动比对输出、返回码，并生成测试报告。

------

## ✅ 安装方式

### 从 PyPI 安装：

```bash
pip install cli-test-framework
```

------

## 📂 项目结构推荐

```bash
your_project/
├── test_cases.json          # 测试用例配置文件
├── test_report.txt          # 测试报告（可选输出）
└── run_tests.py             # 测试执行脚本
```

------

## 🧪 示例测试用例（JSON 格式）

```json
{
  "test_cases": [
    {
      "name": "版本检查测试",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    }
  ]
}
```

------

## 🚀 快速使用示例

### 顺序执行测试

```python
from cli_test_framework.runners import JSONRunner

runner = JSONRunner(config_file="test_cases.json", workspace=".")
runner.run_tests()
```

### 并行执行测试（线程模式）

```python
from cli_test_framework.runners import ParallelJSONRunner

runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,
    execution_mode="thread"  # 可为 "thread" 或 "process"
)
runner.run_tests()
```

------

## 📄 生成测试报告

```python
from cli_test_framework.utils import ReportGenerator

report = ReportGenerator(runner.results, "test_report.txt")
report.print_report()  # 打印至终端
report.save_report()   # 保存至文件
```

------

## ⚙️ 支持的字段说明

| 字段              | 类型      | 说明                               |
| ----------------- | --------- | ---------------------------------- |
| `name`            | str       | 测试名称                           |
| `command`         | str       | 要执行的命令（可为系统命令或脚本） |
| `args`            | List[str] | 命令参数                           |
| `expected`        | dict      | 预期结果                           |
| `return_code`     | int       | 预期返回值（可选）                 |
| `output_contains` | List[str] | 输出中必须包含的内容（可选）       |
| `output_matches`  | str       | 输出需匹配的正则表达式（可选）     |



------

## 🧠 并行执行说明

### 执行模式选项：

| 模式      | 说明                              | 适用场景        |
| --------- | --------------------------------- | --------------- |
| `thread`  | 多线程，适合 I/O 密集型测试       | 网络/文件操作   |
| `process` | 多进程，适合 CPU 密集型、隔离需求 | 重计算/崩溃测试 |



### 设置最大并发数：

```python
import os
max_workers = os.cpu_count() * 2  # 推荐值
```

------

## 📦 高级用法

- 支持 YAML 测试文件：使用 `YAMLRunner`
- 自定义断言模块：继承 `Assertions` 类添加新规则
- 自定义测试格式：继承 `BaseRunner`

------

## 🛠 常见问题排查

| 问题                     | 可能原因                                    |
| ------------------------ | ------------------------------------------- |
| 命令未执行成功           | command 路径错误 / 环境未激活               |
| output_contains 断言失败 | 输出为 stderr 而非 stdout                   |
| 并行模式下报错           | 可能为 `args` 或 `command` 含不可序列化对象 |



------

## 🧪 快速性能对比（可选）

运行内置性能测试脚本：

```bash
python tests/performance_test.py
```

输出示例：

```makefile
顺序执行时间:      2.34 秒
并行执行(线程):    0.88 秒 (加速比: 2.66x)
```

------

## 📎 附加说明

- **支持平台**：Windows / Linux / macOS
- **Python 版本**：3.6+
- **依赖项**：
  - `PyYAML`（仅用于 YAMLRunner）
  - 其余使用标准库（无额外依赖）

------

## 📬 联系方式（可选）

作者：Xiaotong Wang
 邮箱：xiaotongwang98@gmail.com
 GitHub：`https://github.com/ozil111/cli-test-framework`
