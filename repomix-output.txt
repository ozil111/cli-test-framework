This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-27T05:55:03.407Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
Example_usage.py
parallel_example.py
PARALLEL_TESTING_GUIDE.md
README_cn.md
README.md
requirements.txt
setup.py
src/__init__.py
src/core/__init__.py
src/core/assertions.py
src/core/base_runner.py
src/core/parallel_runner.py
src/core/process_worker.py
src/core/test_case.py
src/runners/__init__.py
src/runners/json_runner.py
src/runners/parallel_json_runner.py
src/runners/yaml_runner.py
src/utils/__init__.py
src/utils/path_resolver.py
src/utils/report_generator.py
tests/__init__.py
tests/fixtures/test_cases.json
tests/fixtures/test_cases.yaml
tests/fixtures/test_cases1.json
tests/performance_test.py
tests/test_parallel_runner.py
tests/test_report.txt
tests/test_runners.py
tests/test1.py

================================================================
Files
================================================================

================
File: Example_usage.py
================
# Example usage
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator
import sys

def main():
    runner = JSONRunner(config_file="D:/Document/xcode/Compare-File-Tool/test_script/test_cases.json", workspace="D:/Document/xcode/Compare-File-Tool")
    success = runner.run_tests()
    
    # Generate and save the report
    report_generator = ReportGenerator(runner.results, "D:/Document/xcode/Compare-File-Tool/test_script/test_report.txt")
    report_generator.print_report()
    report_generator.save_report()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

================
File: parallel_example.py
================
# å¹¶è¡Œæµ‹è¯•ç¤ºä¾‹ç”¨æ³•
from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner
from src.utils.report_generator import ReportGenerator
import sys
import time

def run_sequential_test():
    """è¿è¡Œé¡ºåºæµ‹è¯•"""
    print("=" * 60)
    print("è¿è¡Œé¡ºåºæµ‹è¯•...")
    print("=" * 60)
    
    start_time = time.time()
    runner = JSONRunner(config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", workspace=".")
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\né¡ºåºæµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
    return success, runner.results, end_time - start_time

def run_parallel_test(max_workers=None, execution_mode="thread"):
    """è¿è¡Œå¹¶è¡Œæµ‹è¯•"""
    print("=" * 60)
    print(f"è¿è¡Œå¹¶è¡Œæµ‹è¯• (æ¨¡å¼: {execution_mode}, å·¥ä½œçº¿ç¨‹: {max_workers or 'auto'})...")
    print("=" * 60)
    
    start_time = time.time()
    runner = ParallelJSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases.json", 
        workspace=".",
        max_workers=max_workers,
        execution_mode=execution_mode
    )
    success = runner.run_tests()
    end_time = time.time()
    
    print(f"\nå¹¶è¡Œæµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
    return success, runner.results, end_time - start_time

def main():
    """ä¸»å‡½æ•°ï¼šæ¯”è¾ƒé¡ºåºå’Œå¹¶è¡Œæµ‹è¯•çš„æ€§èƒ½"""
    
    # è¿è¡Œé¡ºåºæµ‹è¯•
    seq_success, seq_results, seq_time = run_sequential_test()
    
    # è¿è¡Œå¹¶è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼‰
    par_success, par_results, par_time = run_parallel_test(max_workers=4, execution_mode="thread")
    
    # è¿è¡Œå¹¶è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼‰
    proc_success, proc_results, proc_time = run_parallel_test(max_workers=2, execution_mode="process")
    
    # æ€§èƒ½æ¯”è¾ƒ
    print("\n" + "=" * 60)
    print("æ€§èƒ½æ¯”è¾ƒç»“æžœ:")
    print("=" * 60)
    print(f"é¡ºåºæ‰§è¡Œæ—¶é—´:     {seq_time:.2f} ç§’")
    print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): {par_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x)")
    print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): {proc_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/proc_time:.2f}x)")
    
    # ç”ŸæˆæŠ¥å‘Š
    if par_success:
        report_generator = ReportGenerator(par_results, "parallel_test_report.txt")
        report_generator.print_report()
        report_generator.save_report()
        print(f"\nå¹¶è¡Œæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: parallel_test_report.txt")
    
    # è¿”å›žæœ€ç»ˆç»“æžœ
    return seq_success and par_success and proc_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

================
File: PARALLEL_TESTING_GUIDE.md
================
# å¹¶è¡Œæµ‹è¯•åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

ä½ çš„æµ‹è¯•æ¡†æž¶çŽ°åœ¨æ”¯æŒå¹¶è¡Œæµ‹è¯•æ‰§è¡Œï¼Œå¯ä»¥æ˜¾è‘—æå‡æµ‹è¯•æ‰§è¡Œæ•ˆçŽ‡ã€‚æ¡†æž¶æä¾›äº†ä¸¤ç§å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼š**çº¿ç¨‹æ¨¡å¼**å’Œ**è¿›ç¨‹æ¨¡å¼**ã€‚

## åŠŸèƒ½ç‰¹æ€§

### âœ… å·²å®žçŽ°çš„åŠŸèƒ½

- **å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šé€‚ç”¨äºŽI/Oå¯†é›†åž‹æµ‹è¯•
- **å¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œ**ï¼šé€‚ç”¨äºŽCPUå¯†é›†åž‹æµ‹è¯•ï¼Œæä¾›å®Œå…¨éš”ç¦»çš„æ‰§è¡ŒçŽ¯å¢ƒ
- **å¯é…ç½®å¹¶å‘æ•°**ï¼šæ”¯æŒè‡ªå®šä¹‰æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°
- **çº¿ç¨‹å®‰å…¨è®¾è®¡**ï¼šç¡®ä¿æµ‹è¯•ç»“æžœçš„æ­£ç¡®æ€§å’Œè¾“å‡ºçš„æ¸…æ™°æ€§
- **æ€§èƒ½ç›‘æŽ§**ï¼šæä¾›æ‰§è¡Œæ—¶é—´ç»Ÿè®¡å’ŒåŠ é€Ÿæ¯”åˆ†æž
- **å‘åŽå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹çŽ°æœ‰çš„é¡ºåºæ‰§è¡Œä»£ç 

### ðŸ“Š æ€§èƒ½æå‡

æ ¹æ®æµ‹è¯•ç»“æžœï¼Œå¹¶è¡Œæ‰§è¡Œå¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

- **çº¿ç¨‹æ¨¡å¼**ï¼šé€šå¸¸å¯èŽ·å¾— **2-4å€** çš„åŠ é€Ÿæ¯”
- **è¿›ç¨‹æ¨¡å¼**ï¼šé€‚åˆéœ€è¦å®Œå…¨éš”ç¦»çš„åœºæ™¯ï¼Œä½†å¯åŠ¨å¼€é”€è¾ƒå¤§

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# åˆ›å»ºå¹¶è¡Œè¿è¡Œå™¨
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # æœ€å¤§å¹¶å‘æ•°
    execution_mode="thread"  # æ‰§è¡Œæ¨¡å¼ï¼šthread æˆ– process
)

# è¿è¡Œæµ‹è¯•
success = runner.run_tests()
```

### é…ç½®é€‰é¡¹

| å‚æ•° | ç±»åž‹ | é»˜è®¤å€¼ | è¯´æ˜Ž |
|------|------|--------|------|
| `config_file` | str | "test_cases.json" | æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„ |
| `workspace` | str | None | å·¥ä½œç›®å½• |
| `max_workers` | int | None (è‡ªåŠ¨) | æœ€å¤§å¹¶å‘æ•° |
| `execution_mode` | str | "thread" | æ‰§è¡Œæ¨¡å¼ï¼šthread/process |

### æ‰§è¡Œæ¨¡å¼é€‰æ‹©

#### çº¿ç¨‹æ¨¡å¼ (thread)
- **é€‚ç”¨åœºæ™¯**ï¼šI/Oå¯†é›†åž‹æµ‹è¯•ï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶æ“ä½œï¼‰
- **ä¼˜åŠ¿**ï¼šå¯åŠ¨å¿«ï¼Œå†…å­˜å…±äº«ï¼Œé€‚åˆå¤§å¤šæ•°æµ‹è¯•åœºæ™¯
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•° Ã— 2-4

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=4,
    execution_mode="thread"
)
```

#### è¿›ç¨‹æ¨¡å¼ (process)
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å®Œå…¨éš”ç¦»çš„æµ‹è¯•ï¼ŒCPUå¯†é›†åž‹ä»»åŠ¡
- **ä¼˜åŠ¿**ï¼šå®Œå…¨éš”ç¦»ï¼Œé¿å…GILé™åˆ¶
- **æŽ¨èå¹¶å‘æ•°**ï¼šCPUæ ¸å¿ƒæ•°

```python
runner = ParallelJSONRunner(
    config_file="test_cases.json", 
    max_workers=2,
    execution_mode="process"
)
```

## ç¤ºä¾‹ä»£ç 

### æ€§èƒ½æ¯”è¾ƒç¤ºä¾‹

```python
# è¿è¡Œæ€§èƒ½æ¯”è¾ƒ
python parallel_example.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# é¡ºåºæ‰§è¡Œæ—¶é—´:     0.12 ç§’
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): 0.03 ç§’ (åŠ é€Ÿæ¯”: 3.58x)
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): 0.10 ç§’ (åŠ é€Ÿæ¯”: 1.15x)
```

### å¿«é€ŸéªŒè¯

```python
# å¿«é€ŸéªŒè¯å¹¶è¡ŒåŠŸèƒ½
python test_parallel_simple.py

# å¿«é€Ÿæ€§èƒ½æµ‹è¯•
python performance_test.py
```

## æœ€ä½³å®žè·µ

### 1. é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°

```python
import os

# CPUå¯†é›†åž‹ä»»åŠ¡
max_workers = os.cpu_count()

# I/Oå¯†é›†åž‹ä»»åŠ¡  
max_workers = os.cpu_count() * 2
```

### 2. æµ‹è¯•ç”¨ä¾‹è®¾è®¡åŽŸåˆ™

- âœ… **ç¡®ä¿æµ‹è¯•ç‹¬ç«‹æ€§**ï¼šæµ‹è¯•ç”¨ä¾‹ä¹‹é—´ä¸åº”æœ‰ä¾èµ–å…³ç³»
- âœ… **é¿å…å…±äº«èµ„æºå†²çª**ï¼šä¸åŒæµ‹è¯•ä¸åº”æ“ä½œç›¸åŒçš„æ–‡ä»¶æˆ–ç«¯å£
- âœ… **ä½¿ç”¨ç›¸å¯¹è·¯å¾„**ï¼šæ¡†æž¶ä¼šè‡ªåŠ¨å¤„ç†è·¯å¾„è§£æž

### 3. é”™è¯¯å¤„ç†

```python
try:
    runner = ParallelJSONRunner(config_file="test_cases.json")
    success = runner.run_tests()
    
    if not success:
        # æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•
        for detail in runner.results["details"]:
            if detail["status"] == "failed":
                print(f"å¤±è´¥çš„æµ‹è¯•: {detail['name']}")
                print(f"é”™è¯¯ä¿¡æ¯: {detail['message']}")
                
except Exception as e:
    print(f"æ‰§è¡Œå‡ºé”™: {e}")
    # å›žé€€åˆ°é¡ºåºæ‰§è¡Œ
    runner.run_tests_sequential()
```

## æŠ€æœ¯å®žçŽ°

### æž¶æž„è®¾è®¡

```
ParallelRunner (åŸºç±»)
â”œâ”€â”€ çº¿ç¨‹å®‰å…¨çš„ç»“æžœæ”¶é›†
â”œâ”€â”€ å¯é…ç½®çš„æ‰§è¡Œæ¨¡å¼
â””â”€â”€ å¼‚å¸¸å¤„ç†æœºåˆ¶

ParallelJSONRunner (å®žçŽ°ç±»)
â”œâ”€â”€ ç»§æ‰¿ ParallelRunner
â”œâ”€â”€ JSONé…ç½®è§£æž
â””â”€â”€ è·¯å¾„è§£æžåŠŸèƒ½

è¿›ç¨‹å·¥ä½œå™¨ (process_worker.py)
â”œâ”€â”€ ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œ
â”œâ”€â”€ é¿å…åºåˆ—åŒ–é—®é¢˜
â””â”€â”€ å®Œå…¨éš”ç¦»çŽ¯å¢ƒ
```

### çº¿ç¨‹å®‰å…¨æœºåˆ¶

- **ç»“æžœæ”¶é›†é”**ï¼š`threading.Lock()` ä¿æŠ¤å…±äº«ç»“æžœæ•°æ®
- **è¾“å‡ºæŽ§åˆ¶é”**ï¼šé¿å…å¹¶å‘è¾“å‡ºæ··ä¹±
- **å¼‚å¸¸éš”ç¦»**ï¼šå•ä¸ªæµ‹è¯•å¤±è´¥ä¸å½±å“å…¶ä»–æµ‹è¯•

## æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿›ç¨‹æ¨¡å¼åºåˆ—åŒ–é”™è¯¯**
   - åŽŸå› ï¼šå¯¹è±¡åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å±žæ€§ï¼ˆå¦‚é”ï¼‰
   - è§£å†³ï¼šä½¿ç”¨ç‹¬ç«‹çš„è¿›ç¨‹å·¥ä½œå™¨å‡½æ•°

2. **è·¯å¾„è§£æžé”™è¯¯**
   - åŽŸå› ï¼šç³»ç»Ÿå‘½ä»¤è¢«å½“ä½œç›¸å¯¹è·¯å¾„å¤„ç†
   - è§£å†³ï¼šæ›´æ–° `PathResolver` çš„ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨

3. **æ€§èƒ½æå‡ä¸æ˜Žæ˜¾**
   - åŽŸå› ï¼šæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ—¶é—´å¤ªçŸ­ï¼Œå¹¶è¡Œå¼€é”€å¤§äºŽæ”¶ç›Š
   - è§£å†³ï¼šå¢žåŠ æµ‹è¯•ç”¨ä¾‹æ•°é‡æˆ–ä½¿ç”¨æ›´å¤æ‚çš„æµ‹è¯•

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†è¾“å‡º
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    max_workers=1,  # è®¾ä¸º1ä¾¿äºŽè°ƒè¯•
    execution_mode="thread"
)

# æŸ¥çœ‹è¯¦ç»†ç»“æžœ
print(json.dumps(runner.results, indent=2, ensure_ascii=False))
```

## ç‰ˆæœ¬å…¼å®¹æ€§

- **Pythonç‰ˆæœ¬**ï¼š3.6+
- **ä¾èµ–é¡¹**ï¼šæ— é¢å¤–ä¾èµ–ï¼Œä½¿ç”¨æ ‡å‡†åº“
- **å‘åŽå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹çŽ°æœ‰çš„ `JSONRunner` ä»£ç 

## æ€»ç»“

å¹¶è¡Œæµ‹è¯•åŠŸèƒ½ä¸ºä½ çš„æµ‹è¯•æ¡†æž¶å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«é€‚åˆï¼š

- ðŸš€ **å¤§è§„æ¨¡æµ‹è¯•å¥—ä»¶**ï¼šæ•°åä¸ªæˆ–æ•°ç™¾ä¸ªæµ‹è¯•ç”¨ä¾‹
- ðŸŒ **I/Oå¯†é›†åž‹æµ‹è¯•**ï¼šç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶æ“ä½œç­‰
- âš¡ **CI/CDæµæ°´çº¿**ï¼šç¼©çŸ­æž„å»ºæ—¶é—´
- ðŸ”„ **å›žå½’æµ‹è¯•**ï¼šå¿«é€ŸéªŒè¯ä»£ç å˜æ›´

é€šè¿‡åˆç†é…ç½®å¹¶å‘å‚æ•°å’Œé€‰æ‹©é€‚å½“çš„æ‰§è¡Œæ¨¡å¼ï¼Œä½ å¯ä»¥åœ¨ä¿è¯æµ‹è¯•å¯é æ€§çš„åŒæ—¶ï¼Œå¤§å¹…æå‡æµ‹è¯•æ‰§è¡Œæ•ˆçŽ‡ï¼

================
File: README_cn.md
================
# Command line æµ‹è¯•æ¡†æž¶å¼€å‘æ–‡æ¡£

## 1. æ¦‚è¿°
æœ¬æµ‹è¯•æ¡†æž¶æ˜¯ä¸€ä¸ªè½»é‡çº§ã€å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æµ‹è¯•è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒé€šè¿‡JSON/YAMLæ ¼å¼å®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼Œæä¾›å®Œæ•´çš„æµ‹è¯•æ‰§è¡Œã€ç»“æžœéªŒè¯å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚æ ¸å¿ƒç›®æ ‡æ˜¯ä¸ºå‘½ä»¤è¡Œå·¥å…·å’Œè„šæœ¬æä¾›æ ‡å‡†åŒ–çš„æµ‹è¯•ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒè·¨å¹³å°æµ‹è¯•åœºæ™¯ã€‚

## 2. åŠŸèƒ½ç‰¹ç‚¹
- **æ¨¡å—åŒ–æž¶æž„**ï¼šæ ¸å¿ƒç»„ä»¶è§£è€¦è®¾è®¡ï¼ˆè¿è¡Œå™¨/æ–­è¨€/æŠ¥å‘Šï¼‰
- **å¤šæ ¼å¼æ”¯æŒ**ï¼šåŽŸç”Ÿæ”¯æŒJSON/YAMLæµ‹è¯•ç”¨ä¾‹æ ¼å¼
- **å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ**ï¼šæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹å¹¶è¡Œæµ‹è¯•ï¼Œæ˜¾è‘—æå‡æ‰§è¡Œæ•ˆçŽ‡
- **æ™ºèƒ½è·¯å¾„è§£æž**ï¼šè‡ªåŠ¨å¤„ç†ç›¸å¯¹è·¯å¾„ä¸Žç»å¯¹è·¯å¾„è½¬æ¢
- **ä¸°å¯Œæ–­è¨€æœºåˆ¶**ï¼šåŒ…å«è¿”å›žå€¼æ ¡éªŒã€è¾“å‡ºå†…å®¹åŒ¹é…ã€æ­£åˆ™è¡¨è¾¾å¼éªŒè¯
- **å¯æ‰©å±•æŽ¥å£**ï¼šé€šè¿‡ç»§æ‰¿BaseRunnerå¯å¿«é€Ÿå®žçŽ°æ–°æµ‹è¯•æ ¼å¼æ”¯æŒ
- **æ‰§è¡ŒçŽ¯å¢ƒéš”ç¦»**ï¼šç‹¬ç«‹å­è¿›ç¨‹è¿è¡Œä¿è¯æµ‹è¯•éš”ç¦»æ€§
- **è¯Šæ–­æŠ¥å‘Š**ï¼šæä¾›é€šè¿‡çŽ‡ç»Ÿè®¡å’Œå¤±è´¥è¯¦æƒ…å®šä½

## 3. ä½¿ç”¨è¯´æ˜Ž

### çŽ¯å¢ƒè¦æ±‚
```bash
pip install -r requirements.txt
Python >= 3.6
```

### å¿«é€Ÿå¼€å§‹

1. åˆ›å»ºæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ï¼ˆç¤ºä¾‹è§`tests/fixtures/`ï¼‰
2. ç¼–å†™æ‰§è¡Œè„šæœ¬ï¼š

**é¡ºåºæ‰§è¡Œ**ï¼š
```python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

**å¹¶è¡Œæ‰§è¡Œ**ï¼š
```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ
runner = ParallelJSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root",
    max_workers=4,           # æœ€å¤§å¹¶å‘æ•°
    execution_mode="thread"  # æ‰§è¡Œæ¨¡å¼ï¼šthread æˆ– process
)
success = runner.run_tests()

# æ€§èƒ½æ¯”è¾ƒç¤ºä¾‹
python parallel_example.py
```

### æµ‹è¯•ç”¨ä¾‹æ ¼å¼ç¤ºä¾‹

**JSONæ ¼å¼**ï¼š

```json
{
  "test_cases": [
    {
      "name": "æ–‡ä»¶æ¯”å¯¹æµ‹è¯•",
      "command": "diff",
      "args": ["file1.txt", "file2.txt"],
      "expected": {
        "return_code": 0,
        "output_contains": ["identical"]
      }
    }
  ]
}
```

**YAMLæ ¼å¼**ï¼š

```yaml
test_cases:
  - name: ç›®å½•æ‰«ææµ‹è¯•
    command: ls
    args: 
      - -l
      - docs/
    expected:
      return_code: 0
      output_matches: ".*\.md$"
```

## 4. ç³»ç»Ÿæµç¨‹

### æž¶æž„æ¨¡å—

```mermaid
graph TD
    A[æµ‹è¯•ç”¨ä¾‹] --> B[Runner]
    B --> C[è·¯å¾„è§£æž]
    B --> D[å­è¿›ç¨‹æ‰§è¡Œ]
    D --> E[æ–­è¨€éªŒè¯]
    E --> F[ç»“æžœæ”¶é›†]
    F --> G[æŠ¥å‘Šç”Ÿæˆ]
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜Ž

1. **Test Runner**

   - åŠ è½½æµ‹è¯•é…ç½®
   - ç®¡ç†æµ‹è¯•ç”Ÿå‘½å‘¨æœŸ
   - åè°ƒå„ç»„ä»¶åä½œ

2. **PathResolver**

   ```python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   æ™ºèƒ½å¤„ç†è·¯å¾„å‚æ•°ï¼Œè‡ªåŠ¨å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºåŸºäºŽworkspaceçš„ç»å¯¹è·¯å¾„

3. **Assertion Engine**

   - è¿”å›žå€¼æ ¡éªŒï¼ˆreturn_codeï¼‰
   - è¾“å‡ºå†…å®¹åŒ¹é…ï¼ˆcontains/matchesï¼‰
   - å¼‚å¸¸æ•èŽ·æœºåˆ¶

4. **Report Generator**

   - å®žæ—¶ç»Ÿè®¡æµ‹è¯•è¿›åº¦
   - ç”Ÿæˆå¸¦é”™è¯¯å®šä½çš„è¯¦ç»†æŠ¥å‘Š
   - æ”¯æŒæŽ§åˆ¶å°è¾“å‡ºå’Œæ–‡ä»¶ä¿å­˜

## 5. ä»£ç å®žçŽ°è¯¦è§£

### æ ¸å¿ƒç±»è¯´æ˜Ž

**TestCase æ•°æ®ç±»**ï¼š

```python
@dataclass
class TestCase:
    name: str          # æµ‹è¯•åç§°
    command: str       # æ‰§è¡Œå‘½ä»¤/ç¨‹åº
    args: List[str]    # å‚æ•°åˆ—è¡¨
    expected: Dict[str, Any]  # é¢„æœŸç»“æžœ
```

**BaseRunner æŠ½è±¡ç±»**ï¼š

```python
def run_tests(self) -> bool:
    self.load_test_cases()
    for case in self.test_cases:
        result = self.run_single_test(case)
        # ç»“æžœæ”¶é›†é€»è¾‘...
    return self.results["failed"] == 0
```

**JSONRunner å®žçŽ°**ï¼š

```python
def load_test_cases(self):
    with open(config_path) as f:
        cases = json.load(f)["test_cases"]
        # å­—æ®µæ ¡éªŒå’Œè·¯å¾„é¢„å¤„ç†
        case["command"] = self.path_resolver.resolve_command(case["command"])
```

**æ–­è¨€å­ç³»ç»Ÿ**ï¼š

```python
class Assertions:
    @staticmethod
    def matches(text, pattern):
        if not re.search(pattern, text):
            raise AssertionError(f"Pattern mismatch: {pattern}")
```

## 6. å¥å£®æ€§è®¾è®¡

### å¯é æ€§ä¿éšœæŽªæ–½

- **è¾“å…¥éªŒè¯**ï¼šå¼ºåˆ¶æ ¡éªŒæµ‹è¯•ç”¨ä¾‹å¿…å¡«å­—æ®µ

- **é”™è¯¯éš”ç¦»**ï¼šå•ä¸ªæµ‹è¯•å¤±è´¥ä¸å½±å“åŽç»­æ‰§è¡Œ

- **å­è¿›ç¨‹é˜²æŠ¤**ï¼š

  ```python
  subprocess.run(..., check=False, shell=True)
  ```

- **è·¯å¾„å®‰å…¨**ï¼šè‡ªåŠ¨å¤„ç†è·¯å¾„åˆ†éš”ç¬¦å·®å¼‚

- **å¼‚å¸¸æ•èŽ·**ï¼šä¸‰çº§é”™è¯¯å¤„ç†ï¼ˆæ–­è¨€é”™è¯¯/æ‰§è¡Œé”™è¯¯/ç³»ç»Ÿé”™è¯¯ï¼‰

- **ç»“æžœå®Œæ•´æ€§**ï¼šç¡®ä¿æ‰€æœ‰æµ‹è¯•ç»“æžœéƒ½è¢«è®°å½•

## 7. æ‰©å±•æ€§è®¾è®¡

### æ‰©å±•æ–¹å‘

1. **æ ¼å¼æ‰©å±•**ï¼šå®žçŽ°XMLRunner/TomlRunner
2. **æ‰§è¡Œå™¨æ‰©å±•**ï¼šæ”¯æŒDocker/SSHè¿œç¨‹æ‰§è¡Œ
3. **æ–­è¨€æ‰©å±•**ï¼šæ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–­è¨€
4. **æŠ¥å‘Šæ ¼å¼**ï¼šæ”¯æŒHTML/PDFæŠ¥å‘Šç”Ÿæˆ
5. **åˆ†å¸ƒå¼æ‰§è¡Œ**ï¼šå¤šè¿›ç¨‹å¹¶è¡Œæµ‹è¯•

æ‰©å±•ç¤ºä¾‹ï¼ˆæ–°å¢žXMLè¿è¡Œå™¨ï¼‰ï¼š

```python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # è§£æžXMLç»“æž„å¹¶è½¬æ¢ä¸ºTestCaseå¯¹è±¡
```

## 8. å¹¶è¡Œæµ‹è¯•åŠŸèƒ½

### 8.1 å¹¶è¡Œæµ‹è¯•æ¦‚è¿°

æ¡†æž¶æ”¯æŒä¸¤ç§å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼š
- **çº¿ç¨‹æ¨¡å¼ï¼ˆthreadï¼‰**ï¼šé€‚ç”¨äºŽI/Oå¯†é›†åž‹æµ‹è¯•ï¼Œå…±äº«å†…å­˜ç©ºé—´
- **è¿›ç¨‹æ¨¡å¼ï¼ˆprocessï¼‰**ï¼šé€‚ç”¨äºŽCPUå¯†é›†åž‹æµ‹è¯•ï¼Œå®Œå…¨éš”ç¦»æ‰§è¡ŒçŽ¯å¢ƒ

### 8.2 å¹¶è¡Œæµ‹è¯•é…ç½®

```python
from src.runners.parallel_json_runner import ParallelJSONRunner

# çº¿ç¨‹æ¨¡å¼é…ç½®
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=4,           # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
    execution_mode="thread"  # çº¿ç¨‹æ¨¡å¼
)

# è¿›ç¨‹æ¨¡å¼é…ç½®
runner = ParallelJSONRunner(
    config_file="test_cases.json",
    workspace=".",
    max_workers=2,           # æœ€å¤§å¹¶å‘è¿›ç¨‹æ•°
    execution_mode="process" # è¿›ç¨‹æ¨¡å¼
)
```

### 8.3 æ€§èƒ½ä¼˜åŠ¿

å¹¶è¡Œæµ‹è¯•å¯ä»¥æ˜¾è‘—æå‡æµ‹è¯•æ‰§è¡Œæ•ˆçŽ‡ï¼š

```bash
# è¿è¡Œæ€§èƒ½æ¯”è¾ƒç¤ºä¾‹
python parallel_example.py

# å…¸åž‹è¾“å‡ºï¼š
# é¡ºåºæ‰§è¡Œæ—¶é—´:     12.45 ç§’
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(çº¿ç¨‹): 3.21 ç§’ (åŠ é€Ÿæ¯”: 3.88x)
# å¹¶è¡Œæ‰§è¡Œæ—¶é—´(è¿›ç¨‹): 4.12 ç§’ (åŠ é€Ÿæ¯”: 3.02x)
```

### 8.4 çº¿ç¨‹å®‰å…¨è®¾è®¡

- **ç»“æžœæ”¶é›†**ï¼šä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿æµ‹è¯•ç»“æžœå®‰å…¨æ›´æ–°
- **è¾“å‡ºæŽ§åˆ¶**ï¼šé€šè¿‡æ‰“å°é”é¿å…è¾“å‡ºæ··ä¹±
- **å¼‚å¸¸å¤„ç†**ï¼šæ¯ä¸ªå·¥ä½œçº¿ç¨‹ç‹¬ç«‹å¤„ç†å¼‚å¸¸

### 8.5 æœ€ä½³å®žè·µ

1. **é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°**ï¼š
   - CPUå¯†é›†åž‹ï¼š`max_workers = CPUæ ¸å¿ƒæ•°`
   - I/Oå¯†é›†åž‹ï¼š`max_workers = CPUæ ¸å¿ƒæ•° * 2-4`

2. **é€‰æ‹©åˆé€‚çš„æ‰§è¡Œæ¨¡å¼**ï¼š
   - æµ‹è¯•é—´æ— ä¾èµ–ï¼šæŽ¨èçº¿ç¨‹æ¨¡å¼
   - éœ€è¦å®Œå…¨éš”ç¦»ï¼šæŽ¨èè¿›ç¨‹æ¨¡å¼

3. **æµ‹è¯•ç”¨ä¾‹è®¾è®¡**ï¼š
   - ç¡®ä¿æµ‹è¯•ç”¨ä¾‹é—´æ— ä¾èµ–å…³ç³»
   - é¿å…å…±äº«èµ„æºå†²çª

### 8.6 å•å…ƒæµ‹è¯•

è¿è¡Œå¹¶è¡ŒåŠŸèƒ½çš„å•å…ƒæµ‹è¯•ï¼š

```bash
python -m pytest tests/test_parallel_runner.py -v
```

## 9. ç¤ºä¾‹æ¼”ç¤º

### è¾“å…¥æ ·ä¾‹

```json
{
  "test_cases": [
    {
      "name": "ç‰ˆæœ¬æ£€æŸ¥æµ‹è¯•",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "output_matches": "Python 3\\.[89]\\.",
        "return_code": 0
      }
    }
  ]
}
```

### è¾“å‡ºæŠ¥å‘Š

```
Test Results Summary:
Total Tests: 1
Passed: 1 (100.0%)
Failed: 0 (0.0%)

Detailed Results:
âœ“ ç‰ˆæœ¬æ£€æŸ¥æµ‹è¯•
```

## 9. æ³¨æ„äº‹é¡¹

1. **è·¯å¾„å¤„ç†**ï¼š
   - ä½¿ç”¨`--`å¼€å¤´çš„å‚æ•°ä¸ä¼šè¢«è·¯å¾„è½¬æ¢
   - Windowsè·¯å¾„éœ€ä½¿ç”¨`/`æˆ–è½¬ä¹‰`\\`
2. **å‘½ä»¤é™åˆ¶**ï¼š
   - ä»…æ”¯æŒå•å‘½ä»¤æ‰§è¡Œ
   - å¤æ‚ç®¡é“éœ€å°è£…ä¸ºè„šæœ¬
3. **å®‰å…¨è§„èŒƒ**ï¼š
   - ä¸è¦ä»¥rootæƒé™è¿è¡Œ
   - ç¦æ­¢æ‰§è¡Œä¸å¯ä¿¡æµ‹è¯•ç”¨ä¾‹
4. **æ€§èƒ½æ³¨æ„**ï¼š
   - å•ä¸ªæµ‹è¯•è¶…æ—¶é»˜è®¤æ— é™åˆ¶
   - å»ºè®®I/Oå¯†é›†åž‹æµ‹è¯•è‡ªè¡ŒæŽ§åˆ¶å¹¶å‘
5. **çŽ¯å¢ƒä¾èµ–**ï¼š
   - éœ€é¢„å…ˆå®‰è£…è¢«æµ‹ç¨‹åº
   - Pythonè·¯å¾„éœ€åœ¨ç³»ç»ŸPATHä¸­

================
File: README.md
================
# Command line Testing Framework Development Documentation

## 1. Overview

This testing framework is a lightweight and extensible automated testing solution that supports defining test cases via JSON/YAML formats, providing complete test execution, result verification, and report generation capabilities. The core objective is to provide standardized test management capabilities for command-line tools and scripts, supporting cross-platform testing scenarios.

## 2. Features

- **Modular Architecture**: Decoupled design of core components (runner/assertion/report)
- **Multi-Format Support**: Native support for JSON/YAML test case formats
- **Intelligent Path Resolution**: Automatic handling of relative and absolute path conversions
- **Rich Assertion Mechanism**: Includes return value validation, output content matching, regular expression verification
- **Extensible Interfaces**: Quickly implement new test format support by inheriting BaseRunner
- **Isolated Execution Environment**: Independent sub-process execution ensures test isolation
- **Diagnostic Reports**: Provides pass rate statistics and failure detail localization

## 3. Usage Instructions

### Environment Requirements

```Bash
pip install -r requirements.txt
Python >= 3.6
```

### Quick Start

1. Create test case files (examples in `tests/fixtures/`)
2. Write an execution script:

```Python
from src.runners.json_runner import JSONRunner

runner = JSONRunner(
    config_file="path/to/test_cases.json",
    workspace="/project/root"
)
success = runner.run_tests()
```

### Test Case Format Examples

**JSON Format**:

```JSON
{
    "test_cases": [
        {
            "name": "File Comparison Test",
            "command": "diff",
            "args": ["file1.txt", "file2.txt"],
            "expected": {
                "return_code": 0,
                "output_contains": ["identical"]
            }
        }
    ]
}
```

**YAML Format**:

```YAML
test_cases:
    - name: Directory Scan Test
      command: ls
      args:
        - -l
        - docs/
      expected:
        return_code: 0
        output_matches: ".*\.md$"
```

## 4. System Flow

### Architecture Modules

```mermaid
graph TD
    A[Test Cases] --> B[Runner]
    B --> C[Path Resolution]
    B --> D[Sub-process Execution]
    D --> E[Assertion Verification]
    E --> F[Result Collection]
    F --> G[Report Generation]
```

### Core Module Description

1. **Test Runner**

   - Loads test configurations
   - Manages test lifecycle
   - Coordinates component collaboration

2. **PathResolver**

   ```Python
   def resolve_paths(args):
       return [workspace/path if not flag else arg for arg in args]
   ```

   Intelligently handles path parameters, automatically converting relative paths to absolute paths based on the workspace.

3. **Assertion Engine**

   - Return value validation (return_code)
   - Output content matching (contains/matches)
   - Exception capture mechanism

4. **Report Generator**

   - Real-time test progress statistics
   - Generates detailed reports with error localization
   - Supports console output and file saving

## 5. Detailed Code Implementation

### Core Class Description

**TestCase Data Class**:

```Python
@dataclass
class TestCase:
    name: str          # Test name
    command: str       # Execution command/program
    args: List[str]    # Argument list
    expected: Dict[str, Any] # Expected results
```

**BaseRunner Abstract Class**:

```Python
def run_tests(self) -> bool:
    self.load_test_cases()
    for case in self.test_cases:
        result = self.run_single_test(case)
        # Result collection logic...
    return self.results["failed"] == 0
```

**JSONRunner Implementation**:

```Python
def load_test_cases(self):
    with open(config_path) as f:
        cases = json.load(f)["test_cases"]
        # Field validation and path preprocessing
        case["command"] = self.path_resolver.resolve_command(case["command"])
```

**Assertion Subsystem**:


```Python
class Assertions:
    @staticmethod
    def matches(text, pattern):
        if not re.search(pattern, text):
            raise AssertionError(f"Pattern mismatch: {pattern}")
```

## 6. Robustness Design

### Reliability Assurance Measures

- **Input Validation**: Enforces validation of required test case fields

- **Error Isolation**: Individual test failures do not affect subsequent executions

- **Sub-process Protection**:

  ```Python
  subprocess.run(..., check=False, shell=True)
  ```
  
- **Path Security**: Automatically handles path separator differences

- **Exception Capture**: Three-level error handling (assertion error/execution error/system error)

- **Result Integrity**: Ensures all test results are recorded

## 7. Extensibility Design

### Extension Directions

1. **Format Extension**: Implement XMLRunner/TomlRunner
2. **Executor Extension**: Support Docker/SSH remote execution
3. **Assertion Extension**: Add performance metric assertions
4. **Report Format**: Support HTML/PDF report generation
5. **Distributed Execution**: Multi-process parallel testing

Extension Example (adding XML runner):

```Python
class XMLRunner(BaseRunner):
    def load_test_cases(self):
        import xml.etree.ElementTree as ET
        # Parse XML structure and convert to TestCase objects
```

## 8. Example Demonstration

### Input Example

```JSON
{
    "test_cases": [
        {
            "name": "Version Check Test",
            "command": "python",
            "args": ["--version"],
            "expected": {
                "output_matches": "Python 3\\.[89]\\.",
                "return_code": 0
            }
        }
    ]
}
```

### Output Report

```
Test Results Summary:
Total Tests: 1
Passed: 1 (100.0%)
Failed: 0 (0.0%)

Detailed Results:
âœ“ Version Check Test
```

## 9. Precautions

1. Path Handling:

   - Parameters starting with `--` will not be path-converted
   - Windows paths should use `/` or escaped `\\`
   
2. Command Limitations:

   - Only single command execution is supported
   - Complex pipelines need to be encapsulated into scripts
   
3. Security Specifications:

   - Do not run with root privileges
   - Prohibit execution of untrusted test cases
   
4. Performance Notes:

   - Single test timeout is unlimited by default
   - I/O intensive tests are recommended to control concurrency themselves
   
5. Environment Dependencies:

   - Tested programs need to be pre-installed
- Python path needs to be in the system PATH

================
File: requirements.txt
================
pytest
PyYAML
concurrent.futures

================
File: setup.py
================
from setuptools import setup, find_packages

setup(
    name="cli-test-framework",
    version="0.1.0",
    author="Your Name",
    author_email="your.email@example.com",
    description="A small command line testing framework in Python.",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        # List your project dependencies here
    ],
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
)

================
File: src/__init__.py
================
# File: /python-test-framework/python-test-framework/src/__init__.py

# This file is intentionally left blank.

================
File: src/core/__init__.py
================
from .base_runner import BaseRunner
from .parallel_runner import ParallelRunner
from .test_case import TestCase
from .assertions import Assertions

================
File: src/core/assertions.py
================
import re
from typing import Any, Pattern

class Assertions:
    @staticmethod
    def equals(actual: Any, expected: Any, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected: {expected}, but got: {actual}")
        return True

    @staticmethod
    def contains(container: str, item: str, message: str = "") -> bool:
        """
        Check if the item is contained within the container string.
        This method returns True if the item is found anywhere within the container,
        even if the container contains other information.
        """
        if item not in container:
            raise AssertionError(f"{message} Expected to contain: {item}")
        return True

    @staticmethod
    def matches(text: str, pattern: str, message: str = "") -> bool:
        if not re.search(pattern, text):
            raise AssertionError(f"{message} Text does not match pattern: {pattern}")
        return True

    @staticmethod
    def return_code_equals(actual: int, expected: int, message: str = "") -> bool:
        if actual != expected:
            raise AssertionError(f"{message} Expected return code: {expected}, got: {actual}")
        return True

================
File: src/core/base_runner.py
================
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from .test_case import TestCase
from .assertions import Assertions

class BaseRunner(ABC):
    def __init__(self, config_file: str, workspace: Optional[str] = None):
        if workspace:
            self.workspace = Path(workspace)
        else:
            self.workspace = Path(__file__).parent.parent.parent
        self.config_path = self.workspace / config_file
        self.test_cases: List[TestCase] = []
        self.results: Dict[str, Any] = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "details": []
        }
        self.assertions = Assertions()

    @abstractmethod
    def load_test_cases(self) -> None:
        """Load test cases from configuration file"""
        pass

    def run_tests(self) -> bool:
        """Run all test cases and return whether all tests passed"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting test execution... Total tests: {self.results['total']}")
        print("=" * 50)
        
        for i, case in enumerate(self.test_cases, 1):
            print(f"\nRunning test {i}/{self.results['total']}: {case.name}")
            result = self.run_single_test(case)
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"âœ“ Test passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"âœ— Test failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
                
        print("\n" + "=" * 50)
        print(f"Test execution completed. Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0

    @abstractmethod
    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        """Run a single test case and return the result"""
        pass

================
File: src/core/parallel_runner.py
================
from abc import ABC
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Union
import time
import threading
from .base_runner import BaseRunner
from .test_case import TestCase
from .process_worker import run_test_in_process

class ParallelRunner(BaseRunner):
    """å¹¶è¡Œæµ‹è¯•è¿è¡Œå™¨åŸºç±»ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹æ‰§è¡Œ"""
    
    def __init__(self, config_file: str, workspace: Optional[str] = None, 
                 max_workers: Optional[int] = None, 
                 execution_mode: str = "thread"):
        """
        åˆå§‹åŒ–å¹¶è¡Œè¿è¡Œå™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            workspace: å·¥ä½œç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
            execution_mode: æ‰§è¡Œæ¨¡å¼ï¼Œ'thread'(çº¿ç¨‹) æˆ– 'process'(è¿›ç¨‹)
        """
        super().__init__(config_file, workspace)
        self.max_workers = max_workers
        self.execution_mode = execution_mode
        self.lock = threading.Lock()  # ç”¨äºŽçº¿ç¨‹å®‰å…¨çš„ç»“æžœæ›´æ–°
        
    def run_tests(self) -> bool:
        """å¹¶è¡Œè¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        self.load_test_cases()
        self.results["total"] = len(self.test_cases)
        
        print(f"\nStarting parallel test execution... Total tests: {self.results['total']}")
        print(f"Execution mode: {self.execution_mode}, Max workers: {self.max_workers or 'auto'}")
        print("=" * 50)
        
        start_time = time.time()
        
        if self.execution_mode == "process":
            executor_class = ProcessPoolExecutor
        else:
            executor_class = ThreadPoolExecutor
            
        with executor_class(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            if self.execution_mode == "process":
                # è¿›ç¨‹æ¨¡å¼ï¼šä½¿ç”¨ç‹¬ç«‹çš„å·¥ä½œå™¨å‡½æ•°
                future_to_case = {
                    executor.submit(
                        run_test_in_process, 
                        i, 
                        {
                            "name": case.name,
                            "command": case.command,
                            "args": case.args,
                            "expected": case.expected
                        },
                        str(self.workspace) if self.workspace else None
                    ): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            else:
                # çº¿ç¨‹æ¨¡å¼ï¼šä½¿ç”¨å®žä¾‹æ–¹æ³•
                future_to_case = {
                    executor.submit(self._run_test_with_index, i, case): (i, case) 
                    for i, case in enumerate(self.test_cases, 1)
                }
            
            # æ”¶é›†ç»“æžœ
            for future in as_completed(future_to_case):
                test_index, case = future_to_case[future]
                try:
                    result = future.result()
                    self._update_results(result, test_index, case)
                except Exception as exc:
                    error_result = {
                        "name": case.name,
                        "status": "failed",
                        "message": f"Test execution failed: {str(exc)}",
                        "output": "",
                        "command": "",
                        "return_code": None
                    }
                    self._update_results(error_result, test_index, case)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print("\n" + "=" * 50)
        print(f"Parallel test execution completed in {execution_time:.2f} seconds")
        print(f"Passed: {self.results['passed']}, Failed: {self.results['failed']}")
        return self.results["failed"] == 0
    
    def _run_test_with_index(self, test_index: int, case: TestCase) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è¿”å›žç»“æžœï¼ˆåŒ…å«ç´¢å¼•ä¿¡æ¯ï¼‰"""
        print(f"[Worker] Running test {test_index}: {case.name}")
        result = self.run_single_test(case)
        return result
    
    def _update_results(self, result: Dict[str, Any], test_index: int, case: TestCase) -> None:
        """çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°æµ‹è¯•ç»“æžœ"""
        with self.lock:
            self.results["details"].append(result)
            if result["status"] == "passed":
                self.results["passed"] += 1
                print(f"âœ“ Test {test_index} passed: {case.name}")
            else:
                self.results["failed"] += 1
                print(f"âœ— Test {test_index} failed: {case.name}")
                if result["message"]:
                    print(f"  Error: {result['message']}")
    
    def run_tests_sequential(self) -> bool:
        """å›žé€€åˆ°é¡ºåºæ‰§è¡Œæ¨¡å¼"""
        print("Falling back to sequential execution...")
        return super().run_tests()

================
File: src/core/process_worker.py
================
"""
è¿›ç¨‹å·¥ä½œå™¨æ¨¡å—
ç”¨äºŽå¤šè¿›ç¨‹å¹¶è¡Œæµ‹è¯•æ‰§è¡Œï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
"""

import subprocess
import sys
from typing import Dict, Any
from .test_case import TestCase
from .assertions import Assertions

def run_test_in_process(test_index: int, case_data: Dict[str, Any], workspace: str = None) -> Dict[str, Any]:
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹
    
    Args:
        test_index: æµ‹è¯•ç´¢å¼•
        case_data: æµ‹è¯•ç”¨ä¾‹æ•°æ®å­—å…¸
        workspace: å·¥ä½œç›®å½•
    
    Returns:
        æµ‹è¯•ç»“æžœå­—å…¸
    """
    # é‡æ–°åˆ›å»ºTestCaseå¯¹è±¡ï¼ˆé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
    case = TestCase(
        name=case_data["name"],
        command=case_data["command"],
        args=case_data["args"],
        expected=case_data["expected"]
    )
    
    # åˆ›å»ºæ–­è¨€å¯¹è±¡
    assertions = Assertions()
    
    result = {
        "name": case.name,
        "status": "failed",
        "message": "",
        "output": "",
        "command": "",
        "return_code": None
    }

    try:
        command = f"{case.command} {' '.join(case.args)}"
        result["command"] = command
        print(f"  [Process Worker {test_index}] Executing command: {command}")
        
        process = subprocess.run(
            command,
            cwd=workspace if workspace else None,
            capture_output=True,
            text=True,
            check=False,
            shell=True
        )

        output = process.stdout + process.stderr
        result["output"] = output
        result["return_code"] = process.returncode
        
        if output.strip():
            print(f"  [Process Worker {test_index}] Command output for {case.name}:")
            for line in output.splitlines():
                print(f"    {line}")

        # æ£€æŸ¥è¿”å›žç 
        if "return_code" in case.expected:
            print(f"  [Process Worker {test_index}] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
            assertions.return_code_equals(
                process.returncode,
                case.expected["return_code"]
            )

        # æ£€æŸ¥è¾“å‡ºåŒ…å«
        if "output_contains" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output contains for {case.name}...")
            for expected_text in case.expected["output_contains"]:
                assertions.contains(output, expected_text)

        # æ£€æŸ¥æ­£åˆ™åŒ¹é…
        if "output_matches" in case.expected:
            print(f"  [Process Worker {test_index}] Checking output matches regex for {case.name}...")
            assertions.matches(output, case.expected["output_matches"])

        result["status"] = "passed"
        
    except AssertionError as e:
        result["message"] = str(e)
    except Exception as e:
        result["message"] = f"Execution error: {str(e)}"

    return result

================
File: src/core/test_case.py
================
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class TestCase:
    name: str
    command: str
    args: List[str]
    expected: Dict[str, Any]
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert test case to dictionary format"""
        print("Convert test case to dictionary format")
        print(self.command)
        return {
            "name": self.name,
            "command": self.command,
            "args": self.args,
            "expected": self.expected
        }

================
File: src/runners/__init__.py
================
from .json_runner import JSONRunner
from .yaml_runner import YAMLRunner
from .parallel_json_runner import ParallelJSONRunner

================
File: src/runners/json_runner.py
================
from typing import Optional
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
from typing import Dict, Any

class JSONRunner(BaseRunner):
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self) -> None:
        """Load test cases from a JSON file."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
            # print(self.test_cases)
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, str]:
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",  # æ·»åŠ å‘½ä»¤è¾“å‡ºå­—æ®µ
            "command": "",  # æ·»åŠ æ‰§è¡Œçš„å‘½ä»¤å­—æ®µ
            "return_code": None  # æ·»åŠ è¿”å›žç å­—æ®µ
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command  # ä¿å­˜æ‰§è¡Œçš„å‘½ä»¤
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output  # ä¿å­˜å‘½ä»¤çš„å®Œæ•´è¾“å‡º
            result["return_code"] = process.returncode  # ä¿å­˜è¿”å›žç 
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/runners/parallel_json_runner.py
================
from typing import Optional, Dict, Any
from ..core.parallel_runner import ParallelRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import json
import subprocess
import sys
import threading

class ParallelJSONRunner(ParallelRunner):
    """å¹¶è¡ŒJSONæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_file="test_cases.json", workspace: Optional[str] = None,
                 max_workers: Optional[int] = None, execution_mode: str = "thread"):
        """
        åˆå§‹åŒ–å¹¶è¡ŒJSONè¿è¡Œå™¨
        
        Args:
            config_file: JSONé…ç½®æ–‡ä»¶è·¯å¾„
            workspace: å·¥ä½œç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°
            execution_mode: æ‰§è¡Œæ¨¡å¼ï¼Œ'thread' æˆ– 'process'
        """
        super().__init__(config_file, workspace, max_workers, execution_mode)
        self.path_resolver = PathResolver(self.workspace)
        self._print_lock = threading.Lock()  # ç”¨äºŽæŽ§åˆ¶è¾“å‡ºé¡ºåº

    def load_test_cases(self) -> None:
        """ä»ŽJSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")

                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))

            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            
            # çº¿ç¨‹å®‰å…¨çš„è¾“å‡º
            with self._print_lock:
                print(f"  [Worker] Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            # çº¿ç¨‹å®‰å…¨çš„è¾“å‡º
            if output.strip():
                with self._print_lock:
                    print(f"  [Worker] Command output for {case.name}:")
                    for line in output.splitlines():
                        print(f"    {line}")

            # æ£€æŸ¥è¿”å›žç 
            if "return_code" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking return code for {case.name}: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # æ£€æŸ¥è¾“å‡ºåŒ…å«
            if "output_contains" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output contains for {case.name}...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # æ£€æŸ¥æ­£åˆ™åŒ¹é…
            if "output_matches" in case.expected:
                with self._print_lock:
                    print(f"  [Worker] Checking output matches regex for {case.name}...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/runners/yaml_runner.py
================
from typing import Optional, Dict, Any
from ..core.base_runner import BaseRunner
from ..core.test_case import TestCase
from ..utils.path_resolver import PathResolver
import subprocess
import sys

class YAMLRunner(BaseRunner):
    def __init__(self, config_file="test_cases.yaml", workspace: Optional[str] = None):
        super().__init__(config_file, workspace)
        self.path_resolver = PathResolver(self.workspace)

    def load_test_cases(self):
        """Load test cases from a YAML file."""
        try:
            import yaml
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            required_fields = ["name", "command", "args", "expected"]
            for case in config["test_cases"]:
                if not all(field in case for field in required_fields):
                    raise ValueError(f"Test case {case.get('name', 'unnamed')} is missing required fields")
                
                case["command"] = self.path_resolver.resolve_command(case["command"])
                case["args"] = self.path_resolver.resolve_paths(case["args"])
                self.test_cases.append(TestCase(**case))
                
            print(f"Successfully loaded {len(self.test_cases)} test cases")
        except Exception as e:
            sys.exit(f"Failed to load configuration file: {str(e)}")

    def run_single_test(self, case: TestCase) -> Dict[str, Any]:
        """Run a single test case and return the result"""
        result = {
            "name": case.name,
            "status": "failed",
            "message": "",
            "output": "",
            "command": "",
            "return_code": None
        }

        try:
            command = f"{case.command} {' '.join(case.args)}"
            result["command"] = command
            print(f"  Executing command: {command}")
            
            process = subprocess.run(
                command,
                cwd=self.workspace if self.workspace else None,
                capture_output=True,
                text=True,
                check=False,
                shell=True
            )

            output = process.stdout + process.stderr
            result["output"] = output
            result["return_code"] = process.returncode
            
            if output.strip():
                print("  Command output:")
                for line in output.splitlines():
                    print(f"    {line}")

            # Check return code
            if "return_code" in case.expected:
                print(f"  Checking return code: {process.returncode} (expected: {case.expected['return_code']})")
                self.assertions.return_code_equals(
                    process.returncode,
                    case.expected["return_code"]
                )

            # Check output contains
            if "output_contains" in case.expected:
                print("  Checking output contains expected text...")
                for expected_text in case.expected["output_contains"]:
                    self.assertions.contains(output, expected_text)

            # Check regex patterns
            if "output_matches" in case.expected:
                print("  Checking output matches regex pattern...")
                self.assertions.matches(output, case.expected["output_matches"])

            result["status"] = "passed"
            
        except AssertionError as e:
            result["message"] = str(e)
        except Exception as e:
            result["message"] = f"Execution error: {str(e)}"

        return result

================
File: src/utils/__init__.py
================
# File: /python-test-framework/python-test-framework/src/utils/__init__.py

# This file is intentionally left blank.

================
File: src/utils/path_resolver.py
================
from pathlib import Path
from typing import List

class PathResolver:
    def __init__(self, workspace: Path):
        self.workspace = workspace

    def resolve_paths(self, args: List[str]) -> List[str]:
        resolved_args = []
        for arg in args:
            if not arg.startswith("--"):
                # Only prepend workspace if the path is relative
                if not Path(arg).is_absolute():
                    resolved_args.append(str(self.workspace / arg))
                else:
                    resolved_args.append(arg)
            else:
                resolved_args.append(arg)
        return resolved_args

    def resolve_command(self, command: str) -> str:
        """
        è§£æžå‘½ä»¤è·¯å¾„
        - ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚echo, ping, dirç­‰ï¼‰ä¿æŒåŽŸæ ·
        - ç›¸å¯¹è·¯å¾„çš„å¯æ‰§è¡Œæ–‡ä»¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        """
        # å¸¸è§çš„ç³»ç»Ÿå‘½ä»¤åˆ—è¡¨
        system_commands = {
            'echo', 'ping', 'dir', 'ls', 'cat', 'grep', 'find', 'sort', 
            'head', 'tail', 'wc', 'curl', 'wget', 'git', 'python', 'node',
            'npm', 'pip', 'java', 'javac', 'gcc', 'make', 'cmake', 'docker',
            'kubectl', 'helm', 'terraform', 'ansible', 'ssh', 'scp', 'rsync'
        }
        
        # å¦‚æžœæ˜¯ç³»ç»Ÿå‘½ä»¤æˆ–ç»å¯¹è·¯å¾„ï¼Œä¿æŒåŽŸæ ·
        if command in system_commands or Path(command).is_absolute():
            return command
        
        # å¦åˆ™å½“ä½œç›¸å¯¹è·¯å¾„å¤„ç†
        return str(self.workspace / command)

================
File: src/utils/report_generator.py
================
class ReportGenerator:
    def __init__(self, results: dict, file_path: str):
        self.results = results
        self.file_path = file_path

    def generate_report(self) -> str:
        report = "Test Results Summary:\n"
        report += f"Total Tests: {self.results['total']}\n"
        report += f"Passed: {self.results['passed']}\n"
        report += f"Failed: {self.results['failed']}\n\n"
        
        report += "Detailed Results:\n"
        for detail in self.results['details']:
            status_icon = "âœ“" if detail['status'] == 'passed' else "âœ—"
            report += f"{status_icon} {detail['name']}\n"
            if detail.get('message'):
                report += f"   -> {detail['message']}\n"
        
        # æ·»åŠ å¤±è´¥æ¡ˆä¾‹çš„è¯¦ç»†è¾“å‡ºä¿¡æ¯
        failed_tests = [detail for detail in self.results['details'] if detail['status'] == 'failed']
        if failed_tests:
            report += "\n" + "="*50 + "\n"
            report += "FAILED TEST CASES DETAILS:\n"
            report += "="*50 + "\n\n"
            
            for i, failed_test in enumerate(failed_tests, 1):
                report += f"{i}. Test: {failed_test['name']}\n"
                report += "-" * 40 + "\n"
                
                # æ·»åŠ æ‰§è¡Œçš„å‘½ä»¤
                if failed_test.get('command'):
                    report += f"Command: {failed_test['command']}\n"
                
                # æ·»åŠ è¿”å›žç 
                if failed_test.get('return_code') is not None:
                    report += f"Return Code: {failed_test['return_code']}\n"
                
                # æ·»åŠ å¤±è´¥åŽŸå› 
                if failed_test.get('message'):
                    report += f"Error Message: {failed_test['message']}\n"
                
                # æ·»åŠ å‘½ä»¤çš„å®Œæ•´è¾“å‡ºï¼ˆè¿™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼‰
                if failed_test.get('output'):
                    report += f"\nCommand Output:\n"
                    report += "=" * 30 + "\n"
                    report += f"{failed_test['output']}\n"
                    report += "=" * 30 + "\n"
                
                # æ·»åŠ é”™è¯¯å †æ ˆä¿¡æ¯ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
                if failed_test.get('error_trace'):
                    report += f"Error Trace:\n{failed_test['error_trace']}\n"
                
                # æ·»åŠ æ‰§è¡Œæ—¶é—´ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
                if failed_test.get('duration'):
                    report += f"Duration: {failed_test['duration']}s\n"
                
                report += "\n"
        
        return report

    def save_report(self) -> None:
        report = self.generate_report()
        with open(self.file_path, 'w', encoding='utf-8') as f:
            f.write(report)

    def print_report(self) -> None:
        report = self.generate_report()
        print(report)

================
File: tests/__init__.py
================
# File: /python-test-framework/python-test-framework/tests/__init__.py

# This file is intentionally left blank.

================
File: tests/fixtures/test_cases.json
================
{
  "test_cases": [
    {
      "name": "æµ‹è¯•Pythonç‰ˆæœ¬",
      "command": "python",
      "args": ["--version"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Python"]
      }
    },
    {
      "name": "æµ‹è¯•ç›®å½•åˆ—è¡¨",
      "command": "dir",
      "args": ["."],
      "expected": {
        "return_code": 0,
        "output_contains": ["src"]
      }
    },
    {
      "name": "æµ‹è¯•echoå‘½ä»¤",
      "command": "echo",
      "args": ["Hello World"],
      "expected": {
        "return_code": 0,
        "output_contains": ["Hello World"]
      }
    },
    {
      "name": "æµ‹è¯•pingæœ¬åœ°å›žçŽ¯",
      "command": "ping",
      "args": ["-n", "1", "127.0.0.1"],
      "expected": {
        "return_code": 0,
        "output_contains": ["127.0.0.1"]
      }
    },
    {
      "name": "æµ‹è¯•æ—¶é—´å‘½ä»¤",
      "command": "echo",
      "args": ["%time%"],
      "expected": {
        "return_code": 0
      }
    },
    {
      "name": "æµ‹è¯•æ–‡ä»¶å­˜åœ¨æ€§",
      "command": "dir",
      "args": ["src"],
      "expected": {
        "return_code": 0,
        "output_contains": ["core", "runners"]
      }
    }
  ]
}

================
File: tests/fixtures/test_cases.yaml
================
name: Sample Test Case
command: python script.py
args:
  - --input
  - input.txt
expected:
  return_code: 0
  output_contains:
    - "Success"
    - "Processed"

================
File: tests/fixtures/test_cases1.json
================
{
    "test_cases": [
        {
            "name": "text_identical_default",
            "description": "é»˜è®¤æ–‡æœ¬æ¯”è¾ƒï¼ˆç›¸åŒæ–‡ä»¶ï¼‰",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Files i identical"]
                
            }
        },
        {
            "name": "text_different_range",
            "description": "å¸¦è¡ŒèŒƒå›´é™åˆ¶çš„æ–‡æœ¬æ¯”è¾ƒ",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf", 
                "./test/1_copy.bdf",
                "--start-line=93",
                "--end-line=96",
                "--start-column=1",
                "--end-column=30"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_exact_match",
            "description": "ä¸¥æ ¼JSONæ¯”è¾ƒï¼ˆç›¸åŒæ–‡ä»¶ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/1_copy.json",
                "--file-type=json",
                "--json-compare-mode=exact"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "json_key_based_match",
            "description": "JSONé”®å€¼æ¯”è¾ƒï¼ˆç»“æž„ä¸åŒä½†å…³é”®å­—æ®µåŒ¹é…ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--file-type=json",
                "--json-compare-mode=key-based",
                "--json-key-field=\"phone\""
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "binary_comparison",
            "description": "äºŒè¿›åˆ¶æ–‡ä»¶æ¯”è¾ƒï¼ˆå¸¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/2.bdf",
                "--file-type=binary",
                "--similarity",
                "--chunk-size=4096"
            ],
            "expected": {
                "output_contains": ["Similarity Index"]
                
            }
        },
        {
            "name": "multi_thread_comparison",
            "description": "å¤šçº¿ç¨‹æ–‡æœ¬æ¯”è¾ƒ",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.bdf",
                "./test/1_copy.bdf",
                "--num-threads=8",
                "--verbose"
            ],
            "expected": {
                "output_contains": ["Files are identical"]
                
            }
        },
        {
            "name": "different_output_format",
            "description": "JSONè¾“å‡ºæ ¼å¼æµ‹è¯•",
            "command": "python ./compare_text.py",
            "args": [
                "./test/1.json",
                "./test/2.json",
                "--output-format=json"
            ],
            "expected": {
                "output_contains": ["\"position\""]
                
            }
        },
        {
            "name": "auto_detect_filetype",
            "description": "è‡ªåŠ¨æ–‡ä»¶ç±»åž‹æ£€æµ‹",
            "command": "python ./compare_text.py",
            "args": ["./test/1.bdf", "./test/1_copy.bdf"],
            "expected": {
                "output_contains": ["Auto-detected file type: text"]
                
            }
        },
        {
            "name": "h5_comparison",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒç‰¹å®šè¡¨æ ¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODAL/DISPLACEMENT"],
            "expected": {
                "output_contains": ["Files are different"]
                
            }
        },
        {
            "name": "h5_comparison_all_tables",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒæ‰€æœ‰è¡¨æ ¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5"],
            "expected": {
                "output_contains": ["Files are different."]
                
            }
        },
        {
            "name": "h5_comparison_with_wrong_table",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œé”™è¯¯è¡¨æ ¼è·¯å¾„",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--file-type=h5", "--h5-table=NASTRAN/RESULT/NODALl/DISPLACEMENT"],
            "expected": {
                "matches": ["WARNING - Table .* not found"]                
            }
        },
        {
            "name": "h5_comparison_with_strucutre_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œç»“æž„æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-structure-only"],
            "expected": {
                "output_contains": ["Files are identical."]                
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œå†…å®¹æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/2.h5", "--h5-show-content-diff"],
            "expected": {
                "output_contains": ["Difference at"]
            }
        },
        {
            "name": "h5_comparison_with_content_mode",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œå†…å®¹æ¨¡å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-rtol=1e-5", "--h5-atol=1e-8"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        },
        {
            "name": "h5_comparison_with_table_regex",
            "description": "HDF5æ–‡ä»¶æ¯”è¾ƒï¼Œè¡¨æ ¼æ­£åˆ™è¡¨è¾¾å¼",
            "command": "python ./compare_text.py",
            "args": ["./test/1.h5", "./test/1_copy.h5", "--h5-table-regex=NASTRAN/RESULT/\\b\\w*al\\b/STRESS"],
            "expected": {
                "output_contains": ["Files are identical"]
            }
        }
    ]
}

================
File: tests/performance_test.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œæµ‹è¯•æ€§èƒ½éªŒè¯è„šæœ¬
å¿«é€ŸéªŒè¯å¹¶è¡Œæµ‹è¯•åŠŸèƒ½å’Œæ€§èƒ½æå‡
"""

import sys
import time
import json
import tempfile
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.runners.json_runner import JSONRunner
from src.runners.parallel_json_runner import ParallelJSONRunner

def create_test_config(num_tests=10):
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    test_cases = []
    
    for i in range(num_tests):
        test_cases.append({
            "name": f"æµ‹è¯•ç”¨ä¾‹ {i+1}",
            "command": "echo",
            "args": [f"test_{i+1}"],
            "expected": {
                "return_code": 0,
                "output_contains": [f"test_{i+1}"]
            }
        })
    
    return {"test_cases": test_cases}

def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("=" * 60)
    print("å¹¶è¡Œæµ‹è¯•æ¡†æž¶æ€§èƒ½éªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•é…ç½®
    temp_dir = tempfile.mkdtemp()
    config_file = os.path.join(temp_dir, "perf_test.json")
    
    # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ï¼ˆå¯ä»¥è°ƒæ•´æ•°é‡ï¼‰
    num_tests = 8
    config = create_test_config(num_tests)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"åˆ›å»ºäº† {num_tests} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"æµ‹è¯•é…ç½®æ–‡ä»¶: {config_file}")
    
    results = {}
    
    # 1. é¡ºåºæ‰§è¡Œæµ‹è¯•
    print(f"\n1. é¡ºåºæ‰§è¡Œæµ‹è¯•...")
    start_time = time.time()
    sequential_runner = JSONRunner(config_file, temp_dir)
    seq_success = sequential_runner.run_tests()
    seq_time = time.time() - start_time
    results['sequential'] = {'time': seq_time, 'success': seq_success}
    
    # 2. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼‰
    print(f"\n2. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆçº¿ç¨‹æ¨¡å¼ï¼Œ4ä¸ªå·¥ä½œçº¿ç¨‹ï¼‰...")
    start_time = time.time()
    parallel_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=4, 
        execution_mode="thread"
    )
    par_success = parallel_runner.run_tests()
    par_time = time.time() - start_time
    results['parallel_thread'] = {'time': par_time, 'success': par_success}
    
    # 3. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼‰
    print(f"\n3. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆè¿›ç¨‹æ¨¡å¼ï¼Œ2ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
    start_time = time.time()
    process_runner = ParallelJSONRunner(
        config_file, temp_dir, 
        max_workers=2, 
        execution_mode="process"
    )
    proc_success = process_runner.run_tests()
    proc_time = time.time() - start_time
    results['parallel_process'] = {'time': proc_time, 'success': proc_success}
    
    # æ€§èƒ½åˆ†æž
    print("\n" + "=" * 60)
    print("æ€§èƒ½åˆ†æžç»“æžœ:")
    print("=" * 60)
    
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°é‡:      {num_tests}")
    print(f"é¡ºåºæ‰§è¡Œæ—¶é—´:      {seq_time:.2f} ç§’")
    print(f"å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹):    {par_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x)")
    print(f"å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹):    {proc_time:.2f} ç§’ (åŠ é€Ÿæ¯”: {seq_time/proc_time:.2f}x)")
    
    # éªŒè¯ç»“æžœä¸€è‡´æ€§
    print(f"\nç»“æžœéªŒè¯:")
    print(f"é¡ºåºæ‰§è¡ŒæˆåŠŸ:      {seq_success}")
    print(f"å¹¶è¡Œæ‰§è¡Œ(çº¿ç¨‹)æˆåŠŸ: {par_success}")
    print(f"å¹¶è¡Œæ‰§è¡Œ(è¿›ç¨‹)æˆåŠŸ: {proc_success}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)
    
    # æ€»ç»“
    if all(results[key]['success'] for key in results):
        print(f"\nâœ“ æ‰€æœ‰æµ‹è¯•æ¨¡å¼éƒ½æˆåŠŸæ‰§è¡Œ")
        if par_time < seq_time:
            print(f"âœ“ å¹¶è¡Œæ‰§è¡Œç¡®å®žæå‡äº†æ€§èƒ½")
        else:
            print(f"âš  åœ¨å½“å‰æµ‹è¯•è§„æ¨¡ä¸‹ï¼Œå¹¶è¡Œä¼˜åŠ¿ä¸æ˜Žæ˜¾")
    else:
        print(f"\nâœ— éƒ¨åˆ†æµ‹è¯•æ¨¡å¼æ‰§è¡Œå¤±è´¥")
        return False
    
    return True

if __name__ == "__main__":
    try:
        success = run_performance_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)

================
File: tests/test_parallel_runner.py
================
import unittest
import tempfile
import json
import os
import sys
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.runners.parallel_json_runner import ParallelJSONRunner
from src.runners.json_runner import JSONRunner

class TestParallelRunner(unittest.TestCase):
    """å¹¶è¡Œè¿è¡Œå™¨æµ‹è¯•ç±»"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•çŽ¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.config_file = os.path.join(self.temp_dir, "test_config.json")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = {
            "test_cases": [
                {
                    "name": "æµ‹è¯•1",
                    "command": "echo",
                    "args": ["test1"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test1"]
                    }
                },
                {
                    "name": "æµ‹è¯•2",
                    "command": "echo",
                    "args": ["test2"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test2"]
                    }
                },
                {
                    "name": "æµ‹è¯•3",
                    "command": "echo",
                    "args": ["test3"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test3"]
                    }
                },
                {
                    "name": "æµ‹è¯•4",
                    "command": "echo",
                    "args": ["test4"],
                    "expected": {
                        "return_code": 0,
                        "output_contains": ["test4"]
                    }
                }
            ]
        }
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(test_config, f, ensure_ascii=False, indent=2)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•çŽ¯å¢ƒ"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def test_parallel_vs_sequential_performance(self):
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œç›¸æ¯”é¡ºåºæ‰§è¡Œçš„æ€§èƒ½æå‡"""
        # é¡ºåºæ‰§è¡Œ
        sequential_runner = JSONRunner(self.config_file, self.temp_dir)
        start_time = time.time()
        seq_success = sequential_runner.run_tests()
        seq_time = time.time() - start_time
        
        # å¹¶è¡Œæ‰§è¡Œ
        parallel_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        start_time = time.time()
        par_success = parallel_runner.run_tests()
        par_time = time.time() - start_time
        
        # éªŒè¯ç»“æžœ
        self.assertTrue(seq_success, "é¡ºåºæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertTrue(par_success, "å¹¶è¡Œæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertEqual(
            sequential_runner.results["total"], 
            parallel_runner.results["total"],
            "æµ‹è¯•æ€»æ•°åº”è¯¥ç›¸åŒ"
        )
        self.assertEqual(
            sequential_runner.results["passed"], 
            parallel_runner.results["passed"],
            "é€šè¿‡çš„æµ‹è¯•æ•°åº”è¯¥ç›¸åŒ"
        )
        
        print(f"\næ€§èƒ½æ¯”è¾ƒ:")
        print(f"é¡ºåºæ‰§è¡Œæ—¶é—´: {seq_time:.3f}ç§’")
        print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {par_time:.3f}ç§’")
        if par_time > 0:
            print(f"åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x")
    
    def test_thread_vs_process_mode(self):
        """æµ‹è¯•çº¿ç¨‹æ¨¡å¼å’Œè¿›ç¨‹æ¨¡å¼"""
        # çº¿ç¨‹æ¨¡å¼
        thread_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        thread_success = thread_runner.run_tests()
        
        # è¿›ç¨‹æ¨¡å¼
        process_runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="process"
        )
        process_success = process_runner.run_tests()
        
        # éªŒè¯ç»“æžœ
        self.assertTrue(thread_success, "çº¿ç¨‹æ¨¡å¼åº”è¯¥æˆåŠŸ")
        self.assertTrue(process_success, "è¿›ç¨‹æ¨¡å¼åº”è¯¥æˆåŠŸ")
        self.assertEqual(
            thread_runner.results["passed"], 
            process_runner.results["passed"],
            "ä¸¤ç§æ¨¡å¼çš„é€šè¿‡æµ‹è¯•æ•°åº”è¯¥ç›¸åŒ"
        )
    
    def test_max_workers_configuration(self):
        """æµ‹è¯•ä¸åŒçš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°é…ç½®"""
        for max_workers in [1, 2, 4]:
            with self.subTest(max_workers=max_workers):
                runner = ParallelJSONRunner(
                    self.config_file, 
                    self.temp_dir, 
                    max_workers=max_workers, 
                    execution_mode="thread"
                )
                success = runner.run_tests()
                self.assertTrue(success, f"max_workers={max_workers}æ—¶åº”è¯¥æˆåŠŸ")
                self.assertEqual(runner.results["passed"], 4, "åº”è¯¥é€šè¿‡4ä¸ªæµ‹è¯•")
    
    def test_fallback_to_sequential(self):
        """æµ‹è¯•å›žé€€åˆ°é¡ºåºæ‰§è¡Œ"""
        runner = ParallelJSONRunner(
            self.config_file, 
            self.temp_dir, 
            max_workers=2, 
            execution_mode="thread"
        )
        
        # æµ‹è¯•å›žé€€åŠŸèƒ½
        success = runner.run_tests_sequential()
        self.assertTrue(success, "å›žé€€åˆ°é¡ºåºæ‰§è¡Œåº”è¯¥æˆåŠŸ")
        self.assertEqual(runner.results["passed"], 4, "åº”è¯¥é€šè¿‡4ä¸ªæµ‹è¯•")

if __name__ == '__main__':
    unittest.main()

================
File: tests/test_report.txt
================
Test Results Summary:
Total Tests: 15
Passed: 14
Failed: 1

Detailed Results:
âœ— text_identical_default
   ->  Expected to contain: Files i identical
âœ“ text_different_range
âœ“ json_exact_match
âœ“ json_key_based_match
âœ“ binary_comparison
âœ“ multi_thread_comparison
âœ“ different_output_format
âœ“ auto_detect_filetype
âœ“ h5_comparison
âœ“ h5_comparison_all_tables
âœ“ h5_comparison_with_wrong_table
âœ“ h5_comparison_with_strucutre_mode
âœ“ h5_comparison_with_content_mode
âœ“ h5_comparison_with_content_mode
âœ“ h5_comparison_with_table_regex

==================================================
FAILED TEST CASES DETAILS:
==================================================

1. Test: text_identical_default
----------------------------------------
Command: python ./compare_text.py D:\Document\xcode\Compare-File-Tool\test\1.bdf D:\Document\xcode\Compare-File-Tool\test\1_copy.bdf
Return Code: 0
Error Message:  Expected to contain: Files i identical

Command Output:
==============================
Files are identical.
2025-05-27 11:08:06,566 - file_comparator - INFO - Auto-detected file type: text
2025-05-27 11:08:06,668 - file_comparator.TextComparator - INFO - Comparing files: D:\Document\xcode\Compare-File-Tool\test\1.bdf and D:\Document\xcode\Compare-File-Tool\test\1_copy.bdf

==============================

================
File: tests/test_runners.py
================
import unittest
from src.runners.json_runner import JSONRunner
from src.runners.yaml_runner import YAMLRunner

class TestJSONRunner(unittest.TestCase):
    def setUp(self):
        self.runner = JSONRunner("tests/fixtures/test_cases.json")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

class TestYAMLRunner(unittest.TestCase):
    def setUp(self):
        self.runner = YAMLRunner("tests/fixtures/test_cases.yaml")

    def test_load_test_cases(self):
        self.runner.load_test_cases()
        self.assertGreater(len(self.runner.test_cases), 0, "No test cases loaded")

    def test_run_tests(self):
        self.runner.load_test_cases()
        results = self.runner.run_all_tests()
        self.assertTrue(results, "Some tests failed")

if __name__ == "__main__":
    unittest.main()

================
File: tests/test1.py
================
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import unittest
from src.runners.json_runner import JSONRunner
from src.core.base_runner import BaseRunner
from src.utils.report_generator import ReportGenerator

def main():
    runner = JSONRunner(
        config_file="D:/Document/xcode/cli-test-framework/tests/fixtures/test_cases1.json",
        workspace="D:/Document/xcode/Compare-File-Tool"
    )
    success = runner.run_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_generator = ReportGenerator(
        runner.results, 
        "D:/Document/xcode/cli-test-framework/tests/test_report.txt"
    )
    report_generator.print_report()  # æ‰“å°åˆ°æŽ§åˆ¶å°
    report_generator.save_report()   # ä¿å­˜åˆ°æ–‡ä»¶
    
    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: D:/Document/xcode/cli-test-framework/tests/test_report.txt")
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
